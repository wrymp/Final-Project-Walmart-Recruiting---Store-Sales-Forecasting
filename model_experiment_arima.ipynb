{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0xc_xtTn6ECN",
        "outputId": "9dca2845-85a2-413d-f902-03d3761ad687"
      },
      "id": "0xc_xtTn6ECN",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1527773442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install kaggle wandb onnx -Uq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "3Q92W4PQ6EaA"
      },
      "id": "3Q92W4PQ6EaA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "cDQaBGLX6FfU"
      },
      "id": "cDQaBGLX6FfU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "HkcOB55i6G8d"
      },
      "id": "HkcOB55i6G8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "MPwqMv2z6H6S"
      },
      "id": "MPwqMv2z6H6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "! unzip /content/train.csv.zip\n",
        "! unzip /content/test.csv.zip\n",
        "! unzip /content/features.csv.zip\n",
        "! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "id": "jQr6E5zG6KBU"
      },
      "id": "jQr6E5zG6KBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "myvAj7pC7CyH"
      },
      "id": "myvAj7pC7CyH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "c5Vm5Z5I7DRW"
      },
      "id": "c5Vm5Z5I7DRW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "mR9ELoN67Ef_"
      },
      "id": "mR9ELoN67Ef_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "# Initialize wandb for pipeline development\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ARIMA and time series imports\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from pmdarima import auto_arima\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import wandb\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA EXPLORATION"
      ],
      "metadata": {
        "id": "IOnTObXcLdmJ"
      },
      "id": "IOnTObXcLdmJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Change project name in all wandb.init() calls:\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Data_Exploration\", tags=[\"exploration\"])\n",
        "\n",
        "print(\"=== WALMART SALES FORECASTING - DATA EXPLORATION ===\\n\")\n",
        "\n",
        "# Load all datasets\n",
        "print(\"Loading datasets...\")\n",
        "train = pd.read_csv(\"/content/train.csv\")\n",
        "features = pd.read_csv(\"/content/features.csv\")\n",
        "stores = pd.read_csv(\"/content/stores.csv\")\n",
        "test = pd.read_csv(\"/content/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "\n",
        "print(\"Dataset shapes:\")\n",
        "print(f\"Train: {train.shape}\")\n",
        "print(f\"Features: {features.shape}\")\n",
        "print(f\"Stores: {stores.shape}\")\n",
        "print(f\"Test: {test.shape}\")\n",
        "print(f\"Sample Submission: {sample_submission.shape}\")\n",
        "\n",
        "# Log basic dataset info\n",
        "wandb.log({\n",
        "    \"train_rows\": train.shape[0],\n",
        "    \"train_cols\": train.shape[1],\n",
        "    \"features_rows\": features.shape[0],\n",
        "    \"features_cols\": features.shape[1],\n",
        "    \"stores_count\": stores.shape[0],\n",
        "    \"test_rows\": test.shape[0]\n",
        "})"
      ],
      "metadata": {
        "id": "54PZZRYuLhju"
      },
      "id": "54PZZRYuLhju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore individual datasets\n",
        "print(\"\\n=== TRAIN DATASET ===\")\n",
        "print(\"Columns:\", train.columns.tolist())\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(train.head())\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(train.describe())\n",
        "print(f\"\\nMissing values:\\n{train.isnull().sum()}\")\n",
        "\n",
        "print(\"\\n=== FEATURES DATASET ===\")\n",
        "print(\"Columns:\", features.columns.tolist())\n",
        "print(f\"\\nMissing values:\\n{features.isnull().sum()}\")\n",
        "print(\"\\nFeatures data sample:\")\n",
        "print(features.head())\n",
        "\n",
        "print(\"\\n=== STORES DATASET ===\")\n",
        "print(\"Columns:\", stores.columns.tolist())\n",
        "print(f\"\\nStore types: {stores['Type'].value_counts().to_dict()}\")\n",
        "print(f\"Store sizes range: {stores['Size'].min()} - {stores['Size'].max()}\")\n",
        "print(stores.head())"
      ],
      "metadata": {
        "id": "Z23MaHPlLkkf"
      },
      "id": "Z23MaHPlLkkf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge datasets for exploration\n",
        "print(\"\\n=== MERGING DATASETS ===\")\n",
        "data = train.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "data = data.merge(stores, on='Store', how='left')\n",
        "\n",
        "print(f\"Merged dataset shape: {data.shape}\")\n",
        "print(f\"Missing values after merge:\\n{data.isnull().sum()}\")\n",
        "\n",
        "# Convert date to datetime\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Basic time series analysis\n",
        "print(f\"\\nDate range: {data['Date'].min()} to {data['Date'].max()}\")\n",
        "print(f\"Unique stores: {data['Store'].nunique()}\")\n",
        "print(f\"Unique departments: {data['Dept'].nunique()}\")\n",
        "print(f\"Total weeks: {data['Date'].nunique()}\")\n",
        "\n",
        "# Log exploration metrics\n",
        "wandb.log({\n",
        "    \"merged_shape_rows\": data.shape[0],\n",
        "    \"merged_shape_cols\": data.shape[1],\n",
        "    \"unique_stores\": data['Store'].nunique(),\n",
        "    \"unique_departments\": data['Dept'].nunique(),\n",
        "    \"unique_weeks\": data['Date'].nunique(),\n",
        "    \"missing_values_total\": data.isnull().sum().sum()\n",
        "})"
      ],
      "metadata": {
        "id": "aw2te6AbLpd4"
      },
      "id": "aw2te6AbLpd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOME VISUALS"
      ],
      "metadata": {
        "id": "t0TFm9WwLuS5"
      },
      "id": "t0TFm9WwLuS5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sales distribution and patterns\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Sales distribution\n",
        "axes[0, 0].hist(data['Weekly_Sales'], bins=50, alpha=0.7, color='skyblue')\n",
        "axes[0, 0].set_title('Distribution of Weekly Sales')\n",
        "axes[0, 0].set_xlabel('Weekly Sales')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Sales by store type\n",
        "store_sales = data.groupby('Type')['Weekly_Sales'].mean().reset_index()\n",
        "axes[0, 1].bar(store_sales['Type'], store_sales['Weekly_Sales'], color=['coral', 'lightgreen', 'gold'])\n",
        "axes[0, 1].set_title('Average Weekly Sales by Store Type')\n",
        "axes[0, 1].set_xlabel('Store Type')\n",
        "axes[0, 1].set_ylabel('Average Weekly Sales')\n",
        "\n",
        "# Sales over time (monthly average)\n",
        "data['Month'] = data['Date'].dt.month\n",
        "monthly_sales = data.groupby('Month')['Weekly_Sales'].mean().reset_index()\n",
        "axes[1, 0].plot(monthly_sales['Month'], monthly_sales['Weekly_Sales'], marker='o', color='purple')\n",
        "axes[1, 0].set_title('Average Sales by Month')\n",
        "axes[1, 0].set_xlabel('Month')\n",
        "axes[1, 0].set_ylabel('Average Weekly Sales')\n",
        "axes[1, 0].set_xticks(range(1, 13))\n",
        "\n",
        "# Holiday vs Non-holiday sales\n",
        "holiday_sales = data.groupby('IsHoliday')['Weekly_Sales'].mean()\n",
        "axes[1, 1].bar(['Non-Holiday', 'Holiday'], holiday_sales.values, color=['lightcoral', 'lightsalmon'])\n",
        "axes[1, 1].set_title('Average Sales: Holiday vs Non-Holiday')\n",
        "axes[1, 1].set_ylabel('Average Weekly Sales')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')\n",
        "wandb.log({\"data_exploration\": wandb.Image('data_exploration.png')})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hltq64qULukT"
      },
      "id": "hltq64qULukT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation analysis"
      ],
      "metadata": {
        "id": "0NbIOGZZLyg1"
      },
      "id": "0NbIOGZZLyg1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation analysis\n",
        "print(\"\\n=== CORRELATION ANALYSIS ===\")\n",
        "numeric_cols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size']\n",
        "correlation_matrix = data[numeric_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True)\n",
        "plt.title('Correlation Matrix of Numeric Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "wandb.log({\"correlation_matrix\": wandb.Image('correlation_matrix.png')})\n",
        "plt.show()\n",
        "\n",
        "print(\"Key correlations with Weekly_Sales:\")\n",
        "sales_corr = correlation_matrix['Weekly_Sales'].sort_values(key=abs, ascending=False)\n",
        "for feature, corr in sales_corr.items():\n",
        "    if feature != 'Weekly_Sales':\n",
        "        print(f\"{feature}: {corr:.3f}\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "9cUVo7AdLy0q"
      },
      "id": "9cUVo7AdLy0q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline Definition"
      ],
      "metadata": {
        "id": "-1Ccq41QL3dp"
      },
      "id": "-1Ccq41QL3dp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb for pipeline development\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"ARIMA_Pipeline_Development\", tags=[\"pipeline\", \"arima\"])\n",
        "\n",
        "print(\"=== DEVELOPING PREPROCESSING PIPELINE ===\\n\")\n",
        "\n",
        "class WalmartARIMAPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom preprocessor for Walmart sales data for ARIMA modeling:\n",
        "    - Time series structure preparation\n",
        "    - Stationarity checks\n",
        "    - Seasonal decomposition\n",
        "    - Exogenous variable preparation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, freq='W', include_exogenous=True, aggregate_level='store_dept'):\n",
        "        self.freq = freq  # Weekly frequency\n",
        "        self.include_exogenous = include_exogenous\n",
        "        self.aggregate_level = aggregate_level  # 'store_dept', 'store', or 'total'\n",
        "        self.exog_columns = []\n",
        "        self.time_series_data = {}\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the preprocessor on training data\"\"\"\n",
        "        print(\"Fitting preprocessor...\")\n",
        "\n",
        "        # Combine X and y for time series preparation\n",
        "        data = X.copy()\n",
        "        if y is not None:\n",
        "            data['Weekly_Sales'] = y\n",
        "\n",
        "        # Convert Date to datetime\n",
        "        data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "        # Prepare exogenous variables\n",
        "        if self.include_exogenous:\n",
        "            self.exog_columns = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday']\n",
        "            # Add Type encoding\n",
        "            if 'Type' in data.columns:\n",
        "                data['Type_A'] = (data['Type'] == 'A').astype(int)\n",
        "                data['Type_B'] = (data['Type'] == 'B').astype(int)\n",
        "                self.exog_columns.extend(['Type_A', 'Type_B'])\n",
        "\n",
        "        self.fitted = True\n",
        "        print(\"Preprocessor fitted successfully!\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform the data for ARIMA modeling\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Preprocessor must be fitted before transform!\")\n",
        "\n",
        "        data = X.copy()\n",
        "        data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "        # Add Type encoding if needed\n",
        "        if 'Type' in data.columns and self.include_exogenous:\n",
        "            data['Type_A'] = (data['Type'] == 'A').astype(int)\n",
        "            data['Type_B'] = (data['Type'] == 'B').astype(int)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def prepare_time_series(self, data, target_col='Weekly_Sales'):\n",
        "        \"\"\"Prepare individual time series for ARIMA modeling\"\"\"\n",
        "        time_series_dict = {}\n",
        "\n",
        "        if self.aggregate_level == 'store_dept':\n",
        "            # Group by Store and Dept - most granular\n",
        "            for name, group in data.groupby(['Store', 'Dept']):\n",
        "                ts_data = self._prepare_single_series(group, target_col, name)\n",
        "                if ts_data is not None:\n",
        "                    time_series_dict[name] = ts_data\n",
        "\n",
        "        elif self.aggregate_level == 'store':\n",
        "            # Group by Store only\n",
        "            for name, group in data.groupby(['Store']):\n",
        "                # Aggregate by store\n",
        "                agg_dict = {'Weekly_Sales': 'sum', 'Date': 'first'}\n",
        "                for col in self.exog_columns:\n",
        "                    if col in group.columns:\n",
        "                        agg_dict[col] = 'mean'\n",
        "\n",
        "                store_data = group.groupby('Date').agg(agg_dict).reset_index()\n",
        "                ts_data = self._prepare_single_series(store_data, target_col, name)\n",
        "                if ts_data is not None:\n",
        "                    time_series_dict[name] = ts_data\n",
        "        else:\n",
        "            # Total aggregation\n",
        "            agg_dict = {'Weekly_Sales': 'sum'}\n",
        "            for col in self.exog_columns:\n",
        "                if col in data.columns:\n",
        "                    agg_dict[col] = 'mean'\n",
        "\n",
        "            total_data = data.groupby('Date').agg(agg_dict).reset_index()\n",
        "            ts_data = self._prepare_single_series(total_data, target_col, 'total')\n",
        "            if ts_data is not None:\n",
        "                time_series_dict['total'] = ts_data\n",
        "\n",
        "        return time_series_dict\n",
        "\n",
        "    def _prepare_single_series(self, group, target_col, name):\n",
        "        \"\"\"Prepare a single time series\"\"\"\n",
        "        try:\n",
        "            # Sort by date and set as index\n",
        "            ts_data = group.sort_values('Date').set_index('Date')\n",
        "\n",
        "            # Create complete date range and reindex\n",
        "            date_range = pd.date_range(start=ts_data.index.min(),\n",
        "                                     end=ts_data.index.max(),\n",
        "                                     freq=self.freq)\n",
        "            ts_data = ts_data.reindex(date_range)\n",
        "\n",
        "            # Handle exogenous variables\n",
        "            if self.include_exogenous:\n",
        "                for col in self.exog_columns:\n",
        "                    if col in ts_data.columns:\n",
        "                        ts_data[col] = ts_data[col].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "            # Handle missing target values\n",
        "            if target_col in ts_data.columns:\n",
        "                ts_data[target_col] = ts_data[target_col].fillna(ts_data[target_col].median())\n",
        "\n",
        "                # Remove series with too few observations\n",
        "                if ts_data[target_col].count() < 20:\n",
        "                    return None\n",
        "\n",
        "            return ts_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to prepare series {name}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "print(\"ARIMA Preprocessor class defined successfully!\")\n",
        "# Test the preprocessor\n",
        "print(\"Testing preprocessor with sample data...\")\n",
        "\n",
        "# Use a subset of data for testing\n",
        "sample_data = data.head(1000).copy()\n",
        "X_sample = sample_data.drop('Weekly_Sales', axis=1)\n",
        "y_sample = sample_data['Weekly_Sales']\n",
        "\n",
        "# Initialize and fit preprocessor\n",
        "arima_preprocessor = WalmartARIMAPreprocessor(aggregate_level='store')\n",
        "arima_preprocessor.fit(X_sample, y_sample)\n",
        "\n",
        "# Transform data\n",
        "X_transformed = arima_preprocessor.transform(X_sample.assign(Weekly_Sales=y_sample))\n",
        "\n",
        "# Prepare time series\n",
        "ts_data_dict = arima_preprocessor.prepare_time_series(X_transformed)\n",
        "\n",
        "print(f\"Original features: {X_sample.shape[1]}\")\n",
        "print(f\"Time series created: {len(ts_data_dict)}\")\n",
        "if ts_data_dict:\n",
        "    sample_ts = list(ts_data_dict.values())[0]\n",
        "    print(f\"Sample time series shape: {sample_ts.shape}\")\n",
        "    feature_names = list(sample_ts.columns)\n",
        "    print(f\"New feature names: {feature_names}\")\n",
        "\n",
        "# Log preprocessing results\n",
        "wandb.log({\n",
        "    \"original_features\": X_sample.shape[1],\n",
        "    \"transformed_features\": len(ts_data_dict),\n",
        "    \"feature_engineering_ratio\": len(ts_data_dict) / X_sample.shape[1] if X_sample.shape[1] > 0 else 0\n",
        "})\n",
        "\n",
        "print(\"\\nPreprocessor created successfully!\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "3YdCA0K1L3vt"
      },
      "id": "3YdCA0K1L3vt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING AND INITIAL TRAINING"
      ],
      "metadata": {
        "id": "PqRn7XPfL7iw"
      },
      "id": "PqRn7XPfL7iw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb for preprocessing and training\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"ARIMA_Preprocessing_Training\", tags=[\"preprocessing\", \"training\", \"arima\"])\n",
        "\n",
        "print(\"=== DATA PREPROCESSING AND INITIAL TRAINING ===\\n\")\n",
        "\n",
        "# Load and prepare full dataset\n",
        "print(\"Loading and preparing full dataset...\")\n",
        "data = train.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "data = data.merge(stores, on='Store', how='left')\n",
        "\n",
        "print(f\"Full dataset shape: {data.shape}\")\n",
        "\n",
        "# Remove rows with missing target\n",
        "initial_rows = data.shape[0]\n",
        "data = data.dropna(subset=['Weekly_Sales'])\n",
        "final_rows = data.shape[0]\n",
        "\n",
        "print(f\"Removed {initial_rows - final_rows} rows with missing target\")\n",
        "print(f\"Final dataset shape: {data.shape}\")\n",
        "\n",
        "# Sort by store, department, and date for time series\n",
        "data = data.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Prepare features and target\n",
        "X = data.drop('Weekly_Sales', axis=1)\n",
        "y = data['Weekly_Sales']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Target statistics:\")\n",
        "print(f\"  Mean: {y.mean():.2f}\")\n",
        "print(f\"  Std: {y.std():.2f}\")\n",
        "print(f\"  Min: {y.min():.2f}\")\n",
        "print(f\"  Max: {y.max():.2f}\")\n",
        "\n",
        "# Log data statistics\n",
        "wandb.log({\n",
        "    \"dataset_rows_final\": final_rows,\n",
        "    \"features_count\": X.shape[1],\n",
        "    \"target_mean\": y.mean(),\n",
        "    \"target_std\": y.std(),\n",
        "    \"target_min\": y.min(),\n",
        "    \"target_max\": y.max(),\n",
        "    \"rows_removed\": initial_rows - final_rows\n",
        "})"
      ],
      "metadata": {
        "id": "tEXGvCSNL73b"
      },
      "id": "tEXGvCSNL73b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "X1lWPhcnMBKs"
      },
      "id": "X1lWPhcnMBKs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit preprocessor\n",
        "print(\"\\nFitting preprocessor on full dataset...\")\n",
        "preprocessor = WalmartDataPreprocessor(\n",
        "    create_lag_features=True,\n",
        "    lag_periods=[1, 2, 4, 8],\n",
        "    rolling_windows=[4, 8, 12],\n",
        "    create_interactions=True\n",
        ")\n",
        "\n",
        "# Fit preprocessor\n",
        "preprocessor.fit(X)\n",
        "\n",
        "# Transform features\n",
        "print(\"Transforming features...\")\n",
        "X_processed = preprocessor.transform(X)\n",
        "\n",
        "print(f\"Processed features shape: {X_processed.shape}\")\n",
        "print(f\"Feature engineering created {X_processed.shape[1] - X.shape[1]} new features\")\n",
        "\n",
        "# Check for any remaining missing values\n",
        "missing_counts = X_processed.isnull().sum()\n",
        "if missing_counts.sum() > 0:\n",
        "    print(f\"Warning: Still have missing values:\")\n",
        "    print(missing_counts[missing_counts > 0])\n",
        "else:\n",
        "    print(\"✓ No missing values in processed data\")\n",
        "\n",
        "# Log preprocessing results\n",
        "wandb.log({\n",
        "    \"processed_features_count\": X_processed.shape[1],\n",
        "    \"new_features_created\": X_processed.shape[1] - X.shape[1],\n",
        "    \"remaining_missing_values\": missing_counts.sum()\n",
        "})"
      ],
      "metadata": {
        "id": "WGmDKvkyMBco"
      },
      "id": "WGmDKvkyMBco",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  INITIAL MODEL TRAINING"
      ],
      "metadata": {
        "id": "BN6I6TnHMFCB"
      },
      "id": "BN6I6TnHMFCB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit preprocessor\n",
        "print(\"\\nFitting preprocessor on full dataset...\")\n",
        "arima_preprocessor = WalmartARIMAPreprocessor(\n",
        "    freq='W',\n",
        "    include_exogenous=True,\n",
        "    aggregate_level='store'  # Aggregate by store for manageable number of models\n",
        ")\n",
        "\n",
        "# Fit preprocessor\n",
        "arima_preprocessor.fit(X, y)\n",
        "\n",
        "# Transform features\n",
        "print(\"Transforming features...\")\n",
        "data_transformed = arima_preprocessor.transform(X.assign(Weekly_Sales=y))\n",
        "\n",
        "# Prepare time series\n",
        "ts_data_dict = arima_preprocessor.prepare_time_series(data_transformed)\n",
        "\n",
        "print(f\"Time series created: {len(ts_data_dict)}\")\n",
        "print(f\"Time series preparation created {len(ts_data_dict)} individual models needed\")\n",
        "\n",
        "# Check for any remaining missing values in sample series\n",
        "if ts_data_dict:\n",
        "    sample_ts = list(ts_data_dict.values())[0]\n",
        "    missing_counts = sample_ts.isnull().sum()\n",
        "    if missing_counts.sum() > 0:\n",
        "        print(f\"Warning: Still have missing values in sample series:\")\n",
        "        print(missing_counts[missing_counts > 0])\n",
        "    else:\n",
        "        print(\"✓ No missing values in processed time series\")\n",
        "\n",
        "# Log preprocessing results\n",
        "wandb.log({\n",
        "    \"processed_features_count\": len(ts_data_dict),\n",
        "    \"new_features_created\": len(ts_data_dict) - X.shape[1],\n",
        "    \"remaining_missing_values\": missing_counts.sum() if 'missing_counts' in locals() else 0\n",
        "})\n",
        "\n",
        "print(\"\\n=== INITIAL ARIMA MODEL TRAINING ===\")\n",
        "\n",
        "# Initialize ARIMA ensemble for initial training\n",
        "initial_arima = ARIMAEnsemble(\n",
        "    max_p=2,  # Reduced for initial training\n",
        "    max_d=1,\n",
        "    max_q=2,\n",
        "    seasonal=True,\n",
        "    m=52\n",
        ")\n",
        "\n",
        "# Train on subset for initial assessment\n",
        "sample_series = dict(list(ts_data_dict.items())[:5])  # First 5 series\n",
        "print(f\"Training initial ARIMA models on {len(sample_series)} time series...\")\n",
        "\n",
        "exog_cols = arima_preprocessor.exog_columns\n",
        "successful_fits, failed_fits = initial_arima.fit(sample_series, exog_cols=exog_cols)\n",
        "\n",
        "# Get initial performance estimates\n",
        "if successful_fits > 0:\n",
        "    fitted_values_dict = initial_arima.get_fitted_values()\n",
        "\n",
        "    initial_maes = []\n",
        "    initial_rmses = []\n",
        "\n",
        "    for name, fitted_values in fitted_values_dict.items():\n",
        "        if name in sample_series:\n",
        "            y_actual = sample_series[name]['Weekly_Sales'].dropna()\n",
        "            common_index = y_actual.index.intersection(fitted_values.index)\n",
        "\n",
        "            if len(common_index) > 0:\n",
        "                y_aligned = y_actual.loc[common_index]\n",
        "                fitted_aligned = fitted_values.loc[common_index]\n",
        "\n",
        "                mae = mean_absolute_error(y_aligned, fitted_aligned)\n",
        "                rmse = np.sqrt(mean_squared_error(y_aligned, fitted_aligned))\n",
        "\n",
        "                initial_maes.append(mae)\n",
        "                initial_rmses.append(rmse)\n",
        "\n",
        "    if initial_maes:\n",
        "        avg_mae = np.mean(initial_maes)\n",
        "        avg_rmse = np.mean(initial_rmses)\n",
        "\n",
        "        print(f\"\\nInitial Model Results (sample of {len(initial_maes)} models):\")\n",
        "        print(f\"Average MAE: {avg_mae:.2f}\")\n",
        "        print(f\"Average RMSE: {avg_rmse:.2f}\")\n",
        "        print(f\"MAE Range: {min(initial_maes):.2f} - {max(initial_maes):.2f}\")\n",
        "\n",
        "        # Log initial results\n",
        "        wandb.log({\n",
        "            \"initial_sample_models\": len(initial_maes),\n",
        "            \"initial_avg_mae\": avg_mae,\n",
        "            \"initial_avg_rmse\": avg_rmse,\n",
        "            \"initial_min_mae\": min(initial_maes),\n",
        "            \"initial_max_mae\": max(initial_maes),\n",
        "            \"initial_successful_fits\": successful_fits,\n",
        "            \"initial_failed_fits\": failed_fits\n",
        "        })\n",
        "\n",
        "print(f\"\\nSuccessfully fitted {successful_fits} initial ARIMA models!\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "vh07uywWMFSa"
      },
      "id": "vh07uywWMFSa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb for cross-validation and tuning\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"ARIMA_CrossValidation_Tuning\", tags=[\"cv\", \"tuning\", \"arima\"])\n",
        "\n",
        "print(\"=== CROSS-VALIDATION AND HYPERPARAMETER TUNING ===\\n\")\n",
        "\n",
        "# ARIMA parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_p': [2, 3],\n",
        "    'max_d': [1, 2],\n",
        "    'max_q': [2, 3],\n",
        "    'seasonal': [True],  # Always use seasonal\n",
        "    'm': [52]  # Weekly seasonality\n",
        "}\n",
        "\n",
        "print(f\"ARIMA hyperparameter search space:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
        "print(f\"\\nTotal combinations to test: {total_combinations}\")\n",
        "\n",
        "def evaluate_arima_params(params, ts_sample, cv_splits=3, test_size=8):\n",
        "    \"\"\"Evaluate ARIMA parameters using time series cross-validation\"\"\"\n",
        "    all_cv_scores = []\n",
        "\n",
        "    for name, ts_data in ts_sample.items():\n",
        "        try:\n",
        "            y = ts_data['Weekly_Sales'].dropna()\n",
        "\n",
        "            if len(y) < test_size * (cv_splits + 1) + 20:\n",
        "                continue\n",
        "\n",
        "            cv_scores = []\n",
        "\n",
        "            for i in range(cv_splits):\n",
        "                test_end = len(y) - i * test_size\n",
        "                test_start = test_end - test_size\n",
        "                train_end = test_start\n",
        "\n",
        "                if train_end < 20:\n",
        "                    break\n",
        "\n",
        "                y_train = y.iloc[:train_end]\n",
        "                y_test = y.iloc[test_start:test_end]\n",
        "\n",
        "                try:\n",
        "                    model = auto_arima(\n",
        "                        y_train,\n",
        "                        start_p=0, start_q=0,\n",
        "                        max_p=params['max_p'],\n",
        "                        max_d=params['max_d'],\n",
        "                        max_q=params['max_q'],\n",
        "                        seasonal=params['seasonal'],\n",
        "                        m=params['m'],\n",
        "                        stepwise=True,\n",
        "                        suppress_warnings=True,\n",
        "                        error_action='ignore'\n",
        "                    )\n",
        "\n",
        "                    predictions = model.predict(n_periods=len(y_test))\n",
        "                    mae = mean_absolute_error(y_test, predictions)\n",
        "                    cv_scores.append(mae)\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if cv_scores:\n",
        "                all_cv_scores.extend(cv_scores)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return all_cv_scores\n",
        "\n",
        "# Hyperparameter tuning\n",
        "print(\"\\nStarting hyperparameter tuning...\")\n",
        "best_score = float('inf')\n",
        "best_params = None\n",
        "all_results = []\n",
        "\n",
        "# Use subset of time series for tuning\n",
        "tuning_sample = dict(list(ts_data_dict.items())[:3])  # First 3 series\n",
        "print(f\"Using {len(tuning_sample)} time series for hyperparameter tuning\")\n",
        "\n",
        "iteration = 0\n",
        "for max_p in param_grid['max_p']:\n",
        "    for max_d in param_grid['max_d']:\n",
        "        for max_q in param_grid['max_q']:\n",
        "            for seasonal in param_grid['seasonal']:\n",
        "                for m in param_grid['m']:\n",
        "                    iteration += 1\n",
        "\n",
        "                    params = {\n",
        "                        'max_p': max_p,\n",
        "                        'max_d': max_d,\n",
        "                        'max_q': max_q,\n",
        "                        'seasonal': seasonal,\n",
        "                        'm': m\n",
        "                    }\n",
        "\n",
        "                    print(f\"\\n[{iteration}/{total_combinations}] Testing parameters:\")\n",
        "                    print(f\"  max_p={max_p}, max_d={max_d}, max_q={max_q}, seasonal={seasonal}, m={m}\")\n",
        "\n",
        "                    # Evaluate with cross-validation\n",
        "                    cv_scores = evaluate_arima_params(params, tuning_sample)\n",
        "\n",
        "                    if cv_scores:\n",
        "                        avg_score = np.mean(cv_scores)\n",
        "                        std_score = np.std(cv_scores)\n",
        "\n",
        "                        print(f\"  CV Score: {avg_score:.3f} (+/- {std_score:.3f}) from {len(cv_scores)} folds\")\n",
        "\n",
        "                        # Log to wandb\n",
        "                        wandb.log({\n",
        "                            **params,\n",
        "                            'cv_mae_mean': avg_score,\n",
        "                            'cv_mae_std': std_score,\n",
        "                            'cv_mae_min': min(cv_scores),\n",
        "                            'cv_mae_max': max(cv_scores),\n",
        "                            'cv_folds': len(cv_scores),\n",
        "                            'iteration': iteration\n",
        "                        })\n",
        "\n",
        "                        # Store results\n",
        "                        result = {\n",
        "                            'params': params.copy(),\n",
        "                            'cv_mean': avg_score,\n",
        "                            'cv_std': std_score,\n",
        "                            'cv_scores': cv_scores\n",
        "                        }\n",
        "                        all_results.append(result)\n",
        "\n",
        "                        # Update best parameters\n",
        "                        if avg_score < best_score:\n",
        "                            best_score = avg_score\n",
        "                            best_params = params.copy()\n",
        "                            print(f\"  *** NEW BEST SCORE: {best_score:.3f} ***\")\n",
        "                    else:\n",
        "                        print(f\"  No valid CV scores obtained\")\n",
        "\n",
        "print(f\"\\n=== HYPERPARAMETER TUNING COMPLETED ===\")\n",
        "if best_params:\n",
        "    print(f\"Best CV Score: {best_score:.3f}\")\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "    # Log best results\n",
        "    wandb.log({\n",
        "        'best_cv_mae': best_score,\n",
        "        'best_params': best_params,\n",
        "        'total_combinations_tested': total_combinations\n",
        "    })\n",
        "else:\n",
        "    print(\"No valid parameter combinations found\")\n",
        "    best_params = {\n",
        "        'max_p': 3,\n",
        "        'max_d': 1,\n",
        "        'max_q': 3,\n",
        "        'seasonal': True,\n",
        "        'm': 52\n",
        "    }\n",
        "    print(f\"Using default parameters: {best_params}\")\n",
        "\n",
        "# Analysis of results\n",
        "if all_results:\n",
        "    results_df = pd.DataFrame([\n",
        "        {**r['params'], 'cv_mean': r['cv_mean'], 'cv_std': r['cv_std']}\n",
        "        for r in all_results\n",
        "    ])\n",
        "\n",
        "    print(f\"\\nTop parameter combinations:\")\n",
        "    top_results = results_df.nsmallest(min(5, len(results_df)), 'cv_mean')[['max_p', 'max_d', 'max_q', 'cv_mean', 'cv_std']]\n",
        "    print(top_results)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "CK46jGQbDSpi"
      },
      "id": "CK46jGQbDSpi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL MODEL TRAINING AND ANALYSIS"
      ],
      "metadata": {
        "id": "8rrJ88rGrMeR"
      },
      "id": "8rrJ88rGrMeR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb for final training\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"ARIMA_Final_Training\", tags=[\"final\", \"training\", \"arima\", \"production\"])\n",
        "\n",
        "print(\"=== FINAL MODEL TRAINING AND ANALYSIS ===\\n\")\n",
        "\n",
        "# Load best parameters (use best found or defaults if none)\n",
        "final_params = best_params if 'best_params' in locals() and best_params else {\n",
        "    'max_p': 3,\n",
        "    'max_d': 1,\n",
        "    'max_q': 3,\n",
        "    'seasonal': True,\n",
        "    'm': 52\n",
        "}\n",
        "\n",
        "print(\"Final model parameters:\")\n",
        "for param, value in final_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Log final parameters\n",
        "wandb.log(final_params)\n",
        "\n",
        "# Create final ARIMA ensemble\n",
        "print(\"\\nCreating final ARIMA ensemble...\")\n",
        "\n",
        "final_arima_ensemble = ARIMAEnsemble(\n",
        "    max_p=final_params['max_p'],\n",
        "    max_d=final_params['max_d'],\n",
        "    max_q=final_params['max_q'],\n",
        "    seasonal=final_params['seasonal'],\n",
        "    m=final_params['m']\n",
        ")\n",
        "\n",
        "print(\"ARIMA ensemble created successfully!\")\n",
        "\n",
        "# Train final model on full dataset\n",
        "print(\"\\nTraining final ARIMA ensemble on full dataset...\")\n",
        "\n",
        "print(f\"Training on {len(ts_data_dict)} time series\")\n",
        "\n",
        "# Fit ensemble\n",
        "exog_cols = arima_preprocessor.exog_columns\n",
        "successful_fits, failed_fits = final_arima_ensemble.fit(ts_data_dict, exog_cols=exog_cols)\n",
        "\n",
        "print(\"Final ARIMA ensemble trained successfully!\")\n",
        "print(f\"Successfully fitted: {successful_fits} models\")\n",
        "print(f\"Failed fits: {failed_fits} models\")\n",
        "\n",
        "# Make predictions for evaluation\n",
        "print(\"Making predictions on training data...\")\n",
        "\n",
        "# Get fitted values from all models\n",
        "fitted_values_dict = final_arima_ensemble.get_fitted_values()\n",
        "\n",
        "all_predictions_final = []\n",
        "all_actuals_final = []\n",
        "model_performance_final = {}\n",
        "\n",
        "for name, fitted_values in fitted_values_dict.items():\n",
        "    if name in ts_data_dict:\n",
        "        try:\n",
        "            ts_data = ts_data_dict[name]\n",
        "            y_actual = ts_data['Weekly_Sales'].dropna()\n",
        "\n",
        "            # Align fitted values with actual values\n",
        "            common_index = y_actual.index.intersection(fitted_values.index)\n",
        "\n",
        "            if len(common_index) > 0:\n",
        "                y_aligned = y_actual.loc[common_index]\n",
        "                fitted_aligned = fitted_values.loc[common_index]\n",
        "\n",
        "                # Calculate metrics for this model\n",
        "                mae = mean_absolute_error(y_aligned, fitted_aligned)\n",
        "                rmse = np.sqrt(mean_squared_error(y_aligned, fitted_aligned))\n",
        "                r2 = r2_score(y_aligned, fitted_aligned)\n",
        "\n",
        "                model_performance_final[name] = {\n",
        "                    'mae': mae,\n",
        "                    'rmse': rmse,\n",
        "                    'r2': r2,\n",
        "                    'observations': len(y_aligned)\n",
        "                }\n",
        "\n",
        "                # Collect for overall metrics\n",
        "                all_predictions_final.extend(fitted_aligned.values)\n",
        "                all_actuals_final.extend(y_aligned.values)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Evaluation failed for {name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "if all_predictions_final and all_actuals_final:\n",
        "    train_mae = mean_absolute_error(all_actuals_final, all_predictions_final)\n",
        "    train_rmse = np.sqrt(mean_squared_error(all_actuals_final, all_predictions_final))\n",
        "    train_r2 = r2_score(all_actuals_final, all_actuals_final)\n",
        "\n",
        "    # Fix MAPE calculation (handle zero values)\n",
        "    def safe_mape(y_true, y_pred):\n",
        "        \"\"\"Calculate MAPE while handling zero values\"\"\"\n",
        "        mask = np.array(y_true) != 0\n",
        "        if mask.sum() == 0:\n",
        "            return float('inf')\n",
        "        return np.mean(np.abs((np.array(y_true)[mask] - np.array(y_pred)[mask]) / np.array(y_true)[mask])) * 100\n",
        "\n",
        "    train_mape_fixed = safe_mape(all_actuals_final, all_predictions_final)\n",
        "\n",
        "    print(f\"\\n=== FINAL MODEL PERFORMANCE ===\")\n",
        "    print(f\"Training MAE: {train_mae:.2f}\")\n",
        "    print(f\"Training RMSE: {train_rmse:.2f}\")\n",
        "    print(f\"Training MAPE: {train_mape_fixed:.2f}%\")\n",
        "    print(f\"Training R²: {train_r2:.4f}\")\n",
        "\n",
        "    # Log final metrics\n",
        "    wandb.log({\n",
        "        'final_train_mae': train_mae,\n",
        "        'final_train_rmse': train_rmse,\n",
        "        'final_train_mape': train_mape_fixed if not np.isinf(train_mape_fixed) else 0.0,\n",
        "        'final_train_r2': train_r2,\n",
        "        'final_samples_count': len(all_actuals_final)\n",
        "    })\n",
        "\n",
        "    # Individual model statistics\n",
        "    individual_maes = [perf['mae'] for perf in model_performance_final.values()]\n",
        "\n",
        "    print(f\"\\nIndividual Model Performance:\")\n",
        "    print(f\"  Best MAE: {min(individual_maes):.2f}\")\n",
        "    print(f\"  Worst MAE: {max(individual_maes):.2f}\")\n",
        "    print(f\"  Median MAE: {np.median(individual_maes):.2f}\")\n",
        "\n",
        "# Get feature names equivalent\n",
        "feature_names = arima_preprocessor.exog_columns\n",
        "\n",
        "print(\"\\n=== SAVING FINAL MODEL ===\")\n",
        "\n",
        "# Save ARIMA ensemble filename\n",
        "pipeline_filename = f\"arima_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
        "\n",
        "# Create comprehensive save object\n",
        "arima_save_object = {\n",
        "    'ensemble': final_arima_ensemble,\n",
        "    'preprocessor': arima_preprocessor,\n",
        "    'performance': model_performance_final,\n",
        "    'overall_metrics': {\n",
        "        'mae': train_mae if 'train_mae' in locals() else 0,\n",
        "        'rmse': train_rmse if 'train_rmse' in locals() else 0,\n",
        "        'mape': train_mape_fixed if 'train_mape_fixed' in locals() and not np.isinf(train_mape_fixed) else 0.0,\n",
        "        'r2': train_r2 if 'train_r2' in locals() else 0\n",
        "    },\n",
        "    'metadata': {\n",
        "        'created_date': datetime.now().isoformat(),\n",
        "        'successful_models': successful_fits,\n",
        "        'total_series': len(ts_data_dict),\n",
        "        'exogenous_variables': feature_names\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save with cloudpickle\n",
        "try:\n",
        "    import cloudpickle\n",
        "    with open(pipeline_filename, 'wb') as f:\n",
        "        cloudpickle.dump(arima_save_object, f)\n",
        "    print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "except ImportError:\n",
        "    import pickle\n",
        "    with open(pipeline_filename, 'wb') as f:\n",
        "        pickle.dump(arima_save_object, f)\n",
        "    print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "\n",
        "# Try to upload to wandb with error handling\n",
        "try:\n",
        "    model_artifact = wandb.Artifact(\n",
        "        name=\"ARIMA_pipeline\",\n",
        "        type=\"model\",\n",
        "        description=\"Final ARIMA pipeline for Walmart sales forecasting\",\n",
        "        metadata={\n",
        "            \"train_mae\": float(train_mae) if 'train_mae' in locals() else 0.0,\n",
        "            \"train_rmse\": float(train_rmse) if 'train_rmse' in locals() else 0.0,\n",
        "            \"train_mape\": float(train_mape_fixed) if 'train_mape_fixed' in locals() and not np.isinf(train_mape_fixed) else 0.0,\n",
        "            \"train_r2\": float(train_r2) if 'train_r2' in locals() else 0.0,\n",
        "            \"features_count\": len(feature_names),\n",
        "            \"training_samples\": len(all_actuals_final) if 'all_actuals_final' in locals() else 0,\n",
        "            \"model_type\": \"ARIMA\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    model_artifact.add_file(pipeline_filename)\n",
        "    wandb.log_artifact(model_artifact)\n",
        "    print(\"✓ Model artifact logged to wandb successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error uploading to wandb: {e}\")\n",
        "    print(\"Model saved locally - you can manually upload later\")\n",
        "\n",
        "    # Log just the metrics without artifact\n",
        "    wandb.log({\n",
        "        'final_train_mae': train_mae if 'train_mae' in locals() else 0,\n",
        "        'final_train_rmse': train_rmse if 'train_rmse' in locals() else 0,\n",
        "        'final_train_mape': train_mape_fixed if 'train_mape_fixed' in locals() and not np.isinf(train_mape_fixed) else 0.0,\n",
        "        'final_train_r2': train_r2 if 'train_r2' in locals() else 0,\n",
        "        'model_saved_locally': pipeline_filename\n",
        "    })\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL MODEL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model Type: ARIMA Ensemble\")\n",
        "print(f\"Training Samples: {len(all_actuals_final) if 'all_actuals_final' in locals() else 0:,}\")\n",
        "print(f\"Individual Models: {successful_fits}\")\n",
        "print(f\"Features: {len(feature_names)}\")\n",
        "if 'train_mae' in locals():\n",
        "    print(f\"Training MAE: {train_mae:.2f}\")\n",
        "    print(f\"Training RMSE: {train_rmse:.2f}\")\n",
        "    print(f\"Training MAPE: {train_mape_fixed:.2f}%\")\n",
        "    print(f\"Training R²: {train_r2:.4f}\")\n",
        "print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "wZXq3dZUDTHZ"
      },
      "id": "wZXq3dZUDTHZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final pipeline"
      ],
      "metadata": {
        "id": "pPdoqZ1MrP41"
      },
      "id": "pPdoqZ1MrP41"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create final pipeline equivalent\n",
        "print(\"\\nCreating final ARIMA pipeline...\")\n",
        "\n",
        "final_preprocessor = WalmartARIMAPreprocessor(\n",
        "    freq='W',\n",
        "    include_exogenous=True,\n",
        "    aggregate_level='store'\n",
        ")\n",
        "\n",
        "final_model = ARIMAEnsemble(\n",
        "    max_p=final_params['max_p'],\n",
        "    max_d=final_params['max_d'],\n",
        "    max_q=final_params['max_q'],\n",
        "    seasonal=final_params['seasonal'],\n",
        "    m=final_params['m']\n",
        ")\n",
        "\n",
        "print(\"ARIMA Pipeline created successfully!\")"
      ],
      "metadata": {
        "id": "rDCoGZEeDW6_"
      },
      "id": "rDCoGZEeDW6_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final model training"
      ],
      "metadata": {
        "id": "mTqhs66grSgK"
      },
      "id": "mTqhs66grSgK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train final model on full dataset\n",
        "print(\"\\nTraining final model on full dataset...\")\n",
        "\n",
        "# Prepare final dataset\n",
        "X_final = data.drop('Weekly_Sales', axis=1)\n",
        "y_final = data['Weekly_Sales']\n",
        "\n",
        "print(f\"Training on {len(X_final)} samples with {X_final.shape[1]} features\")\n",
        "\n",
        "# Fit preprocessor\n",
        "final_preprocessor.fit(X_final, y_final)\n",
        "\n",
        "# Transform data\n",
        "data_transformed = final_preprocessor.transform(X_final.assign(Weekly_Sales=y_final))\n",
        "\n",
        "# Prepare time series\n",
        "ts_data_dict = final_preprocessor.prepare_time_series(data_transformed)\n",
        "\n",
        "# Fit ensemble\n",
        "exog_cols = final_preprocessor.exog_columns\n",
        "successful_fits, failed_fits = final_model.fit(ts_data_dict, exog_cols=exog_cols)\n",
        "\n",
        "print(\"Final ARIMA ensemble trained successfully!\")\n",
        "print(f\"Successfully fitted: {successful_fits} models\")\n",
        "print(f\"Failed fits: {failed_fits} models\")\n",
        "\n",
        "# Make predictions for evaluation\n",
        "print(\"Making predictions on training data...\")\n",
        "\n",
        "# Get fitted values from all models\n",
        "fitted_values_dict = final_model.get_fitted_values()\n",
        "\n",
        "all_predictions = []\n",
        "all_actuals = []\n",
        "\n",
        "for name, fitted_values in fitted_values_dict.items():\n",
        "    if name in ts_data_dict:\n",
        "        try:\n",
        "            ts_data = ts_data_dict[name]\n",
        "            y_actual = ts_data['Weekly_Sales'].dropna()\n",
        "            common_index = y_actual.index.intersection(fitted_values.index)\n",
        "\n",
        "            if len(common_index) > 0:\n",
        "                y_aligned = y_actual.loc[common_index]\n",
        "                fitted_aligned = fitted_values.loc[common_index]\n",
        "\n",
        "                all_predictions.extend(fitted_aligned.values)\n",
        "                all_actuals.extend(y_aligned.values)\n",
        "        except Exception as e:\n",
        "            print(f\"Evaluation failed for {name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "y_pred_final = np.array(all_predictions)\n",
        "y_final_aligned = np.array(all_actuals)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "train_mae = mean_absolute_error(y_final_aligned, y_pred_final)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_final_aligned, y_pred_final))\n",
        "train_mape = np.mean(np.abs((y_final_aligned - y_pred_final) / y_final_aligned)) * 100\n",
        "\n",
        "# Calculate R²\n",
        "from sklearn.metrics import r2_score\n",
        "train_r2 = r2_score(y_final_aligned, y_pred_final)\n",
        "\n",
        "print(f\"\\n=== FINAL MODEL PERFORMANCE ===\")\n",
        "print(f\"Training MAE: {train_mae:.2f}\")\n",
        "print(f\"Training RMSE: {train_rmse:.2f}\")\n",
        "print(f\"Training MAPE: {train_mape:.2f}%\")\n",
        "print(f\"Training R²: {train_r2:.4f}\")\n",
        "\n",
        "# Log final metrics\n",
        "wandb.log({\n",
        "    'final_train_mae': train_mae,\n",
        "    'final_train_rmse': train_rmse,\n",
        "    'final_train_mape': train_mape,\n",
        "    'final_train_r2': train_r2,\n",
        "    'final_samples_count': len(y_final_aligned)\n",
        "})"
      ],
      "metadata": {
        "id": "2VuJcZ8ODYAM"
      },
      "id": "2VuJcZ8ODYAM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "AcbKbVmBrVKk"
      },
      "id": "AcbKbVmBrVKk"
    },
    {
      "cell_type": "code",
      "source": [
        "# ARIMA Model Analysis (equivalent to feature importance)\n",
        "print(\"\\n=== ARIMA MODEL ANALYSIS ===\")\n",
        "\n",
        "# Analyze exogenous variable usage across models\n",
        "exog_usage = {}\n",
        "model_orders = {}\n",
        "seasonal_orders = {}\n",
        "\n",
        "for name, params in final_model.model_params.items():\n",
        "    # Track exogenous variable usage\n",
        "    for exog_var in params.get('exog_used', []):\n",
        "        exog_usage[exog_var] = exog_usage.get(exog_var, 0) + 1\n",
        "\n",
        "    # Track model orders\n",
        "    order_str = str(params['order'])\n",
        "    model_orders[order_str] = model_orders.get(order_str, 0) + 1\n",
        "\n",
        "    # Track seasonal orders\n",
        "    if params.get('seasonal_order'):\n",
        "        seasonal_str = str(params['seasonal_order'])\n",
        "        seasonal_orders[seasonal_str] = seasonal_orders.get(seasonal_str, 0) + 1\n",
        "\n",
        "# Create exogenous variable importance based on usage\n",
        "total_models = len(final_model.models)\n",
        "feature_names = list(exog_usage.keys()) if exog_usage else final_preprocessor.exog_columns\n",
        "\n",
        "if exog_usage:\n",
        "    importance_df = pd.DataFrame([\n",
        "        {\n",
        "            'Feature': var,\n",
        "            'Models_Used': count,\n",
        "            'Usage_Pct': (count / total_models) * 100,\n",
        "            'Importance': count / total_models  # Normalized importance\n",
        "        }\n",
        "        for var, count in exog_usage.items()\n",
        "    ]).sort_values('Usage_Pct', ascending=False)\n",
        "\n",
        "    print(f\"Exogenous Variable Usage Across {total_models} Models:\")\n",
        "    for i, (_, row) in enumerate(importance_df.iterrows()):\n",
        "        print(f\"{i+1:2d}. {row['Feature']:<25} | {row['Models_Used']}/{total_models} models ({row['Usage_Pct']:.1f}%)\")\n",
        "else:\n",
        "    print(\"No exogenous variables were used in the models\")\n",
        "    importance_df = pd.DataFrame(columns=['Feature', 'Models_Used', 'Usage_Pct', 'Importance'])\n",
        "\n",
        "print(f\"\\nMost Common ARIMA Orders:\")\n",
        "for i, (order, count) in enumerate(sorted(model_orders.items(), key=lambda x: x[1], reverse=True)[:10]):\n",
        "    print(f\"{i+1:2d}. {order:<15} | {count} models ({count/total_models*100:.1f}%)\")\n",
        "\n",
        "if seasonal_orders:\n",
        "    print(f\"\\nMost Common Seasonal Orders:\")\n",
        "    for i, (order, count) in enumerate(sorted(seasonal_orders.items(), key=lambda x: x[1], reverse=True)[:5]):\n",
        "        print(f\"{i+1:2d}. {order:<20} | {count} models ({count/total_models*100:.1f}%)\")\n",
        "\n",
        "# Create visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
        "\n",
        "# Exogenous variable usage\n",
        "if not importance_df.empty:\n",
        "    top_vars = importance_df.head(10)\n",
        "    ax1.barh(range(len(top_vars)), top_vars['Usage_Pct'])\n",
        "    ax1.set_yticks(range(len(top_vars)))\n",
        "    ax1.set_yticklabels(top_vars['Feature'])\n",
        "    ax1.set_xlabel('Usage Percentage (%)')\n",
        "    ax1.set_title('Exogenous Variable Usage Across Models')\n",
        "    ax1.invert_yaxis()\n",
        "else:\n",
        "    ax1.text(0.5, 0.5, 'No exogenous variables used', ha='center', va='center', transform=ax1.transAxes)\n",
        "    ax1.set_title('Exogenous Variable Usage')\n",
        "\n",
        "# ARIMA order distribution\n",
        "orders_list = list(model_orders.keys())[:10]\n",
        "counts_list = [model_orders[order] for order in orders_list]\n",
        "ax2.bar(range(len(orders_list)), counts_list, color='lightblue')\n",
        "ax2.set_xlabel('ARIMA Order')\n",
        "ax2.set_ylabel('Number of Models')\n",
        "ax2.set_title('Distribution of ARIMA Orders')\n",
        "ax2.set_xticks(range(len(orders_list)))\n",
        "ax2.set_xticklabels(orders_list, rotation=45)\n",
        "\n",
        "# Model performance distribution\n",
        "individual_maes = []\n",
        "for name, fitted_values in fitted_values_dict.items():\n",
        "    if name in ts_data_dict:\n",
        "        try:\n",
        "            ts_data = ts_data_dict[name]\n",
        "            y_actual = ts_data['Weekly_Sales'].dropna()\n",
        "            common_index = y_actual.index.intersection(fitted_values.index)\n",
        "\n",
        "            if len(common_index) > 0:\n",
        "                y_aligned = y_actual.loc[common_index]\n",
        "                fitted_aligned = fitted_values.loc[common_index]\n",
        "                mae = mean_absolute_error(y_aligned, fitted_aligned)\n",
        "                individual_maes.append(mae)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "if individual_maes:\n",
        "    ax3.hist(individual_maes, bins=20, alpha=0.7, color='lightgreen')\n",
        "    ax3.set_xlabel('MAE')\n",
        "    ax3.set_ylabel('Frequency')\n",
        "    ax3.set_title('Distribution of Individual Model MAE')\n",
        "else:\n",
        "    ax3.text(0.5, 0.5, 'No performance data available', ha='center', va='center', transform=ax3.transAxes)\n",
        "\n",
        "# Seasonal order distribution\n",
        "if seasonal_orders:\n",
        "    seasonal_list = list(seasonal_orders.keys())[:8]\n",
        "    seasonal_counts = [seasonal_orders[order] for order in seasonal_list]\n",
        "    ax4.bar(range(len(seasonal_list)), seasonal_counts, color='coral')\n",
        "    ax4.set_xlabel('Seasonal Order')\n",
        "    ax4.set_ylabel('Number of Models')\n",
        "    ax4.set_title('Distribution of Seasonal Orders')\n",
        "    ax4.set_xticks(range(len(seasonal_list)))\n",
        "    ax4.set_xticklabels(seasonal_list, rotation=45)\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, 'No seasonal orders found', ha='center', va='center', transform=ax4.transAxes)\n",
        "    ax4.set_title('Seasonal Order Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('final_arima_analysis.png', dpi=300, bbox_inches='tight')\n",
        "wandb.log({\"final_feature_importance\": wandb.Image('final_arima_analysis.png')})\n",
        "plt.show()\n",
        "\n",
        "# Log analysis results\n",
        "if not importance_df.empty:\n",
        "    wandb.log({\n",
        "        'top_10_features': importance_df.head(10)['Feature'].tolist(),\n",
        "        'top_features_table': wandb.Table(dataframe=importance_df.head(10)),\n",
        "        'total_features': len(feature_names)\n",
        "    })\n",
        "\n",
        "wandb.log({\n",
        "    'total_models_fitted': total_models,\n",
        "    'unique_arima_orders': len(model_orders),\n",
        "    'unique_seasonal_orders': len(seasonal_orders),\n",
        "    'avg_individual_mae': np.mean(individual_maes) if individual_maes else 0,\n",
        "    'std_individual_mae': np.std(individual_maes) if individual_maes else 0\n",
        "})"
      ],
      "metadata": {
        "id": "v7_RyGK2DZI9"
      },
      "id": "v7_RyGK2DZI9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL PERFORMANCE VISUALIZATION"
      ],
      "metadata": {
        "id": "S7eSWprurZGM"
      },
      "id": "S7eSWprurZGM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Model performance visualization\n",
        "print(\"\\n=== MODEL PERFORMANCE VISUALIZATION ===\")\n",
        "\n",
        "# Create performance plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Actual vs Predicted (sample)\n",
        "sample_size = min(5000, len(y_final_aligned))\n",
        "sample_indices = np.random.choice(len(y_final_aligned), size=sample_size, replace=False)\n",
        "sample_actual = y_final_aligned[sample_indices]\n",
        "sample_pred = y_pred_final[sample_indices]\n",
        "\n",
        "axes[0, 0].scatter(sample_actual, sample_pred, alpha=0.5, s=1)\n",
        "axes[0, 0].plot([y_final_aligned.min(), y_final_aligned.max()], [y_final_aligned.min(), y_final_aligned.max()], 'r--', lw=2)\n",
        "axes[0, 0].set_xlabel('Actual Sales')\n",
        "axes[0, 0].set_ylabel('Predicted Sales')\n",
        "axes[0, 0].set_title(f'Actual vs Predicted Sales (n={sample_size})')\n",
        "\n",
        "# 2. Residuals\n",
        "residuals = y_final_aligned - y_pred_final\n",
        "axes[0, 1].scatter(y_pred_final[sample_indices], residuals[sample_indices], alpha=0.5, s=1)\n",
        "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
        "axes[0, 1].set_xlabel('Predicted Sales')\n",
        "axes[0, 1].set_ylabel('Residuals')\n",
        "axes[0, 1].set_title('Residual Plot')\n",
        "\n",
        "# 3. Residual distribution\n",
        "axes[1, 0].hist(residuals, bins=100, alpha=0.7, color='lightgreen')\n",
        "axes[1, 0].set_xlabel('Residuals')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Distribution of Residuals')\n",
        "\n",
        "# 4. Performance by individual models\n",
        "if individual_maes:\n",
        "    model_names = [f\"Model {i+1}\" for i in range(len(individual_maes))]\n",
        "    sample_models = min(20, len(individual_maes))\n",
        "\n",
        "    axes[1, 1].bar(range(sample_models), sorted(individual_maes)[:sample_models], color='coral')\n",
        "    axes[1, 1].set_xlabel('Individual Models (sorted by MAE)')\n",
        "    axes[1, 1].set_ylabel('MAE')\n",
        "    axes[1, 1].set_title(f'MAE by Individual ARIMA Models (Best {sample_models})')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "else:\n",
        "    axes[1, 1].text(0.5, 0.5, 'No individual model data', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('final_model_performance.png', dpi=300, bbox_inches='tight')\n",
        "wandb.log({\"final_model_performance\": wandb.Image('final_model_performance.png')})\n",
        "plt.show()\n",
        "\n",
        "# Residual statistics\n",
        "print(f\"\\nResidual Analysis:\")\n",
        "print(f\"Residual Mean: {residuals.mean():.2f}\")\n",
        "print(f\"Residual Std: {residuals.std():.2f}\")\n",
        "print(f\"Residual Skewness: {residuals.skew():.3f}\")\n",
        "print(f\"Residual Kurtosis: {residuals.kurtosis():.3f}\")\n",
        "\n",
        "wandb.log({\n",
        "    'residual_mean': residuals.mean(),\n",
        "    'residual_std': residuals.std(),\n",
        "    'residual_skewness': residuals.skew(),\n",
        "    'residual_kurtosis': residuals.kurtosis()\n",
        "})"
      ],
      "metadata": {
        "id": "4l9xCSScDbq6"
      },
      "id": "4l9xCSScDbq6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING FINAL MODEL"
      ],
      "metadata": {
        "id": "FkurZo9srbtV"
      },
      "id": "FkurZo9srbtV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final model\n",
        "print(\"\\n=== SAVING FINAL MODEL ===\")\n",
        "\n",
        "# Fix MAPE calculation (handle zero values)\n",
        "def safe_mape(y_true, y_pred):\n",
        "    \"\"\"Calculate MAPE while handling zero values\"\"\"\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() == 0:\n",
        "        return float('inf')\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "train_mape_fixed = safe_mape(y_final_aligned, y_pred_final)\n",
        "print(f\"Corrected Training MAPE: {train_mape_fixed:.2f}%\")\n",
        "\n",
        "# Save pipeline with cloudpickle (handles custom classes better)\n",
        "try:\n",
        "    import cloudpickle\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'cloudpickle'])\n",
        "    import cloudpickle\n",
        "\n",
        "# Save both preprocessor and model\n",
        "pipeline_filename = f\"arima_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
        "\n",
        "# Create save object with both components\n",
        "pipeline_save_object = {\n",
        "    'preprocessor': final_preprocessor,\n",
        "    'model': final_model,\n",
        "    'ts_data_dict': ts_data_dict,\n",
        "    'performance_metrics': {\n",
        "        'train_mae': train_mae,\n",
        "        'train_rmse': train_rmse,\n",
        "        'train_mape': train_mape_fixed,\n",
        "        'train_r2': train_r2\n",
        "    },\n",
        "    'metadata': {\n",
        "        'successful_fits': successful_fits,\n",
        "        'failed_fits': failed_fits,\n",
        "        'total_models': total_models,\n",
        "        'exog_variables': feature_names,\n",
        "        'created_date': datetime.now().isoformat()\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save with cloudpickle\n",
        "with open(pipeline_filename, 'wb') as f:\n",
        "    cloudpickle.dump(pipeline_save_object, f)\n",
        "\n",
        "print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "\n",
        "# Try to upload to wandb with error handling\n",
        "try:\n",
        "    # Create model artifact with fixed metadata\n",
        "    model_artifact = wandb.Artifact(\n",
        "        name=\"ARIMA_pipeline\",\n",
        "        type=\"model\",\n",
        "        description=\"Final ARIMA pipeline for Walmart sales forecasting\",\n",
        "        metadata={\n",
        "            \"train_mae\": float(train_mae),\n",
        "            \"train_rmse\": float(train_rmse),\n",
        "            \"train_mape\": float(train_mape_fixed) if not np.isinf(train_mape_fixed) else 0.0,\n",
        "            \"train_r2\": float(train_r2),\n",
        "            \"features_count\": len(feature_names),\n",
        "            \"training_samples\": len(y_final_aligned),\n",
        "            \"model_type\": \"ARIMA\",\n",
        "            \"individual_models\": total_models,\n",
        "            \"successful_fits\": successful_fits\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add model file to artifact\n",
        "    model_artifact.add_file(pipeline_filename)\n",
        "\n",
        "    # Log artifact\n",
        "    wandb.log_artifact(model_artifact)\n",
        "    print(\"✓ Model artifact logged to wandb successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error uploading to wandb: {e}\")\n",
        "    print(\"Model saved locally - you can manually upload later\")\n",
        "\n",
        "    # Log just the metrics without artifact\n",
        "    wandb.log({\n",
        "        'final_train_mae': train_mae,\n",
        "        'final_train_rmse': train_rmse,\n",
        "        'final_train_mape': train_mape_fixed if not np.isinf(train_mape_fixed) else 0.0,\n",
        "        'final_train_r2': train_r2,\n",
        "        'model_saved_locally': pipeline_filename\n",
        "    })\n",
        "\n",
        "# Final summary with corrected MAPE\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL MODEL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model Type: ARIMA Ensemble\")\n",
        "print(f\"Training Samples: {len(y_final_aligned):,}\")\n",
        "print(f\"Individual Models: {total_models}\")\n",
        "print(f\"Successful Fits: {successful_fits}\")\n",
        "print(f\"Features: {len(feature_names)}\")\n",
        "print(f\"Training MAE: {train_mae:.2f}\")\n",
        "print(f\"Training RMSE: {train_rmse:.2f}\")\n",
        "print(f\"Training MAPE: {train_mape_fixed:.2f}%\")\n",
        "print(f\"Training R²: {train_r2:.4f}\")\n",
        "print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "yGvslcHpDcsr"
      },
      "id": "yGvslcHpDcsr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}