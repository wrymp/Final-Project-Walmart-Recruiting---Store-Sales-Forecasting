{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrymp/Final-Project-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_NBEATS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "title_cell"
      },
      "cell_type": "markdown",
      "source": [
        "# N-BEATS Implementation for Walmart Sales Forecasting\n",
        "\n",
        "This notebook implements N-BEATS (Neural Basis Expansion Analysis for Time Series) for Walmart sales forecasting following the exact pipeline structure from fraud detection experiments."
      ],
      "id": "title_cell"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "!pip install wandb -q\n",
        "!pip install kaggle -q\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugu4ciGwnvFv",
        "outputId": "47e000cb-11f0-41e1-9aeb-85a1c1d9f6d8"
      },
      "id": "Ugu4ciGwnvFv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "# ! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# ! unzip /content/train.csv.zip\n",
        "# ! unzip /content/test.csv.zip\n",
        "# ! unzip /content/features.csv.zip\n",
        "# ! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "id": "0Y9pdn2FpeCC"
      },
      "id": "0Y9pdn2FpeCC",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "setup_cell"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup & Imports"
      ],
      "id": "setup_cell"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_imports",
        "outputId": "e07b8397-806f-4bf4-e897-b6e39386d7db"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import gc\n",
        "import os\n",
        "import pickle\n",
        "import cloudpickle\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 3,
      "id": "setup_imports"
    },
    {
      "metadata": {
        "id": "wandb_setup"
      },
      "cell_type": "markdown",
      "source": [
        "# Wandb Initialization"
      ],
      "id": "wandb_setup"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "wandb_init",
        "outputId": "094e05bf-e3a6-4296-bb18-f04dad1bf517"
      },
      "cell_type": "code",
      "source": [
        "# Initialize Wandb project\n",
        "wandb.login()\n",
        "try:\n",
        "    wandb.init(\n",
        "        project=\"walmart-sales-forecasting\",\n",
        "        name=\"NBEATS_Initial_Setup\",\n",
        "        config={\n",
        "            \"model_type\": \"NBEATS\",\n",
        "            \"framework\": \"PyTorch\",\n",
        "            \"device\": str(device),\n",
        "            \"random_seed\": 42\n",
        "        }\n",
        "    )\n",
        "    print(\"✓ Wandb initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Wandb initialization failed: {e}\")\n",
        "    print(\"Continuing without wandb logging...\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqitiashvili13\u001b[0m (\u001b[33mdshan21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_230350-wtrylkqq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/wtrylkqq' target=\"_blank\">NBEATS_Initial_Setup</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/wtrylkqq' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/wtrylkqq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Wandb initialized successfully!\n"
          ]
        }
      ],
      "execution_count": 4,
      "id": "wandb_init"
    },
    {
      "metadata": {
        "id": "data_loading"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "id": "data_loading"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "data_loading_code",
        "outputId": "18fc3765-8430-4986-9668-eb1a4fc68b77"
      },
      "cell_type": "code",
      "source": [
        "# Load Walmart datasets\n",
        "print(\"Loading Walmart datasets...\")\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/walmart-recruiting-store-sales-forecasting/train.csv/train.csv')\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/walmart-recruiting-store-sales-forecasting/test.csv/test.csv')\n",
        "    stores_df = pd.read_csv('/content/drive/MyDrive/walmart-recruiting-store-sales-forecasting/stores.csv')\n",
        "    features_df = pd.read_csv('/content/drive/MyDrive/walmart-recruiting-store-sales-forecasting/features.csv/features.csv')\n",
        "\n",
        "    print(f\"✓ Train data shape: {train_df.shape}\")\n",
        "    print(f\"✓ Test data shape: {test_df.shape}\")\n",
        "    print(f\"✓ Stores data shape: {stores_df.shape}\")\n",
        "    print(f\"✓ Features data shape: {features_df.shape}\")\n",
        "\n",
        "    # Log basic dataset info\n",
        "    wandb.log({\n",
        "        \"train_samples\": len(train_df),\n",
        "        \"test_samples\": len(test_df),\n",
        "        \"num_stores\": stores_df['Store'].nunique(),\n",
        "        \"num_departments\": train_df['Dept'].nunique(),\n",
        "        \"date_range_train\": f\"{train_df['Date'].min()} to {train_df['Date'].max()}\"\n",
        "    })\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    print(\"Please ensure data files are in './data/' directory\")\n",
        "    raise"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Walmart datasets...\n",
            "✓ Train data shape: (421570, 5)\n",
            "✓ Test data shape: (115064, 4)\n",
            "✓ Stores data shape: (45, 3)\n",
            "✓ Features data shape: (8190, 12)\n"
          ]
        }
      ],
      "execution_count": 8,
      "id": "data_loading_code"
    },
    {
      "metadata": {
        "id": "exploration_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Exploration Run"
      ],
      "id": "exploration_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "exploration_code",
        "outputId": "f00fcd3b-1954-4b86-9d59-8d7566d86520"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for exploration\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"NBEATS_Exploration\",\n",
        "    config={\"stage\": \"exploration\"}\n",
        ")\n",
        "\n",
        "print(\"\\n=== DATA EXPLORATION ===\")\n",
        "\n",
        "# Convert date columns\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nTrain Data Info:\")\n",
        "print(f\"Date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "print(f\"Unique stores: {train_df['Store'].nunique()}\")\n",
        "print(f\"Unique departments: {train_df['Dept'].nunique()}\")\n",
        "print(f\"Total store-dept combinations: {train_df.groupby(['Store', 'Dept']).ngroups}\")\n",
        "\n",
        "# Sales statistics\n",
        "print(f\"\\nSales Statistics:\")\n",
        "print(f\"Mean weekly sales: ${train_df['Weekly_Sales'].mean():,.2f}\")\n",
        "print(f\"Median weekly sales: ${train_df['Weekly_Sales'].median():,.2f}\")\n",
        "print(f\"Min weekly sales: ${train_df['Weekly_Sales'].min():,.2f}\")\n",
        "print(f\"Max weekly sales: ${train_df['Weekly_Sales'].max():,.2f}\")\n",
        "\n",
        "# Holiday impact\n",
        "holiday_sales = train_df.groupby('IsHoliday')['Weekly_Sales'].agg(['mean', 'count'])\n",
        "print(f\"\\nHoliday Impact:\")\n",
        "print(holiday_sales)\n",
        "\n",
        "# Store types\n",
        "store_types = stores_df['Type'].value_counts()\n",
        "print(f\"\\nStore Types:\")\n",
        "print(store_types)\n",
        "\n",
        "# Missing values in features\n",
        "print(f\"\\nMissing Values in Features:\")\n",
        "missing_pct = (features_df.isnull().sum() / len(features_df)) * 100\n",
        "print(missing_pct[missing_pct > 0].sort_values(ascending=False))\n",
        "\n",
        "# Log exploration metrics\n",
        "wandb.log({\n",
        "    \"unique_stores\": train_df['Store'].nunique(),\n",
        "    \"unique_departments\": train_df['Dept'].nunique(),\n",
        "    \"total_timeseries\": train_df.groupby(['Store', 'Dept']).ngroups,\n",
        "    \"avg_weekly_sales\": train_df['Weekly_Sales'].mean(),\n",
        "    \"median_weekly_sales\": train_df['Weekly_Sales'].median(),\n",
        "    \"sales_std\": train_df['Weekly_Sales'].std(),\n",
        "    \"holiday_sales_boost\": holiday_sales.loc[True, 'mean'] / holiday_sales.loc[False, 'mean'],\n",
        "    \"missing_markdown1_pct\": missing_pct['MarkDown1'],\n",
        "    \"missing_markdown2_pct\": missing_pct['MarkDown2'],\n",
        "    \"missing_markdown3_pct\": missing_pct['MarkDown3'],\n",
        "    \"missing_markdown4_pct\": missing_pct['MarkDown4'],\n",
        "    \"missing_markdown5_pct\": missing_pct['MarkDown5']\n",
        "})\n",
        "\n",
        "print(\"\\n✓ Exploration completed and logged to wandb\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>num_departments</td><td>▁▁</td></tr><tr><td>num_stores</td><td>▁▁</td></tr><tr><td>test_samples</td><td>▁▁</td></tr><tr><td>train_samples</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date_range_train</td><td>2010-02-05 to 2012-1...</td></tr><tr><td>num_departments</td><td>81</td></tr><tr><td>num_stores</td><td>45</td></tr><tr><td>test_samples</td><td>115064</td></tr><tr><td>train_samples</td><td>421570</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">NBEATS_Initial_Setup</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/wtrylkqq' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/wtrylkqq</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_230350-wtrylkqq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_230531-jbs39ijv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/jbs39ijv' target=\"_blank\">NBEATS_Exploration</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/jbs39ijv' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/jbs39ijv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA EXPLORATION ===\n",
            "\n",
            "Train Data Info:\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Unique stores: 45\n",
            "Unique departments: 81\n",
            "Total store-dept combinations: 3331\n",
            "\n",
            "Sales Statistics:\n",
            "Mean weekly sales: $15,981.26\n",
            "Median weekly sales: $7,612.03\n",
            "Min weekly sales: $-4,988.94\n",
            "Max weekly sales: $693,099.36\n",
            "\n",
            "Holiday Impact:\n",
            "                   mean   count\n",
            "IsHoliday                      \n",
            "False      15901.445069  391909\n",
            "True       17035.823187   29661\n",
            "\n",
            "Store Types:\n",
            "Type\n",
            "A    22\n",
            "B    17\n",
            "C     6\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Missing Values in Features:\n",
            "MarkDown2       64.334554\n",
            "MarkDown4       57.704518\n",
            "MarkDown3       55.885226\n",
            "MarkDown1       50.769231\n",
            "MarkDown5       50.549451\n",
            "CPI              7.142857\n",
            "Unemployment     7.142857\n",
            "dtype: float64\n",
            "\n",
            "✓ Exploration completed and logged to wandb\n"
          ]
        }
      ],
      "execution_count": 9,
      "id": "exploration_code"
    },
    {
      "metadata": {
        "id": "transformers_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Custom Transformers for Time-Series Pipeline"
      ],
      "id": "transformers_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "transformers_code",
        "outputId": "3cba21ed-fb5a-49da-ddf9-817989d8821d"
      },
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataProcessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Processes raw Walmart data into time-series format for N-BEATS\"\"\"\n",
        "\n",
        "    def __init__(self, lookback_window=52, forecast_horizon=1):\n",
        "        self.lookback_window = lookback_window  # 52 weeks = 1 year\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.store_dept_combinations = None\n",
        "        self.date_range = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Learn the store-department combinations and date range\"\"\"\n",
        "        # X should be merged dataframe with all info\n",
        "        self.store_dept_combinations = X.groupby(['Store', 'Dept']).size().index.tolist()\n",
        "        self.date_range = sorted(X['Date'].unique())\n",
        "        print(f\"Found {len(self.store_dept_combinations)} store-dept combinations\")\n",
        "        print(f\"Date range: {self.date_range[0]} to {self.date_range[-1]}\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "          \"\"\"Transform data into sequences for N-BEATS\"\"\"\n",
        "          sequences = []\n",
        "          targets = []\n",
        "          metadata = []\n",
        "\n",
        "          for store, dept in self.store_dept_combinations:\n",
        "              # Get time series for this store-dept combination\n",
        "              series_data = X[(X['Store'] == store) & (X['Dept'] == dept)].copy()\n",
        "              series_data = series_data.sort_values('Date')\n",
        "\n",
        "              if len(series_data) < self.lookback_window + self.forecast_horizon:\n",
        "                  continue  # Skip if not enough data\n",
        "\n",
        "              # Create sliding windows\n",
        "              for i in range(len(series_data) - self.lookback_window - self.forecast_horizon + 1):\n",
        "                  # Input sequence (features + target)\n",
        "                  window_data = series_data.iloc[i:i + self.lookback_window]\n",
        "\n",
        "                  # Sales sequence (target variable)\n",
        "                  sales_sequence = window_data['Weekly_Sales'].values\n",
        "\n",
        "                  # Check for valid sales data\n",
        "                  if np.any(np.isnan(sales_sequence)) or np.any(np.isinf(sales_sequence)):\n",
        "                      continue\n",
        "\n",
        "                  # External features (if available)\n",
        "                  external_features = []\n",
        "                  if 'Temperature' in window_data.columns:\n",
        "                      temp_vals = window_data['Temperature'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "                      external_features.append(temp_vals.values)\n",
        "                  if 'Fuel_Price' in window_data.columns:\n",
        "                      fuel_vals = window_data['Fuel_Price'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "                      external_features.append(fuel_vals.values)\n",
        "                  if 'CPI' in window_data.columns:\n",
        "                      cpi_vals = window_data['CPI'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "                      external_features.append(cpi_vals.values)\n",
        "                  if 'Unemployment' in window_data.columns:\n",
        "                      unemp_vals = window_data['Unemployment'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "                      external_features.append(unemp_vals.values)\n",
        "\n",
        "                  # Combine sales with external features\n",
        "                  if external_features:\n",
        "                      try:\n",
        "                          feature_matrix = np.column_stack([sales_sequence] + external_features)\n",
        "                      except ValueError:\n",
        "                          # If dimensions don't match, use only sales\n",
        "                          feature_matrix = sales_sequence.reshape(-1, 1)\n",
        "                  else:\n",
        "                      feature_matrix = sales_sequence.reshape(-1, 1)\n",
        "\n",
        "                  sequences.append(feature_matrix)\n",
        "\n",
        "                  # Target (next forecast_horizon values)\n",
        "                  target_data = series_data.iloc[i + self.lookback_window:i + self.lookback_window + self.forecast_horizon]\n",
        "                  target_sales = target_data['Weekly_Sales'].values\n",
        "\n",
        "                  # Check for valid target data\n",
        "                  if np.any(np.isnan(target_sales)) or np.any(np.isinf(target_sales)):\n",
        "                      continue\n",
        "\n",
        "                  targets.append(target_sales)\n",
        "\n",
        "            # Metadata\n",
        "                  metadata.append({\n",
        "                      'store': store,\n",
        "                      'dept': dept,\n",
        "                      'start_date': window_data['Date'].iloc[0],\n",
        "                      'end_date': window_data['Date'].iloc[-1],\n",
        "                      'forecast_date': target_data['Date'].iloc[0] if len(target_data) > 0 else None\n",
        "                  })\n",
        "\n",
        "          print(f\"Generated {len(sequences)} valid sequences from {len(self.store_dept_combinations)} store-dept combinations\")\n",
        "\n",
        "          return {\n",
        "              'sequences': np.array(sequences, dtype=object),\n",
        "              'targets': np.array(targets, dtype=object),\n",
        "              'metadata': metadata\n",
        "          }\n",
        "\n",
        "class FeatureMerger(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Merges train/test data with stores and features data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stores_data = None\n",
        "        self.features_data = None\n",
        "\n",
        "    def fit(self, X, y=None, stores_df=None, features_df=None):\n",
        "        \"\"\"Store the auxiliary dataframes\"\"\"\n",
        "        self.stores_data = stores_df.copy() if stores_df is not None else None\n",
        "        self.features_data = features_df.copy() if features_df is not None else None\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Merge main data with stores and features\"\"\"\n",
        "        result = X.copy()\n",
        "\n",
        "        # Merge with stores data\n",
        "        if self.stores_data is not None:\n",
        "            result = result.merge(self.stores_data, on='Store', how='left')\n",
        "\n",
        "        # Merge with features data\n",
        "        if self.features_data is not None:\n",
        "            result = result.merge(self.features_data, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        return result\n",
        "\n",
        "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Handle missing values in time-series data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Learn fill values for missing data\"\"\"\n",
        "        # For MarkDown columns, fill with 0 (no markdown)\n",
        "        markdown_cols = [col for col in X.columns if 'MarkDown' in col]\n",
        "        for col in markdown_cols:\n",
        "            self.fill_values[col] = 0.0\n",
        "\n",
        "        # For other numerical columns, use median\n",
        "        numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        for col in numerical_cols:\n",
        "            if col not in self.fill_values and X[col].isnull().any():\n",
        "                self.fill_values[col] = X[col].median()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Fill missing values\"\"\"\n",
        "        result = X.copy()\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in result.columns:\n",
        "                result[col] = result[col].fillna(fill_value)\n",
        "        return result\n",
        "\n",
        "print(\"✓ Custom transformers defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Custom transformers defined\n"
          ]
        }
      ],
      "execution_count": 46,
      "id": "transformers_code"
    },
    {
      "metadata": {
        "id": "nbeats_model"
      },
      "cell_type": "markdown",
      "source": [
        "# N-BEATS Model Implementation"
      ],
      "id": "nbeats_model"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbeats_model_code",
        "outputId": "5d34cf70-3a4e-48bb-f525-0c2341159891"
      },
      "cell_type": "code",
      "source": [
        "class NBeatsBlock(nn.Module):\n",
        "    \"\"\"Single N-BEATS block\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, theta_size, basis_size, num_layers=4, layer_size=512):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.theta_size = theta_size\n",
        "        self.basis_size = basis_size\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        self.fc_layers.append(nn.Linear(input_size, layer_size))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.fc_layers.append(nn.Linear(layer_size, layer_size))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Output layers\n",
        "        self.theta_b = nn.Linear(layer_size, theta_size)  # Backcast coefficients\n",
        "        self.theta_f = nn.Linear(layer_size, theta_size)  # Forecast coefficients\n",
        "\n",
        "        print(f\"NBeatsBlock initialized - input_size: {input_size}, layer_size: {layer_size}, theta_size: {theta_size}\")\n",
        "        print(f\"NBeatsBlock theta_b weight shape (in __init__): {self.theta_b.weight.shape}\") # Debug print\n",
        "        print(f\"NBeatsBlock theta_f weight shape (in __init__): {self.theta_f.weight.shape}\") # Debug print\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(f\"NBeatsBlock input shape (in forward): {x.shape}\") # Debug print\n",
        "        for i, layer in enumerate(self.fc_layers):\n",
        "            x = self.relu(layer(x))\n",
        "            # print(f\"NBeatsBlock after layer {i} shape (in forward): {x.shape}\") # Debug print\n",
        "\n",
        "        # print(f\"Shape before theta_b: {x.shape}, theta_b weight shape: {self.theta_b.weight.shape}\") # Debug print before multiplication\n",
        "        theta_b = self.theta_b(x)\n",
        "        # print(f\"NBeatsBlock output theta_b shape (in forward): {theta_b.shape}\") # Debug print\n",
        "\n",
        "        # print(f\"Shape before theta_f: {x.shape}, theta_f weight shape: {self.theta_f.weight.shape}\") # Debug print before multiplication\n",
        "        theta_f = self.theta_f(x)\n",
        "        # print(f\"NBeatsBlock output theta_f shape (in forward): {theta_f.shape}\") # Debug print\n",
        "        return theta_b, theta_f\n",
        "\n",
        "class GenericBasis(nn.Module):\n",
        "    \"\"\"Generic basis functions for N-BEATS\"\"\"\n",
        "\n",
        "    def __init__(self, backcast_size, forecast_size):\n",
        "        super().__init__()\n",
        "        self.backcast_size = backcast_size\n",
        "        self.forecast_size = forecast_size\n",
        "\n",
        "    def forward(self, theta_b, theta_f):\n",
        "        # For generic basis, theta directly represents the values\n",
        "        backcast = theta_b[:, :self.backcast_size]\n",
        "        forecast = theta_f[:, :self.forecast_size]\n",
        "        return backcast, forecast\n",
        "\n",
        "class NBEATSModel(nn.Module):\n",
        "    \"\"\"N-BEATS model with multiple stacks and blocks\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, forecast_size, num_stacks=2, num_blocks_per_stack=3,\n",
        "                 num_layers=4, layer_size=512, num_features=1):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.forecast_size = forecast_size\n",
        "        self.num_features = num_features\n",
        "\n",
        "        # If we have multiple features, add a projection layer\n",
        "        if num_features > 1:\n",
        "            self.feature_projection = nn.Linear(num_features, 1)\n",
        "        else:\n",
        "            self.feature_projection = None\n",
        "\n",
        "        # Create stacks of blocks\n",
        "        self.stacks = nn.ModuleList()\n",
        "\n",
        "        for stack_id in range(num_stacks):\n",
        "            stack = nn.ModuleList()\n",
        "            for block_id in range(num_blocks_per_stack):\n",
        "                block = NBeatsBlock(\n",
        "                    input_size=input_size,\n",
        "                    theta_size=max(input_size, forecast_size),\n",
        "                    basis_size=max(input_size, forecast_size),\n",
        "                    num_layers=num_layers,\n",
        "                    layer_size=layer_size\n",
        "                )\n",
        "                stack.append(block)\n",
        "            self.stacks.append(stack)\n",
        "\n",
        "        # Basis functions\n",
        "        self.basis = GenericBasis(input_size, forecast_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, num_features)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # If multiple features, project to univariate\n",
        "        if self.feature_projection is not None:\n",
        "            # Take only the sales feature (first column) for now\n",
        "            x = x[:, :, 0]  # Shape: (batch_size, sequence_length)\n",
        "        else:\n",
        "            x = x.squeeze(-1)  # Remove last dimension if single feature\n",
        "\n",
        "        residual = x\n",
        "        forecast = torch.zeros(batch_size, self.forecast_size, device=x.device)\n",
        "\n",
        "        # Process through stacks\n",
        "        for stack in self.stacks:\n",
        "            for block in stack:\n",
        "                theta_b, theta_f = block(residual)\n",
        "                backcast, block_forecast = self.basis(theta_b, theta_f)\n",
        "\n",
        "                # Ensure correct dimensions\n",
        "                if backcast.size(1) != residual.size(1):\n",
        "                    backcast = backcast[:, :residual.size(1)]\n",
        "                if block_forecast.size(1) != self.forecast_size:\n",
        "                    block_forecast = block_forecast[:, :self.forecast_size]\n",
        "\n",
        "                residual = residual - backcast\n",
        "                forecast = forecast + block_forecast\n",
        "\n",
        "        return forecast\n",
        "\n",
        "class WalmartDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for Walmart time series data\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, targets, device='cpu'):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "        self.device = device # Store the device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self, idx):\n",
        "          # Create tensors on CPU - device transfer happens in training loop\n",
        "          sequence = torch.FloatTensor(self.sequences[idx])\n",
        "          target = torch.FloatTensor(self.targets[idx])\n",
        "          return sequence, target\n",
        "\n",
        "print(\"✓ N-BEATS model architecture defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ N-BEATS model architecture defined\n"
          ]
        }
      ],
      "execution_count": 48,
      "id": "nbeats_model_code"
    },
    {
      "metadata": {
        "id": "cleaning_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning Run"
      ],
      "id": "cleaning_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "cleaning_code",
        "outputId": "3f1e3466-a2fe-4675-de2d-f3231604f2a3"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for data cleaning\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"NBEATS_Cleaning\",\n",
        "    config={\"stage\": \"cleaning\"}\n",
        ")\n",
        "\n",
        "print(\"\\n=== DATA CLEANING ===\")\n",
        "\n",
        "# Create feature merger and missing value handler\n",
        "feature_merger = FeatureMerger()\n",
        "missing_handler = MissingValueHandler()\n",
        "\n",
        "# Fit the merger with auxiliary data\n",
        "feature_merger.fit(train_df, stores_df=stores_df, features_df=features_df)\n",
        "\n",
        "# Merge train data with stores and features\n",
        "print(\"Merging train data with stores and features...\")\n",
        "train_merged = feature_merger.transform(train_df)\n",
        "print(f\"Train data shape after merging: {train_merged.shape}\")\n",
        "\n",
        "# Fit and transform missing values\n",
        "print(\"Handling missing values...\")\n",
        "missing_handler.fit(train_merged)\n",
        "train_cleaned = missing_handler.transform(train_merged)\n",
        "\n",
        "# Check for remaining missing values\n",
        "remaining_missing = train_cleaned.isnull().sum()\n",
        "remaining_missing = remaining_missing[remaining_missing > 0]\n",
        "\n",
        "print(f\"\\nRemaining missing values after cleaning:\")\n",
        "if len(remaining_missing) > 0:\n",
        "    print(remaining_missing)\n",
        "else:\n",
        "    print(\"No missing values remaining!\")\n",
        "\n",
        "# Basic data quality checks\n",
        "print(f\"\\nData quality checks:\")\n",
        "print(f\"Total records: {len(train_cleaned):,}\")\n",
        "print(f\"Date range: {train_cleaned['Date'].min()} to {train_cleaned['Date'].max()}\")\n",
        "print(f\"Unique store-dept combinations: {train_cleaned.groupby(['Store', 'Dept']).ngroups:,}\")\n",
        "\n",
        "# Check for negative sales (data quality issue)\n",
        "negative_sales = (train_cleaned['Weekly_Sales'] < 0).sum()\n",
        "print(f\"Records with negative sales: {negative_sales:,} ({negative_sales/len(train_cleaned)*100:.2f}%)\")\n",
        "\n",
        "# Log cleaning metrics\n",
        "wandb.log({\n",
        "    \"cleaned_records\": len(train_cleaned),\n",
        "    \"remaining_missing_values\": len(remaining_missing),\n",
        "    \"negative_sales_count\": int(negative_sales),\n",
        "    \"negative_sales_pct\": float(negative_sales/len(train_cleaned)*100),\n",
        "    \"store_dept_combinations\": train_cleaned.groupby(['Store', 'Dept']).ngroups\n",
        "})\n",
        "\n",
        "print(\"\\n✓ Data cleaning completed and logged to wandb\")\n",
        "\n",
        "# Save cleaned data for next steps\n",
        "print(\"\\nSample of cleaned data:\")\n",
        "print(train_cleaned.head())\n",
        "print(f\"\\nColumns: {list(train_cleaned.columns)}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">NBEATS_Final_Training</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/5t3qpcj1' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/5t3qpcj1</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_234302-5t3qpcj1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_234427-nbm6ux3y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/nbm6ux3y' target=\"_blank\">NBEATS_Cleaning</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/nbm6ux3y' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/nbm6ux3y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA CLEANING ===\n",
            "Merging train data with stores and features...\n",
            "Train data shape after merging: (421570, 17)\n",
            "Handling missing values...\n",
            "\n",
            "Remaining missing values after cleaning:\n",
            "No missing values remaining!\n",
            "\n",
            "Data quality checks:\n",
            "Total records: 421,570\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Unique store-dept combinations: 3,331\n",
            "Records with negative sales: 1,285 (0.30%)\n",
            "\n",
            "✓ Data cleaning completed and logged to wandb\n",
            "\n",
            "Sample of cleaned data:\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday_x Type    Size  \\\n",
            "0      1     1 2010-02-05      24924.50        False    A  151315   \n",
            "1      1     1 2010-02-12      46039.49         True    A  151315   \n",
            "2      1     1 2010-02-19      41595.55        False    A  151315   \n",
            "3      1     1 2010-02-26      19403.54        False    A  151315   \n",
            "4      1     1 2010-03-05      21827.90        False    A  151315   \n",
            "\n",
            "   Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  \\\n",
            "0        42.31       2.572        0.0        0.0        0.0        0.0   \n",
            "1        38.51       2.548        0.0        0.0        0.0        0.0   \n",
            "2        39.93       2.514        0.0        0.0        0.0        0.0   \n",
            "3        46.63       2.561        0.0        0.0        0.0        0.0   \n",
            "4        46.50       2.625        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   MarkDown5         CPI  Unemployment  IsHoliday_y  \n",
            "0        0.0  211.096358         8.106        False  \n",
            "1        0.0  211.242170         8.106         True  \n",
            "2        0.0  211.289143         8.106        False  \n",
            "3        0.0  211.319643         8.106        False  \n",
            "4        0.0  211.350143         8.106        False  \n",
            "\n",
            "Columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday_x', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y']\n"
          ]
        }
      ],
      "execution_count": 49,
      "id": "cleaning_code"
    },
    {
      "metadata": {
        "id": "feature_selection_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Selection Run"
      ],
      "id": "feature_selection_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "feature_selection_code",
        "outputId": "29cbec1f-dce1-402c-b39b-e4696f4aeaf1"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for feature selection\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"NBEATS_Feature_Selection\",\n",
        "    config={\"stage\": \"feature_selection\"}\n",
        ")\n",
        "\n",
        "print(\"\\n=== FEATURE SELECTION ===\")\n",
        "\n",
        "# For N-BEATS, we'll focus on selecting the most relevant external features\n",
        "# The main target will always be Weekly_Sales\n",
        "\n",
        "# Define feature categories\n",
        "core_features = ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday_x'] # Updated to IsHoliday_x\n",
        "store_features = ['Type', 'Size']\n",
        "economic_features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "markdown_features = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "print(f\"Available columns: {list(train_cleaned.columns)}\")\n",
        "\n",
        "# Analyze correlation between external features and sales\n",
        "correlation_analysis = {}\n",
        "\n",
        "# Economic features correlation\n",
        "for feature in economic_features:\n",
        "    if feature in train_cleaned.columns:\n",
        "        corr = train_cleaned['Weekly_Sales'].corr(train_cleaned[feature])\n",
        "        correlation_analysis[feature] = corr\n",
        "        print(f\"Correlation between Weekly_Sales and {feature}: {corr:.4f}\")\n",
        "\n",
        "# Markdown features correlation (only for periods where they exist)\n",
        "markdown_period = train_cleaned[train_cleaned['Date'] >= '2011-11-01']  # MarkDowns available after Nov 2011\n",
        "for feature in markdown_features:\n",
        "    if feature in markdown_period.columns:\n",
        "        # Only calculate correlation where markdown is not null\n",
        "        valid_data = markdown_period.dropna(subset=[feature])\n",
        "        if len(valid_data) > 0:\n",
        "            corr = valid_data['Weekly_Sales'].corr(valid_data[feature])\n",
        "            correlation_analysis[feature] = corr\n",
        "            print(f\"Correlation between Weekly_Sales and {feature}: {corr:.4f} (from {len(valid_data)} records)\")\n",
        "\n",
        "# Holiday impact analysis\n",
        "# Using 'IsHoliday_x' as the merged dataframe has this column\n",
        "holiday_impact = train_cleaned.groupby('IsHoliday_x')['Weekly_Sales'].mean()\n",
        "holiday_boost = holiday_impact[True] / holiday_impact[False] - 1\n",
        "print(f\"\\nHoliday sales boost: {holiday_boost:.2%}\")\n",
        "\n",
        "# Store type impact\n",
        "if 'Type' in train_cleaned.columns:\n",
        "    store_type_sales = train_cleaned.groupby('Type')['Weekly_Sales'].mean()\n",
        "    print(f\"\\nAverage sales by store type:\")\n",
        "    print(store_type_sales)\n",
        "\n",
        "# Select features based on correlation and business logic\n",
        "selected_features = core_features.copy()\n",
        "\n",
        "# Add economic features with significant correlation\n",
        "for feature, corr in correlation_analysis.items():\n",
        "    if abs(corr) > 0.01:  # Threshold for significance\n",
        "        if feature in train_cleaned.columns:\n",
        "            selected_features.append(feature)\n",
        "        print(f\"Selected {feature} (correlation: {corr:.4f})\")\n",
        "\n",
        "# Always include store features\n",
        "for feature in store_features:\n",
        "    if feature in train_cleaned.columns:\n",
        "        selected_features.append(feature)\n",
        "\n",
        "# Remove duplicates\n",
        "selected_features = list(set(selected_features))\n",
        "\n",
        "print(f\"\\nFinal selected features ({len(selected_features)}): {selected_features}\")\n",
        "\n",
        "# Create feature-selected dataset\n",
        "train_selected = train_cleaned[selected_features].copy()\n",
        "\n",
        "print(f\"\\nFeature-selected data shape: {train_selected.shape}\")\n",
        "\n",
        "# Log feature selection metrics\n",
        "wandb.log({\n",
        "    \"total_available_features\": len(train_cleaned.columns),\n",
        "    \"selected_features_count\": len(selected_features),\n",
        "    \"holiday_sales_boost\": float(holiday_boost),\n",
        "    \"selected_features\": selected_features,\n",
        "    **{f\"corr_{k}\": v for k, v in correlation_analysis.items() if not np.isnan(v)}\n",
        "})\n",
        "\n",
        "print(\"\\n✓ Feature selection completed and logged to wandb\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cleaned_records</td><td>▁</td></tr><tr><td>negative_sales_count</td><td>▁</td></tr><tr><td>negative_sales_pct</td><td>▁</td></tr><tr><td>remaining_missing_values</td><td>▁</td></tr><tr><td>store_dept_combinations</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cleaned_records</td><td>421570</td></tr><tr><td>negative_sales_count</td><td>1285</td></tr><tr><td>negative_sales_pct</td><td>0.30481</td></tr><tr><td>remaining_missing_values</td><td>0</td></tr><tr><td>store_dept_combinations</td><td>3331</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">NBEATS_Cleaning</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/nbm6ux3y' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/nbm6ux3y</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_234427-nbm6ux3y/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_234433-70anquul</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/70anquul' target=\"_blank\">NBEATS_Feature_Selection</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/70anquul' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/70anquul</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FEATURE SELECTION ===\n",
            "Available columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday_x', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y']\n",
            "Correlation between Weekly_Sales and Temperature: -0.0023\n",
            "Correlation between Weekly_Sales and Fuel_Price: -0.0001\n",
            "Correlation between Weekly_Sales and CPI: -0.0209\n",
            "Correlation between Weekly_Sales and Unemployment: -0.0259\n",
            "Correlation between Weekly_Sales and MarkDown1: 0.0848 (from 154386 records)\n",
            "Correlation between Weekly_Sales and MarkDown2: 0.0318 (from 154386 records)\n",
            "Correlation between Weekly_Sales and MarkDown3: 0.0615 (from 154386 records)\n",
            "Correlation between Weekly_Sales and MarkDown4: 0.0608 (from 154386 records)\n",
            "Correlation between Weekly_Sales and MarkDown5: 0.0888 (from 154386 records)\n",
            "\n",
            "Holiday sales boost: 7.13%\n",
            "\n",
            "Average sales by store type:\n",
            "Type\n",
            "A    20099.568043\n",
            "B    12237.075977\n",
            "C     9519.532538\n",
            "Name: Weekly_Sales, dtype: float64\n",
            "Selected CPI (correlation: -0.0209)\n",
            "Selected Unemployment (correlation: -0.0259)\n",
            "Selected MarkDown1 (correlation: 0.0848)\n",
            "Selected MarkDown2 (correlation: 0.0318)\n",
            "Selected MarkDown3 (correlation: 0.0615)\n",
            "Selected MarkDown4 (correlation: 0.0608)\n",
            "Selected MarkDown5 (correlation: 0.0888)\n",
            "\n",
            "Final selected features (14): ['MarkDown5', 'Store', 'MarkDown1', 'MarkDown2', 'IsHoliday_x', 'Date', 'Type', 'CPI', 'Unemployment', 'Dept', 'MarkDown3', 'Weekly_Sales', 'MarkDown4', 'Size']\n",
            "\n",
            "Feature-selected data shape: (421570, 14)\n",
            "\n",
            "✓ Feature selection completed and logged to wandb\n"
          ]
        }
      ],
      "execution_count": 50,
      "id": "feature_selection_code"
    },
    {
      "metadata": {
        "id": "cross_validation_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Cross Validation Run"
      ],
      "id": "cross_validation_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cross_validation_code",
        "outputId": "43974097-c33f-42e0-ea3d-29b1650aba35"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for cross validation\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"NBEATS_Cross_Validation\",\n",
        "    config={\n",
        "        \"stage\": \"cross_validation\",\n",
        "        \"lookback_window\": 52,\n",
        "        \"forecast_horizon\": 1,\n",
        "        \"model_type\": \"NBEATS\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\n=== CROSS VALIDATION ===\")\n",
        "\n",
        "# Create time-series data processor\n",
        "ts_processor = TimeSeriesDataProcessor(lookback_window=52, forecast_horizon=1)\n",
        "\n",
        "# Fit and transform the data\n",
        "print(\"Processing time-series data...\")\n",
        "ts_processor.fit(train_selected)\n",
        "processed_data = ts_processor.transform(train_selected)\n",
        "\n",
        "sequences = processed_data['sequences']\n",
        "targets = processed_data['targets']\n",
        "metadata = processed_data['metadata']\n",
        "\n",
        "print(f\"Generated {len(sequences)} sequences\")\n",
        "print(f\"Sequence shape example: {sequences[0].shape if len(sequences) > 0 else 'No sequences'}\")\n",
        "print(f\"Target shape example: {targets[0].shape if len(targets) > 0 else 'No targets'}\")\n",
        "\n",
        "if len(sequences) == 0:\n",
        "    print(\"❌ No sequences generated. Check data processing.\")\n",
        "    wandb.log({\"sequences_generated\": 0, \"processing_failed\": True})\n",
        "else:\n",
        "    # Convert to consistent numpy arrays and ensure float type\n",
        "    # Find the maximum number of features\n",
        "    max_features = max([seq.shape[1] if len(seq.shape) > 1 else 1 for seq in sequences])\n",
        "    lookback_length = sequences[0].shape[0]\n",
        "\n",
        "    # Pad sequences to have consistent feature count and convert to float32\n",
        "    padded_sequences = []\n",
        "    valid_targets = []\n",
        "\n",
        "    for i, (seq, tgt) in enumerate(zip(sequences, targets)):\n",
        "        if len(seq.shape) == 1:\n",
        "            seq = seq.reshape(-1, 1)\n",
        "\n",
        "        # Pad features if necessary\n",
        "        if seq.shape[1] < max_features:\n",
        "            padding = np.zeros((seq.shape[0], max_features - seq.shape[1]), dtype=np.float32)\n",
        "            seq = np.column_stack([seq, padding]).astype(np.float32)\n",
        "        else:\n",
        "             seq = seq.astype(np.float32) # Ensure consistent float type\n",
        "\n",
        "        padded_sequences.append(seq)\n",
        "        valid_targets.append(tgt.astype(np.float32)) # Ensure target is also float32\n",
        "\n",
        "\n",
        "    sequences = np.array(padded_sequences, dtype=np.float32) # Explicitly set dtype to float32\n",
        "    targets = np.array(valid_targets, dtype=np.float32) # Explicitly set dtype to float32\n",
        "\n",
        "\n",
        "    print(f\"Processed sequences shape: {sequences.shape}\")\n",
        "    print(f\"Processed targets shape: {targets.shape}\")\n",
        "\n",
        "    # Time-based train-validation split (last 20% of data for validation)\n",
        "    split_idx = int(0.8 * len(sequences))\n",
        "\n",
        "    X_train_cv = sequences[:split_idx]\n",
        "    y_train_cv = targets[:split_idx]\n",
        "    X_val_cv = sequences[split_idx:]\n",
        "    y_val_cv = targets[split_idx:]\n",
        "\n",
        "    print(f\"\\nTrain-Validation Split:\")\n",
        "    print(f\"Training sequences: {len(X_train_cv)}\")\n",
        "    print(f\"Validation sequences: {len(X_val_cv)}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = WalmartDataset(X_train_cv, y_train_cv)\n",
        "    val_dataset = WalmartDataset(X_val_cv, y_val_cv)\n",
        "\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize N-BEATS model\n",
        "    model_config = {\n",
        "        \"input_size\": lookback_length,\n",
        "        \"forecast_size\": 1,\n",
        "        \"num_stacks\": 2,\n",
        "        \"num_blocks_per_stack\": 3,\n",
        "        \"num_layers\": 4,\n",
        "        \"layer_size\": 256,  # Smaller for faster training\n",
        "        \"num_features\": max_features\n",
        "    }\n",
        "\n",
        "    model = NBEATSModel(**model_config).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"\\nModel initialized with config: {model_config}\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5  # Limited epochs for CV\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
        "            sequences = sequences.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences, targets in val_loader:\n",
        "                sequences = sequences.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                all_predictions.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Calculate metrics\n",
        "        all_predictions = np.array(all_predictions).flatten()\n",
        "        all_targets = np.array(all_targets).flatten()\n",
        "\n",
        "        val_mae = mean_absolute_error(all_targets, all_predictions)\n",
        "        val_rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
        "        val_r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "        # Safe MAPE calculation\n",
        "        def safe_mape(y_true, y_pred):\n",
        "            mask = y_true != 0\n",
        "            if mask.sum() == 0:\n",
        "                return float('inf')\n",
        "            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "        val_mape = safe_mape(all_targets, all_predictions)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'  Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'  Val MAE: {val_mae:.2f}')\n",
        "        print(f'  Val RMSE: {val_rmse:.2f}')\n",
        "        print(f'  Val MAPE: {val_mape:.2f}%')\n",
        "        print(f'  Val R²: {val_r2:.4f}')\n",
        "\n",
        "        # Log to wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": avg_val_loss,\n",
        "            \"val_mae\": val_mae,\n",
        "            \"val_rmse\": val_rmse,\n",
        "            \"val_mape\": val_mape if not np.isinf(val_mape) else 0.0,\n",
        "            \"val_r2\": val_r2\n",
        "        })\n",
        "\n",
        "    # Final CV results\n",
        "    final_metrics = {\n",
        "        \"cv_final_train_loss\": train_losses[-1],\n",
        "        \"cv_final_val_loss\": val_losses[-1],\n",
        "        \"cv_final_val_mae\": val_mae,\n",
        "        \"cv_final_val_rmse\": val_rmse,\n",
        "        \"cv_final_val_mape\": val_mape if not np.isinf(val_mape) else 0.0,\n",
        "        \"cv_final_val_r2\": val_r2,\n",
        "        \"sequences_generated\": len(sequences),\n",
        "        \"train_sequences\": len(X_train_cv),\n",
        "        \"val_sequences\": len(X_val_cv)\n",
        "    }\n",
        "\n",
        "    wandb.log(final_metrics)\n",
        "\n",
        "    print(\"\\n✓ Cross validation completed and logged to wandb\")\n",
        "    print(f\"Final validation metrics: MAE={val_mae:.2f}, RMSE={val_rmse:.2f}, MAPE={val_mape:.2f}%, R²={val_r2:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>corr_CPI</td><td>▁</td></tr><tr><td>corr_Fuel_Price</td><td>▁</td></tr><tr><td>corr_MarkDown1</td><td>▁</td></tr><tr><td>corr_MarkDown2</td><td>▁</td></tr><tr><td>corr_MarkDown3</td><td>▁</td></tr><tr><td>corr_MarkDown4</td><td>▁</td></tr><tr><td>corr_MarkDown5</td><td>▁</td></tr><tr><td>corr_Temperature</td><td>▁</td></tr><tr><td>corr_Unemployment</td><td>▁</td></tr><tr><td>holiday_sales_boost</td><td>▁</td></tr><tr><td>selected_features_count</td><td>▁</td></tr><tr><td>total_available_features</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>corr_CPI</td><td>-0.02092</td></tr><tr><td>corr_Fuel_Price</td><td>-0.00012</td></tr><tr><td>corr_MarkDown1</td><td>0.08478</td></tr><tr><td>corr_MarkDown2</td><td>0.03182</td></tr><tr><td>corr_MarkDown3</td><td>0.06146</td></tr><tr><td>corr_MarkDown4</td><td>0.06075</td></tr><tr><td>corr_MarkDown5</td><td>0.0888</td></tr><tr><td>corr_Temperature</td><td>-0.00231</td></tr><tr><td>corr_Unemployment</td><td>-0.02586</td></tr><tr><td>holiday_sales_boost</td><td>0.07134</td></tr><tr><td>selected_features_count</td><td>14</td></tr><tr><td>total_available_features</td><td>17</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">NBEATS_Feature_Selection</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/70anquul' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/70anquul</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_234433-70anquul/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_234442-88brtckg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/88brtckg' target=\"_blank\">NBEATS_Cross_Validation</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/88brtckg' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/88brtckg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CROSS VALIDATION ===\n",
            "Processing time-series data...\n",
            "Found 3331 store-dept combinations\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Generated 261083 valid sequences from 3331 store-dept combinations\n",
            "Generated 261083 sequences\n",
            "Sequence shape example: (52, 3)\n",
            "Target shape example: (1,)\n",
            "Processed sequences shape: (261083, 52, 3)\n",
            "Processed targets shape: (261083, 1)\n",
            "\n",
            "Train-Validation Split:\n",
            "Training sequences: 208866\n",
            "Validation sequences: 52217\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "\n",
            "Model initialized with config: {'input_size': 52, 'forecast_size': 1, 'num_stacks': 2, 'num_blocks_per_stack': 3, 'num_layers': 4, 'layer_size': 256, 'num_features': 3}\n",
            "Total parameters: 1,426,036\n",
            "Epoch 1/5, Batch 0, Loss: 421401728.0000\n",
            "Epoch 1/5, Batch 50, Loss: 178276240.0000\n",
            "Epoch 1/5, Batch 100, Loss: 5490144.5000\n",
            "Epoch 1/5, Batch 150, Loss: 13142996.0000\n",
            "Epoch 1/5, Batch 200, Loss: 7928619.0000\n",
            "Epoch 1/5, Batch 250, Loss: 12125604.0000\n",
            "Epoch 1/5, Batch 300, Loss: 123733944.0000\n",
            "Epoch 1/5, Batch 350, Loss: 7325812.0000\n",
            "Epoch 1/5, Batch 400, Loss: 28331616.0000\n",
            "Epoch 1/5, Batch 450, Loss: 12132256.0000\n",
            "Epoch 1/5, Batch 500, Loss: 26288144.0000\n",
            "Epoch 1/5, Batch 550, Loss: 9045241.0000\n",
            "Epoch 1/5, Batch 600, Loss: 7339031.5000\n",
            "Epoch 1/5, Batch 650, Loss: 6062101.5000\n",
            "Epoch 1/5, Batch 700, Loss: 33106130.0000\n",
            "Epoch 1/5, Batch 750, Loss: 12854776.0000\n",
            "Epoch 1/5, Batch 800, Loss: 6431811.0000\n",
            "Epoch 1/5, Batch 850, Loss: 6117185.0000\n",
            "Epoch 1/5, Batch 900, Loss: 37657224.0000\n",
            "Epoch 1/5, Batch 950, Loss: 26135596.0000\n",
            "Epoch 1/5, Batch 1000, Loss: 4576371.5000\n",
            "Epoch 1/5, Batch 1050, Loss: 5950702.0000\n",
            "Epoch 1/5, Batch 1100, Loss: 8313557.0000\n",
            "Epoch 1/5, Batch 1150, Loss: 1875192.1250\n",
            "Epoch 1/5, Batch 1200, Loss: 5052400.0000\n",
            "Epoch 1/5, Batch 1250, Loss: 173358128.0000\n",
            "Epoch 1/5, Batch 1300, Loss: 3983509.0000\n",
            "Epoch 1/5, Batch 1350, Loss: 52674324.0000\n",
            "Epoch 1/5, Batch 1400, Loss: 3783308.2500\n",
            "Epoch 1/5, Batch 1450, Loss: 7372786.0000\n",
            "Epoch 1/5, Batch 1500, Loss: 21801694.0000\n",
            "Epoch 1/5, Batch 1550, Loss: 11608332.0000\n",
            "Epoch 1/5, Batch 1600, Loss: 4309072.0000\n",
            "Epoch 1/5, Batch 1650, Loss: 11070796.0000\n",
            "Epoch 1/5, Batch 1700, Loss: 3895290.7500\n",
            "Epoch 1/5, Batch 1750, Loss: 13844090.0000\n",
            "Epoch 1/5, Batch 1800, Loss: 31058170.0000\n",
            "Epoch 1/5, Batch 1850, Loss: 3224251.5000\n",
            "Epoch 1/5, Batch 1900, Loss: 6248061.0000\n",
            "Epoch 1/5, Batch 1950, Loss: 22298458.0000\n",
            "Epoch 1/5, Batch 2000, Loss: 10833614.0000\n",
            "Epoch 1/5, Batch 2050, Loss: 16578864.0000\n",
            "Epoch 1/5, Batch 2100, Loss: 11617803.0000\n",
            "Epoch 1/5, Batch 2150, Loss: 4213494.5000\n",
            "Epoch 1/5, Batch 2200, Loss: 7943088.0000\n",
            "Epoch 1/5, Batch 2250, Loss: 8392888.0000\n",
            "Epoch 1/5, Batch 2300, Loss: 3883296.5000\n",
            "Epoch 1/5, Batch 2350, Loss: 12519461.0000\n",
            "Epoch 1/5, Batch 2400, Loss: 39137668.0000\n",
            "Epoch 1/5, Batch 2450, Loss: 9858634.0000\n",
            "Epoch 1/5, Batch 2500, Loss: 51839500.0000\n",
            "Epoch 1/5, Batch 2550, Loss: 34657816.0000\n",
            "Epoch 1/5, Batch 2600, Loss: 11730434.0000\n",
            "Epoch 1/5, Batch 2650, Loss: 12528503.0000\n",
            "Epoch 1/5, Batch 2700, Loss: 3695716.5000\n",
            "Epoch 1/5, Batch 2750, Loss: 6942433.0000\n",
            "Epoch 1/5, Batch 2800, Loss: 16917688.0000\n",
            "Epoch 1/5, Batch 2850, Loss: 26218504.0000\n",
            "Epoch 1/5, Batch 2900, Loss: 11090389.0000\n",
            "Epoch 1/5, Batch 2950, Loss: 7795530.5000\n",
            "Epoch 1/5, Batch 3000, Loss: 6193710.5000\n",
            "Epoch 1/5, Batch 3050, Loss: 11842273.0000\n",
            "Epoch 1/5, Batch 3100, Loss: 8508797.0000\n",
            "Epoch 1/5, Batch 3150, Loss: 75054656.0000\n",
            "Epoch 1/5, Batch 3200, Loss: 5171782.0000\n",
            "Epoch 1/5, Batch 3250, Loss: 3345820.0000\n",
            "Epoch 1/5, Batch 3300, Loss: 22029276.0000\n",
            "Epoch 1/5, Batch 3350, Loss: 7482284.5000\n",
            "Epoch 1/5, Batch 3400, Loss: 4639468.5000\n",
            "Epoch 1/5, Batch 3450, Loss: 10336578.0000\n",
            "Epoch 1/5, Batch 3500, Loss: 4830770.0000\n",
            "Epoch 1/5, Batch 3550, Loss: 3203574.5000\n",
            "Epoch 1/5, Batch 3600, Loss: 5897558.0000\n",
            "Epoch 1/5, Batch 3650, Loss: 5529953.0000\n",
            "Epoch 1/5, Batch 3700, Loss: 5612353.5000\n",
            "Epoch 1/5, Batch 3750, Loss: 1750736.0000\n",
            "Epoch 1/5, Batch 3800, Loss: 3829687.7500\n",
            "Epoch 1/5, Batch 3850, Loss: 14337436.0000\n",
            "Epoch 1/5, Batch 3900, Loss: 14176644.0000\n",
            "Epoch 1/5, Batch 3950, Loss: 13368844.0000\n",
            "Epoch 1/5, Batch 4000, Loss: 3442059.7500\n",
            "Epoch 1/5, Batch 4050, Loss: 4581604.0000\n",
            "Epoch 1/5, Batch 4100, Loss: 7180453.0000\n",
            "Epoch 1/5, Batch 4150, Loss: 12238712.0000\n",
            "Epoch 1/5, Batch 4200, Loss: 28659414.0000\n",
            "Epoch 1/5, Batch 4250, Loss: 55686712.0000\n",
            "Epoch 1/5, Batch 4300, Loss: 6021314.0000\n",
            "Epoch 1/5, Batch 4350, Loss: 12343258.0000\n",
            "Epoch 1/5, Batch 4400, Loss: 17463604.0000\n",
            "Epoch 1/5, Batch 4450, Loss: 12660004.0000\n",
            "Epoch 1/5, Batch 4500, Loss: 7109321.5000\n",
            "Epoch 1/5, Batch 4550, Loss: 10924527.0000\n",
            "Epoch 1/5, Batch 4600, Loss: 30003512.0000\n",
            "Epoch 1/5, Batch 4650, Loss: 20723960.0000\n",
            "Epoch 1/5, Batch 4700, Loss: 24774420.0000\n",
            "Epoch 1/5, Batch 4750, Loss: 21681068.0000\n",
            "Epoch 1/5, Batch 4800, Loss: 19311810.0000\n",
            "Epoch 1/5, Batch 4850, Loss: 14934106.0000\n",
            "Epoch 1/5, Batch 4900, Loss: 1345879.5000\n",
            "Epoch 1/5, Batch 4950, Loss: 3622965.0000\n",
            "Epoch 1/5, Batch 5000, Loss: 2864976.7500\n",
            "Epoch 1/5, Batch 5050, Loss: 14448664.0000\n",
            "Epoch 1/5, Batch 5100, Loss: 11365725.0000\n",
            "Epoch 1/5, Batch 5150, Loss: 3712052.7500\n",
            "Epoch 1/5, Batch 5200, Loss: 10477870.0000\n",
            "Epoch 1/5, Batch 5250, Loss: 6984994.0000\n",
            "Epoch 1/5, Batch 5300, Loss: 131966856.0000\n",
            "Epoch 1/5, Batch 5350, Loss: 11780071.0000\n",
            "Epoch 1/5, Batch 5400, Loss: 59153524.0000\n",
            "Epoch 1/5, Batch 5450, Loss: 26292790.0000\n",
            "Epoch 1/5, Batch 5500, Loss: 203504416.0000\n",
            "Epoch 1/5, Batch 5550, Loss: 18070652.0000\n",
            "Epoch 1/5, Batch 5600, Loss: 96414384.0000\n",
            "Epoch 1/5, Batch 5650, Loss: 7610157.5000\n",
            "Epoch 1/5, Batch 5700, Loss: 7402050.0000\n",
            "Epoch 1/5, Batch 5750, Loss: 2704193.0000\n",
            "Epoch 1/5, Batch 5800, Loss: 2116408.0000\n",
            "Epoch 1/5, Batch 5850, Loss: 8500228.0000\n",
            "Epoch 1/5, Batch 5900, Loss: 24895152.0000\n",
            "Epoch 1/5, Batch 5950, Loss: 8711632.0000\n",
            "Epoch 1/5, Batch 6000, Loss: 12793857.0000\n",
            "Epoch 1/5, Batch 6050, Loss: 4149779.7500\n",
            "Epoch 1/5, Batch 6100, Loss: 7591255.0000\n",
            "Epoch 1/5, Batch 6150, Loss: 3507402.0000\n",
            "Epoch 1/5, Batch 6200, Loss: 34488528.0000\n",
            "Epoch 1/5, Batch 6250, Loss: 1921616.3750\n",
            "Epoch 1/5, Batch 6300, Loss: 6064580.5000\n",
            "Epoch 1/5, Batch 6350, Loss: 4031891.5000\n",
            "Epoch 1/5, Batch 6400, Loss: 2819798.5000\n",
            "Epoch 1/5, Batch 6450, Loss: 3741221.0000\n",
            "Epoch 1/5, Batch 6500, Loss: 4072206.7500\n",
            "Epoch 1/5:\n",
            "  Train Loss: 18522789.3145\n",
            "  Val Loss: 13303532.4448\n",
            "  Val MAE: 1212.60\n",
            "  Val RMSE: 3647.64\n",
            "  Val MAPE: 792.89%\n",
            "  Val R²: 0.9644\n",
            "Epoch 2/5, Batch 0, Loss: 7489359.5000\n",
            "Epoch 2/5, Batch 50, Loss: 12016011.0000\n",
            "Epoch 2/5, Batch 100, Loss: 11030494.0000\n",
            "Epoch 2/5, Batch 150, Loss: 4159588.7500\n",
            "Epoch 2/5, Batch 200, Loss: 4531218.5000\n",
            "Epoch 2/5, Batch 250, Loss: 13139112.0000\n",
            "Epoch 2/5, Batch 300, Loss: 33579164.0000\n",
            "Epoch 2/5, Batch 350, Loss: 3831464.0000\n",
            "Epoch 2/5, Batch 400, Loss: 3990350.0000\n",
            "Epoch 2/5, Batch 450, Loss: 2055131.5000\n",
            "Epoch 2/5, Batch 500, Loss: 4023943.0000\n",
            "Epoch 2/5, Batch 550, Loss: 25651190.0000\n",
            "Epoch 2/5, Batch 600, Loss: 3794411.5000\n",
            "Epoch 2/5, Batch 650, Loss: 6526557.0000\n",
            "Epoch 2/5, Batch 700, Loss: 13532884.0000\n",
            "Epoch 2/5, Batch 750, Loss: 33547250.0000\n",
            "Epoch 2/5, Batch 800, Loss: 5470335.0000\n",
            "Epoch 2/5, Batch 850, Loss: 8147749.5000\n",
            "Epoch 2/5, Batch 900, Loss: 10336059.0000\n",
            "Epoch 2/5, Batch 950, Loss: 9707480.0000\n",
            "Epoch 2/5, Batch 1000, Loss: 2212229.0000\n",
            "Epoch 2/5, Batch 1050, Loss: 3859171.0000\n",
            "Epoch 2/5, Batch 1100, Loss: 20306034.0000\n",
            "Epoch 2/5, Batch 1150, Loss: 12798152.0000\n",
            "Epoch 2/5, Batch 1200, Loss: 7655536.5000\n",
            "Epoch 2/5, Batch 1250, Loss: 60395416.0000\n",
            "Epoch 2/5, Batch 1300, Loss: 4386474.0000\n",
            "Epoch 2/5, Batch 1350, Loss: 9077354.0000\n",
            "Epoch 2/5, Batch 1400, Loss: 3452625.5000\n",
            "Epoch 2/5, Batch 1450, Loss: 10782750.0000\n",
            "Epoch 2/5, Batch 1500, Loss: 4335597.5000\n",
            "Epoch 2/5, Batch 1550, Loss: 11079562.0000\n",
            "Epoch 2/5, Batch 1600, Loss: 3826549.5000\n",
            "Epoch 2/5, Batch 1650, Loss: 32787420.0000\n",
            "Epoch 2/5, Batch 1700, Loss: 25499652.0000\n",
            "Epoch 2/5, Batch 1750, Loss: 8606558.0000\n",
            "Epoch 2/5, Batch 1800, Loss: 13413753.0000\n",
            "Epoch 2/5, Batch 1850, Loss: 3488924.5000\n",
            "Epoch 2/5, Batch 1900, Loss: 8837655.0000\n",
            "Epoch 2/5, Batch 1950, Loss: 5240964.0000\n",
            "Epoch 2/5, Batch 2000, Loss: 19776576.0000\n",
            "Epoch 2/5, Batch 2050, Loss: 7558086.5000\n",
            "Epoch 2/5, Batch 2100, Loss: 9778399.0000\n",
            "Epoch 2/5, Batch 2150, Loss: 45559408.0000\n",
            "Epoch 2/5, Batch 2200, Loss: 19775188.0000\n",
            "Epoch 2/5, Batch 2250, Loss: 35219920.0000\n",
            "Epoch 2/5, Batch 2300, Loss: 30312152.0000\n",
            "Epoch 2/5, Batch 2350, Loss: 8339465.0000\n",
            "Epoch 2/5, Batch 2400, Loss: 9082686.0000\n",
            "Epoch 2/5, Batch 2450, Loss: 14815966.0000\n",
            "Epoch 2/5, Batch 2500, Loss: 7067218.0000\n",
            "Epoch 2/5, Batch 2550, Loss: 4595481.0000\n",
            "Epoch 2/5, Batch 2600, Loss: 3134244.7500\n",
            "Epoch 2/5, Batch 2650, Loss: 6147937.0000\n",
            "Epoch 2/5, Batch 2700, Loss: 6240515.5000\n",
            "Epoch 2/5, Batch 2750, Loss: 10689556.0000\n",
            "Epoch 2/5, Batch 2800, Loss: 5172155.0000\n",
            "Epoch 2/5, Batch 2850, Loss: 5039000.5000\n",
            "Epoch 2/5, Batch 2900, Loss: 26139952.0000\n",
            "Epoch 2/5, Batch 2950, Loss: 957347.7500\n",
            "Epoch 2/5, Batch 3000, Loss: 9520094.0000\n",
            "Epoch 2/5, Batch 3050, Loss: 6644433.5000\n",
            "Epoch 2/5, Batch 3100, Loss: 10460826.0000\n",
            "Epoch 2/5, Batch 3150, Loss: 5531931.5000\n",
            "Epoch 2/5, Batch 3200, Loss: 20136204.0000\n",
            "Epoch 2/5, Batch 3250, Loss: 9946035.0000\n",
            "Epoch 2/5, Batch 3300, Loss: 4925919.0000\n",
            "Epoch 2/5, Batch 3350, Loss: 45820524.0000\n",
            "Epoch 2/5, Batch 3400, Loss: 5790965.0000\n",
            "Epoch 2/5, Batch 3450, Loss: 11902232.0000\n",
            "Epoch 2/5, Batch 3500, Loss: 4825507.5000\n",
            "Epoch 2/5, Batch 3550, Loss: 6920455.0000\n",
            "Epoch 2/5, Batch 3600, Loss: 27281414.0000\n",
            "Epoch 2/5, Batch 3650, Loss: 8712669.0000\n",
            "Epoch 2/5, Batch 3700, Loss: 5084093.5000\n",
            "Epoch 2/5, Batch 3750, Loss: 44932480.0000\n",
            "Epoch 2/5, Batch 3800, Loss: 34588240.0000\n",
            "Epoch 2/5, Batch 3850, Loss: 9464829.0000\n",
            "Epoch 2/5, Batch 3900, Loss: 10449194.0000\n",
            "Epoch 2/5, Batch 3950, Loss: 8615793.0000\n",
            "Epoch 2/5, Batch 4000, Loss: 4959246.5000\n",
            "Epoch 2/5, Batch 4050, Loss: 5411872.0000\n",
            "Epoch 2/5, Batch 4100, Loss: 2618053.0000\n",
            "Epoch 2/5, Batch 4150, Loss: 12585480.0000\n",
            "Epoch 2/5, Batch 4200, Loss: 6427838.5000\n",
            "Epoch 2/5, Batch 4250, Loss: 3778963.0000\n",
            "Epoch 2/5, Batch 4300, Loss: 10965995.0000\n",
            "Epoch 2/5, Batch 4350, Loss: 8854786.0000\n",
            "Epoch 2/5, Batch 4400, Loss: 10358561.0000\n",
            "Epoch 2/5, Batch 4450, Loss: 14297830.0000\n",
            "Epoch 2/5, Batch 4500, Loss: 24135860.0000\n",
            "Epoch 2/5, Batch 4550, Loss: 10701526.0000\n",
            "Epoch 2/5, Batch 4600, Loss: 10477065.0000\n",
            "Epoch 2/5, Batch 4650, Loss: 3149338.0000\n",
            "Epoch 2/5, Batch 4700, Loss: 7691741.5000\n",
            "Epoch 2/5, Batch 4750, Loss: 2444722.7500\n",
            "Epoch 2/5, Batch 4800, Loss: 6744399.0000\n",
            "Epoch 2/5, Batch 4850, Loss: 11042342.0000\n",
            "Epoch 2/5, Batch 4900, Loss: 12383270.0000\n",
            "Epoch 2/5, Batch 4950, Loss: 4329738.5000\n",
            "Epoch 2/5, Batch 5000, Loss: 34113056.0000\n",
            "Epoch 2/5, Batch 5050, Loss: 8628627.0000\n",
            "Epoch 2/5, Batch 5100, Loss: 7713181.0000\n",
            "Epoch 2/5, Batch 5150, Loss: 17874620.0000\n",
            "Epoch 2/5, Batch 5200, Loss: 9674098.0000\n",
            "Epoch 2/5, Batch 5250, Loss: 7539934.0000\n",
            "Epoch 2/5, Batch 5300, Loss: 11974692.0000\n",
            "Epoch 2/5, Batch 5350, Loss: 37506828.0000\n",
            "Epoch 2/5, Batch 5400, Loss: 15324322.0000\n",
            "Epoch 2/5, Batch 5450, Loss: 6823948.0000\n",
            "Epoch 2/5, Batch 5500, Loss: 268663008.0000\n",
            "Epoch 2/5, Batch 5550, Loss: 4567321.5000\n",
            "Epoch 2/5, Batch 5600, Loss: 12974930.0000\n",
            "Epoch 2/5, Batch 5650, Loss: 2566741.0000\n",
            "Epoch 2/5, Batch 5700, Loss: 4506529.0000\n",
            "Epoch 2/5, Batch 5750, Loss: 38855008.0000\n",
            "Epoch 2/5, Batch 5800, Loss: 5996837.0000\n",
            "Epoch 2/5, Batch 5850, Loss: 10445963.0000\n",
            "Epoch 2/5, Batch 5900, Loss: 5609242.5000\n",
            "Epoch 2/5, Batch 5950, Loss: 5838978.0000\n",
            "Epoch 2/5, Batch 6000, Loss: 8447724.0000\n",
            "Epoch 2/5, Batch 6050, Loss: 6840408.0000\n",
            "Epoch 2/5, Batch 6100, Loss: 3952957.7500\n",
            "Epoch 2/5, Batch 6150, Loss: 3761688.2500\n",
            "Epoch 2/5, Batch 6200, Loss: 20228772.0000\n",
            "Epoch 2/5, Batch 6250, Loss: 10237999.0000\n",
            "Epoch 2/5, Batch 6300, Loss: 5339718.0000\n",
            "Epoch 2/5, Batch 6350, Loss: 14957292.0000\n",
            "Epoch 2/5, Batch 6400, Loss: 2866517.7500\n",
            "Epoch 2/5, Batch 6450, Loss: 3414567.2500\n",
            "Epoch 2/5, Batch 6500, Loss: 2558889.5000\n",
            "Epoch 2/5:\n",
            "  Train Loss: 15678900.7964\n",
            "  Val Loss: 8508768.8155\n",
            "  Val MAE: 1162.17\n",
            "  Val RMSE: 2917.17\n",
            "  Val MAPE: 344.62%\n",
            "  Val R²: 0.9773\n",
            "Epoch 3/5, Batch 0, Loss: 2657773.0000\n",
            "Epoch 3/5, Batch 50, Loss: 25180614.0000\n",
            "Epoch 3/5, Batch 100, Loss: 2228801.7500\n",
            "Epoch 3/5, Batch 150, Loss: 10787043.0000\n",
            "Epoch 3/5, Batch 200, Loss: 5822992.5000\n",
            "Epoch 3/5, Batch 250, Loss: 4078915.0000\n",
            "Epoch 3/5, Batch 300, Loss: 33802308.0000\n",
            "Epoch 3/5, Batch 350, Loss: 21694016.0000\n",
            "Epoch 3/5, Batch 400, Loss: 9053318.0000\n",
            "Epoch 3/5, Batch 450, Loss: 9650098.0000\n",
            "Epoch 3/5, Batch 500, Loss: 224071328.0000\n",
            "Epoch 3/5, Batch 550, Loss: 19006860.0000\n",
            "Epoch 3/5, Batch 600, Loss: 12942988.0000\n",
            "Epoch 3/5, Batch 650, Loss: 19581246.0000\n",
            "Epoch 3/5, Batch 700, Loss: 8389346.0000\n",
            "Epoch 3/5, Batch 750, Loss: 6416174.0000\n",
            "Epoch 3/5, Batch 800, Loss: 14183266.0000\n",
            "Epoch 3/5, Batch 850, Loss: 2703621.5000\n",
            "Epoch 3/5, Batch 900, Loss: 11123495.0000\n",
            "Epoch 3/5, Batch 950, Loss: 45313148.0000\n",
            "Epoch 3/5, Batch 1000, Loss: 1001796.0000\n",
            "Epoch 3/5, Batch 1050, Loss: 70241184.0000\n",
            "Epoch 3/5, Batch 1100, Loss: 2048377.5000\n",
            "Epoch 3/5, Batch 1150, Loss: 4085030.2500\n",
            "Epoch 3/5, Batch 1200, Loss: 3005289.5000\n",
            "Epoch 3/5, Batch 1250, Loss: 14557807.0000\n",
            "Epoch 3/5, Batch 1300, Loss: 3088539.7500\n",
            "Epoch 3/5, Batch 1350, Loss: 12691012.0000\n",
            "Epoch 3/5, Batch 1400, Loss: 7728042.0000\n",
            "Epoch 3/5, Batch 1450, Loss: 11021970.0000\n",
            "Epoch 3/5, Batch 1500, Loss: 22241546.0000\n",
            "Epoch 3/5, Batch 1550, Loss: 242062432.0000\n",
            "Epoch 3/5, Batch 1600, Loss: 10375502.0000\n",
            "Epoch 3/5, Batch 1650, Loss: 6898214.0000\n",
            "Epoch 3/5, Batch 1700, Loss: 15424006.0000\n",
            "Epoch 3/5, Batch 1750, Loss: 296365728.0000\n",
            "Epoch 3/5, Batch 1800, Loss: 38211936.0000\n",
            "Epoch 3/5, Batch 1850, Loss: 6043093.0000\n",
            "Epoch 3/5, Batch 1900, Loss: 5028340.0000\n",
            "Epoch 3/5, Batch 1950, Loss: 6591311.0000\n",
            "Epoch 3/5, Batch 2000, Loss: 7537498.0000\n",
            "Epoch 3/5, Batch 2050, Loss: 16429078.0000\n",
            "Epoch 3/5, Batch 2100, Loss: 25380448.0000\n",
            "Epoch 3/5, Batch 2150, Loss: 6360472.0000\n",
            "Epoch 3/5, Batch 2200, Loss: 9585930.0000\n",
            "Epoch 3/5, Batch 2250, Loss: 7289296.0000\n",
            "Epoch 3/5, Batch 2300, Loss: 21898086.0000\n",
            "Epoch 3/5, Batch 2350, Loss: 2648053.5000\n",
            "Epoch 3/5, Batch 2400, Loss: 3842340.0000\n",
            "Epoch 3/5, Batch 2450, Loss: 3725626.5000\n",
            "Epoch 3/5, Batch 2500, Loss: 5108235.0000\n",
            "Epoch 3/5, Batch 2550, Loss: 5299450.0000\n",
            "Epoch 3/5, Batch 2600, Loss: 3957623.0000\n",
            "Epoch 3/5, Batch 2650, Loss: 6899502.5000\n",
            "Epoch 3/5, Batch 2700, Loss: 25348354.0000\n",
            "Epoch 3/5, Batch 2750, Loss: 1563837.7500\n",
            "Epoch 3/5, Batch 2800, Loss: 5997584.0000\n",
            "Epoch 3/5, Batch 2850, Loss: 39365228.0000\n",
            "Epoch 3/5, Batch 2900, Loss: 7121425.0000\n",
            "Epoch 3/5, Batch 2950, Loss: 6886399.0000\n",
            "Epoch 3/5, Batch 3000, Loss: 4730784.5000\n",
            "Epoch 3/5, Batch 3050, Loss: 3288116.0000\n",
            "Epoch 3/5, Batch 3100, Loss: 7446895.0000\n",
            "Epoch 3/5, Batch 3150, Loss: 6579334.5000\n",
            "Epoch 3/5, Batch 3200, Loss: 22433936.0000\n",
            "Epoch 3/5, Batch 3250, Loss: 1489988.7500\n",
            "Epoch 3/5, Batch 3300, Loss: 20128542.0000\n",
            "Epoch 3/5, Batch 3350, Loss: 15301694.0000\n",
            "Epoch 3/5, Batch 3400, Loss: 7409053.5000\n",
            "Epoch 3/5, Batch 3450, Loss: 4589400.5000\n",
            "Epoch 3/5, Batch 3500, Loss: 4468235.0000\n",
            "Epoch 3/5, Batch 3550, Loss: 6218698.0000\n",
            "Epoch 3/5, Batch 3600, Loss: 4514885.0000\n",
            "Epoch 3/5, Batch 3650, Loss: 14596375.0000\n",
            "Epoch 3/5, Batch 3700, Loss: 35431388.0000\n",
            "Epoch 3/5, Batch 3750, Loss: 1446641.1250\n",
            "Epoch 3/5, Batch 3800, Loss: 10543062.0000\n",
            "Epoch 3/5, Batch 3850, Loss: 39105764.0000\n",
            "Epoch 3/5, Batch 3900, Loss: 4695937.5000\n",
            "Epoch 3/5, Batch 3950, Loss: 8961090.0000\n",
            "Epoch 3/5, Batch 4000, Loss: 8629803.0000\n",
            "Epoch 3/5, Batch 4050, Loss: 19838466.0000\n",
            "Epoch 3/5, Batch 4100, Loss: 11835416.0000\n",
            "Epoch 3/5, Batch 4150, Loss: 8815657.0000\n",
            "Epoch 3/5, Batch 4200, Loss: 10297019.0000\n",
            "Epoch 3/5, Batch 4250, Loss: 2455158.0000\n",
            "Epoch 3/5, Batch 4300, Loss: 691695.0625\n",
            "Epoch 3/5, Batch 4350, Loss: 12152613.0000\n",
            "Epoch 3/5, Batch 4400, Loss: 5179875.5000\n",
            "Epoch 3/5, Batch 4450, Loss: 12190522.0000\n",
            "Epoch 3/5, Batch 4500, Loss: 3551713.5000\n",
            "Epoch 3/5, Batch 4550, Loss: 9471554.0000\n",
            "Epoch 3/5, Batch 4600, Loss: 4482169.5000\n",
            "Epoch 3/5, Batch 4650, Loss: 9774146.0000\n",
            "Epoch 3/5, Batch 4700, Loss: 2705759.0000\n",
            "Epoch 3/5, Batch 4750, Loss: 10538596.0000\n",
            "Epoch 3/5, Batch 4800, Loss: 5699166.0000\n",
            "Epoch 3/5, Batch 4850, Loss: 3987283.5000\n",
            "Epoch 3/5, Batch 4900, Loss: 7111663.0000\n",
            "Epoch 3/5, Batch 4950, Loss: 8582386.0000\n",
            "Epoch 3/5, Batch 5000, Loss: 11196660.0000\n",
            "Epoch 3/5, Batch 5050, Loss: 7661521.5000\n",
            "Epoch 3/5, Batch 5100, Loss: 7834819.5000\n",
            "Epoch 3/5, Batch 5150, Loss: 5735233.5000\n",
            "Epoch 3/5, Batch 5200, Loss: 18574986.0000\n",
            "Epoch 3/5, Batch 5250, Loss: 17545370.0000\n",
            "Epoch 3/5, Batch 5300, Loss: 22287234.0000\n",
            "Epoch 3/5, Batch 5350, Loss: 732515776.0000\n",
            "Epoch 3/5, Batch 5400, Loss: 23807946.0000\n",
            "Epoch 3/5, Batch 5450, Loss: 10621103.0000\n",
            "Epoch 3/5, Batch 5500, Loss: 2235901.7500\n",
            "Epoch 3/5, Batch 5550, Loss: 6670888.0000\n",
            "Epoch 3/5, Batch 5600, Loss: 28148878.0000\n",
            "Epoch 3/5, Batch 5650, Loss: 5285000.0000\n",
            "Epoch 3/5, Batch 5700, Loss: 2383553.5000\n",
            "Epoch 3/5, Batch 5750, Loss: 3411393.5000\n",
            "Epoch 3/5, Batch 5800, Loss: 31454568.0000\n",
            "Epoch 3/5, Batch 5850, Loss: 10382844.0000\n",
            "Epoch 3/5, Batch 5900, Loss: 8808531.0000\n",
            "Epoch 3/5, Batch 5950, Loss: 1866184.2500\n",
            "Epoch 3/5, Batch 6000, Loss: 4017235.5000\n",
            "Epoch 3/5, Batch 6050, Loss: 6226441.0000\n",
            "Epoch 3/5, Batch 6100, Loss: 5069354.0000\n",
            "Epoch 3/5, Batch 6150, Loss: 4102649.5000\n",
            "Epoch 3/5, Batch 6200, Loss: 7082877.0000\n",
            "Epoch 3/5, Batch 6250, Loss: 6544298.5000\n",
            "Epoch 3/5, Batch 6300, Loss: 4880802.0000\n",
            "Epoch 3/5, Batch 6350, Loss: 10129626.0000\n",
            "Epoch 3/5, Batch 6400, Loss: 11605621.0000\n",
            "Epoch 3/5, Batch 6450, Loss: 46693304.0000\n",
            "Epoch 3/5, Batch 6500, Loss: 10246860.0000\n",
            "Epoch 3/5:\n",
            "  Train Loss: 13871824.4665\n",
            "  Val Loss: 9232049.4400\n",
            "  Val MAE: 1198.54\n",
            "  Val RMSE: 3038.63\n",
            "  Val MAPE: 782.75%\n",
            "  Val R²: 0.9753\n",
            "Epoch 4/5, Batch 0, Loss: 3174669.5000\n",
            "Epoch 4/5, Batch 50, Loss: 13756466.0000\n",
            "Epoch 4/5, Batch 100, Loss: 2536188.5000\n",
            "Epoch 4/5, Batch 150, Loss: 15350284.0000\n",
            "Epoch 4/5, Batch 200, Loss: 5002619.0000\n",
            "Epoch 4/5, Batch 250, Loss: 3793504.0000\n",
            "Epoch 4/5, Batch 300, Loss: 5330350.0000\n",
            "Epoch 4/5, Batch 350, Loss: 8368662.0000\n",
            "Epoch 4/5, Batch 400, Loss: 43995136.0000\n",
            "Epoch 4/5, Batch 450, Loss: 3810717.0000\n",
            "Epoch 4/5, Batch 500, Loss: 11314223.0000\n",
            "Epoch 4/5, Batch 550, Loss: 21714744.0000\n",
            "Epoch 4/5, Batch 600, Loss: 11698383.0000\n",
            "Epoch 4/5, Batch 650, Loss: 17427998.0000\n",
            "Epoch 4/5, Batch 700, Loss: 8473124.0000\n",
            "Epoch 4/5, Batch 750, Loss: 10167604.0000\n",
            "Epoch 4/5, Batch 800, Loss: 40723416.0000\n",
            "Epoch 4/5, Batch 850, Loss: 27579072.0000\n",
            "Epoch 4/5, Batch 900, Loss: 6982639.0000\n",
            "Epoch 4/5, Batch 950, Loss: 7359243.5000\n",
            "Epoch 4/5, Batch 1000, Loss: 17640868.0000\n",
            "Epoch 4/5, Batch 1050, Loss: 59713980.0000\n",
            "Epoch 4/5, Batch 1100, Loss: 17813442.0000\n",
            "Epoch 4/5, Batch 1150, Loss: 19197172.0000\n",
            "Epoch 4/5, Batch 1200, Loss: 5915817.0000\n",
            "Epoch 4/5, Batch 1250, Loss: 2745981.7500\n",
            "Epoch 4/5, Batch 1300, Loss: 17076582.0000\n",
            "Epoch 4/5, Batch 1350, Loss: 5448983.0000\n",
            "Epoch 4/5, Batch 1400, Loss: 3709156.0000\n",
            "Epoch 4/5, Batch 1450, Loss: 6662842.0000\n",
            "Epoch 4/5, Batch 1500, Loss: 39348208.0000\n",
            "Epoch 4/5, Batch 1550, Loss: 7245762.0000\n",
            "Epoch 4/5, Batch 1600, Loss: 9899272.0000\n",
            "Epoch 4/5, Batch 1650, Loss: 53443640.0000\n",
            "Epoch 4/5, Batch 1700, Loss: 15035949.0000\n",
            "Epoch 4/5, Batch 1750, Loss: 17475446.0000\n",
            "Epoch 4/5, Batch 1800, Loss: 2172111.7500\n",
            "Epoch 4/5, Batch 1850, Loss: 7671954.5000\n",
            "Epoch 4/5, Batch 1900, Loss: 6194464.5000\n",
            "Epoch 4/5, Batch 1950, Loss: 5875012.5000\n",
            "Epoch 4/5, Batch 2000, Loss: 12823811.0000\n",
            "Epoch 4/5, Batch 2050, Loss: 11182191.0000\n",
            "Epoch 4/5, Batch 2100, Loss: 3423180.0000\n",
            "Epoch 4/5, Batch 2150, Loss: 5752739.5000\n",
            "Epoch 4/5, Batch 2200, Loss: 30217708.0000\n",
            "Epoch 4/5, Batch 2250, Loss: 15075822.0000\n",
            "Epoch 4/5, Batch 2300, Loss: 7157225.0000\n",
            "Epoch 4/5, Batch 2350, Loss: 2199078.2500\n",
            "Epoch 4/5, Batch 2400, Loss: 13332186.0000\n",
            "Epoch 4/5, Batch 2450, Loss: 10978637.0000\n",
            "Epoch 4/5, Batch 2500, Loss: 23632730.0000\n",
            "Epoch 4/5, Batch 2550, Loss: 8108031.0000\n",
            "Epoch 4/5, Batch 2600, Loss: 5268671.0000\n",
            "Epoch 4/5, Batch 2650, Loss: 8692109.0000\n",
            "Epoch 4/5, Batch 2700, Loss: 7439114.0000\n",
            "Epoch 4/5, Batch 2750, Loss: 7861110.0000\n",
            "Epoch 4/5, Batch 2800, Loss: 12050794.0000\n",
            "Epoch 4/5, Batch 2850, Loss: 4116384.2500\n",
            "Epoch 4/5, Batch 2900, Loss: 2609166.7500\n",
            "Epoch 4/5, Batch 2950, Loss: 2903202.0000\n",
            "Epoch 4/5, Batch 3000, Loss: 10898958.0000\n",
            "Epoch 4/5, Batch 3050, Loss: 6260553.0000\n",
            "Epoch 4/5, Batch 3100, Loss: 2737656.5000\n",
            "Epoch 4/5, Batch 3150, Loss: 28522754.0000\n",
            "Epoch 4/5, Batch 3200, Loss: 6102569.0000\n",
            "Epoch 4/5, Batch 3250, Loss: 5148772.0000\n",
            "Epoch 4/5, Batch 3300, Loss: 1408732.7500\n",
            "Epoch 4/5, Batch 3350, Loss: 5610268.5000\n",
            "Epoch 4/5, Batch 3400, Loss: 9269623.0000\n",
            "Epoch 4/5, Batch 3450, Loss: 6680753.0000\n",
            "Epoch 4/5, Batch 3500, Loss: 38015176.0000\n",
            "Epoch 4/5, Batch 3550, Loss: 37680516.0000\n",
            "Epoch 4/5, Batch 3600, Loss: 4581281.0000\n",
            "Epoch 4/5, Batch 3650, Loss: 6523857.5000\n",
            "Epoch 4/5, Batch 3700, Loss: 4409035.0000\n",
            "Epoch 4/5, Batch 3750, Loss: 1448161.1250\n",
            "Epoch 4/5, Batch 3800, Loss: 5090321.0000\n",
            "Epoch 4/5, Batch 3850, Loss: 2427466.2500\n",
            "Epoch 4/5, Batch 3900, Loss: 2373060.2500\n",
            "Epoch 4/5, Batch 3950, Loss: 3629945.5000\n",
            "Epoch 4/5, Batch 4000, Loss: 2455526.5000\n",
            "Epoch 4/5, Batch 4050, Loss: 4001384.0000\n",
            "Epoch 4/5, Batch 4100, Loss: 5037438.5000\n",
            "Epoch 4/5, Batch 4150, Loss: 12605534.0000\n",
            "Epoch 4/5, Batch 4200, Loss: 2840956.0000\n",
            "Epoch 4/5, Batch 4250, Loss: 9848834.0000\n",
            "Epoch 4/5, Batch 4300, Loss: 4215540.0000\n",
            "Epoch 4/5, Batch 4350, Loss: 15503290.0000\n",
            "Epoch 4/5, Batch 4400, Loss: 7465270.5000\n",
            "Epoch 4/5, Batch 4450, Loss: 11647557.0000\n",
            "Epoch 4/5, Batch 4500, Loss: 3734687.0000\n",
            "Epoch 4/5, Batch 4550, Loss: 35426360.0000\n",
            "Epoch 4/5, Batch 4600, Loss: 30179582.0000\n",
            "Epoch 4/5, Batch 4650, Loss: 5151436.5000\n",
            "Epoch 4/5, Batch 4700, Loss: 3899832.7500\n",
            "Epoch 4/5, Batch 4750, Loss: 19205768.0000\n",
            "Epoch 4/5, Batch 4800, Loss: 17162442.0000\n",
            "Epoch 4/5, Batch 4850, Loss: 7119579.0000\n",
            "Epoch 4/5, Batch 4900, Loss: 3924033.5000\n",
            "Epoch 4/5, Batch 4950, Loss: 3903290.5000\n",
            "Epoch 4/5, Batch 5000, Loss: 6197077.0000\n",
            "Epoch 4/5, Batch 5050, Loss: 10040753.0000\n",
            "Epoch 4/5, Batch 5100, Loss: 5532457.0000\n",
            "Epoch 4/5, Batch 5150, Loss: 6261273.0000\n",
            "Epoch 4/5, Batch 5200, Loss: 16956586.0000\n",
            "Epoch 4/5, Batch 5250, Loss: 2349692.5000\n",
            "Epoch 4/5, Batch 5300, Loss: 33782904.0000\n",
            "Epoch 4/5, Batch 5350, Loss: 5245796.0000\n",
            "Epoch 4/5, Batch 5400, Loss: 14076784.0000\n",
            "Epoch 4/5, Batch 5450, Loss: 9160226.0000\n",
            "Epoch 4/5, Batch 5500, Loss: 7322430.5000\n",
            "Epoch 4/5, Batch 5550, Loss: 6583861.0000\n",
            "Epoch 4/5, Batch 5600, Loss: 4986378.5000\n",
            "Epoch 4/5, Batch 5650, Loss: 12628634.0000\n",
            "Epoch 4/5, Batch 5700, Loss: 4106048.0000\n",
            "Epoch 4/5, Batch 5750, Loss: 11361310.0000\n",
            "Epoch 4/5, Batch 5800, Loss: 21855830.0000\n",
            "Epoch 4/5, Batch 5850, Loss: 11504038.0000\n",
            "Epoch 4/5, Batch 5900, Loss: 2142938.0000\n",
            "Epoch 4/5, Batch 5950, Loss: 3463612.5000\n",
            "Epoch 4/5, Batch 6000, Loss: 5565031.0000\n",
            "Epoch 4/5, Batch 6050, Loss: 5863917.5000\n",
            "Epoch 4/5, Batch 6100, Loss: 3208799.7500\n",
            "Epoch 4/5, Batch 6150, Loss: 3885723.2500\n",
            "Epoch 4/5, Batch 6200, Loss: 3037736.0000\n",
            "Epoch 4/5, Batch 6250, Loss: 10886313.0000\n",
            "Epoch 4/5, Batch 6300, Loss: 3044976.2500\n",
            "Epoch 4/5, Batch 6350, Loss: 3024136.5000\n",
            "Epoch 4/5, Batch 6400, Loss: 3303017.2500\n",
            "Epoch 4/5, Batch 6450, Loss: 21203328.0000\n",
            "Epoch 4/5, Batch 6500, Loss: 3210202.5000\n",
            "Epoch 4/5:\n",
            "  Train Loss: 15447762.9559\n",
            "  Val Loss: 6441071.9615\n",
            "  Val MAE: 1059.72\n",
            "  Val RMSE: 2538.10\n",
            "  Val MAPE: 286.07%\n",
            "  Val R²: 0.9828\n",
            "Epoch 5/5, Batch 0, Loss: 9741362.0000\n",
            "Epoch 5/5, Batch 50, Loss: 24296174.0000\n",
            "Epoch 5/5, Batch 100, Loss: 6105217.5000\n",
            "Epoch 5/5, Batch 150, Loss: 27631878.0000\n",
            "Epoch 5/5, Batch 200, Loss: 10220801.0000\n",
            "Epoch 5/5, Batch 250, Loss: 1918987.7500\n",
            "Epoch 5/5, Batch 300, Loss: 5763568.5000\n",
            "Epoch 5/5, Batch 350, Loss: 24960560.0000\n",
            "Epoch 5/5, Batch 400, Loss: 4428424.0000\n",
            "Epoch 5/5, Batch 450, Loss: 6337570.5000\n",
            "Epoch 5/5, Batch 500, Loss: 3960599.2500\n",
            "Epoch 5/5, Batch 550, Loss: 10903213.0000\n",
            "Epoch 5/5, Batch 600, Loss: 5736545.0000\n",
            "Epoch 5/5, Batch 650, Loss: 2927551.2500\n",
            "Epoch 5/5, Batch 700, Loss: 155430624.0000\n",
            "Epoch 5/5, Batch 750, Loss: 4749534.5000\n",
            "Epoch 5/5, Batch 800, Loss: 25175320.0000\n",
            "Epoch 5/5, Batch 850, Loss: 4108566.7500\n",
            "Epoch 5/5, Batch 900, Loss: 8418633.0000\n",
            "Epoch 5/5, Batch 950, Loss: 8585076.0000\n",
            "Epoch 5/5, Batch 1000, Loss: 9437274.0000\n",
            "Epoch 5/5, Batch 1050, Loss: 4808944.0000\n",
            "Epoch 5/5, Batch 1100, Loss: 4931149.5000\n",
            "Epoch 5/5, Batch 1150, Loss: 11033843.0000\n",
            "Epoch 5/5, Batch 1200, Loss: 9011281.0000\n",
            "Epoch 5/5, Batch 1250, Loss: 2826225.0000\n",
            "Epoch 5/5, Batch 1300, Loss: 4510692.0000\n",
            "Epoch 5/5, Batch 1350, Loss: 23464222.0000\n",
            "Epoch 5/5, Batch 1400, Loss: 1983289.3750\n",
            "Epoch 5/5, Batch 1450, Loss: 4327983.0000\n",
            "Epoch 5/5, Batch 1500, Loss: 4785039.0000\n",
            "Epoch 5/5, Batch 1550, Loss: 3951257.0000\n",
            "Epoch 5/5, Batch 1600, Loss: 7073713.0000\n",
            "Epoch 5/5, Batch 1650, Loss: 12988375.0000\n",
            "Epoch 5/5, Batch 1700, Loss: 2269005.5000\n",
            "Epoch 5/5, Batch 1750, Loss: 464414720.0000\n",
            "Epoch 5/5, Batch 1800, Loss: 11152452.0000\n",
            "Epoch 5/5, Batch 1850, Loss: 6347607.0000\n",
            "Epoch 5/5, Batch 1900, Loss: 8942243.0000\n",
            "Epoch 5/5, Batch 1950, Loss: 2893858.5000\n",
            "Epoch 5/5, Batch 2000, Loss: 11413199.0000\n",
            "Epoch 5/5, Batch 2050, Loss: 7250898.0000\n",
            "Epoch 5/5, Batch 2100, Loss: 4578243.0000\n",
            "Epoch 5/5, Batch 2150, Loss: 2363330.2500\n",
            "Epoch 5/5, Batch 2200, Loss: 7845663.0000\n",
            "Epoch 5/5, Batch 2250, Loss: 8151110.5000\n",
            "Epoch 5/5, Batch 2300, Loss: 3627443.5000\n",
            "Epoch 5/5, Batch 2350, Loss: 3034309.5000\n",
            "Epoch 5/5, Batch 2400, Loss: 7370114.5000\n",
            "Epoch 5/5, Batch 2450, Loss: 11707474.0000\n",
            "Epoch 5/5, Batch 2500, Loss: 8863051.0000\n",
            "Epoch 5/5, Batch 2550, Loss: 2380641.7500\n",
            "Epoch 5/5, Batch 2600, Loss: 2626378.0000\n",
            "Epoch 5/5, Batch 2650, Loss: 8866350.0000\n",
            "Epoch 5/5, Batch 2700, Loss: 20150576.0000\n",
            "Epoch 5/5, Batch 2750, Loss: 1017258.1250\n",
            "Epoch 5/5, Batch 2800, Loss: 7182162.0000\n",
            "Epoch 5/5, Batch 2850, Loss: 4778419.0000\n",
            "Epoch 5/5, Batch 2900, Loss: 7529525.0000\n",
            "Epoch 5/5, Batch 2950, Loss: 2802883.5000\n",
            "Epoch 5/5, Batch 3000, Loss: 3037263.0000\n",
            "Epoch 5/5, Batch 3050, Loss: 20637898.0000\n",
            "Epoch 5/5, Batch 3100, Loss: 11247152.0000\n",
            "Epoch 5/5, Batch 3150, Loss: 13589826.0000\n",
            "Epoch 5/5, Batch 3200, Loss: 10240502.0000\n",
            "Epoch 5/5, Batch 3250, Loss: 4428595.0000\n",
            "Epoch 5/5, Batch 3300, Loss: 2723826.2500\n",
            "Epoch 5/5, Batch 3350, Loss: 12142019.0000\n",
            "Epoch 5/5, Batch 3400, Loss: 3324318.0000\n",
            "Epoch 5/5, Batch 3450, Loss: 3550131.5000\n",
            "Epoch 5/5, Batch 3500, Loss: 3809095.5000\n",
            "Epoch 5/5, Batch 3550, Loss: 33785152.0000\n",
            "Epoch 5/5, Batch 3600, Loss: 9027561.0000\n",
            "Epoch 5/5, Batch 3650, Loss: 15734696.0000\n",
            "Epoch 5/5, Batch 3700, Loss: 13999045.0000\n",
            "Epoch 5/5, Batch 3750, Loss: 1313834.5000\n",
            "Epoch 5/5, Batch 3800, Loss: 6111247.5000\n",
            "Epoch 5/5, Batch 3850, Loss: 6092248.0000\n",
            "Epoch 5/5, Batch 3900, Loss: 10849217.0000\n",
            "Epoch 5/5, Batch 3950, Loss: 7604338.5000\n",
            "Epoch 5/5, Batch 4000, Loss: 20776330.0000\n",
            "Epoch 5/5, Batch 4050, Loss: 12691914.0000\n",
            "Epoch 5/5, Batch 4100, Loss: 2521361.0000\n",
            "Epoch 5/5, Batch 4150, Loss: 25437576.0000\n",
            "Epoch 5/5, Batch 4200, Loss: 46049712.0000\n",
            "Epoch 5/5, Batch 4250, Loss: 12582229.0000\n",
            "Epoch 5/5, Batch 4300, Loss: 3074242.7500\n",
            "Epoch 5/5, Batch 4350, Loss: 2632669.5000\n",
            "Epoch 5/5, Batch 4400, Loss: 7956488.0000\n",
            "Epoch 5/5, Batch 4450, Loss: 3863653.7500\n",
            "Epoch 5/5, Batch 4500, Loss: 2796704.7500\n",
            "Epoch 5/5, Batch 4550, Loss: 1532643.3750\n",
            "Epoch 5/5, Batch 4600, Loss: 12586636.0000\n",
            "Epoch 5/5, Batch 4650, Loss: 3209633.0000\n",
            "Epoch 5/5, Batch 4700, Loss: 8779418.0000\n",
            "Epoch 5/5, Batch 4750, Loss: 7290188.0000\n",
            "Epoch 5/5, Batch 4800, Loss: 6801850.0000\n",
            "Epoch 5/5, Batch 4850, Loss: 5457998.5000\n",
            "Epoch 5/5, Batch 4900, Loss: 3041557.5000\n",
            "Epoch 5/5, Batch 4950, Loss: 1800472.0000\n",
            "Epoch 5/5, Batch 5000, Loss: 4541137.0000\n",
            "Epoch 5/5, Batch 5050, Loss: 1093329.1250\n",
            "Epoch 5/5, Batch 5100, Loss: 5521851.5000\n",
            "Epoch 5/5, Batch 5150, Loss: 2005551.6250\n",
            "Epoch 5/5, Batch 5200, Loss: 5073283.5000\n",
            "Epoch 5/5, Batch 5250, Loss: 17252612.0000\n",
            "Epoch 5/5, Batch 5300, Loss: 4149488.0000\n",
            "Epoch 5/5, Batch 5350, Loss: 8378226.5000\n",
            "Epoch 5/5, Batch 5400, Loss: 9415499.0000\n",
            "Epoch 5/5, Batch 5450, Loss: 5353553.5000\n",
            "Epoch 5/5, Batch 5500, Loss: 8545551.0000\n",
            "Epoch 5/5, Batch 5550, Loss: 10260689.0000\n",
            "Epoch 5/5, Batch 5600, Loss: 2702619.7500\n",
            "Epoch 5/5, Batch 5650, Loss: 9813253.0000\n",
            "Epoch 5/5, Batch 5700, Loss: 3234051.2500\n",
            "Epoch 5/5, Batch 5750, Loss: 7374916.0000\n",
            "Epoch 5/5, Batch 5800, Loss: 6566632.0000\n",
            "Epoch 5/5, Batch 5850, Loss: 4810267.5000\n",
            "Epoch 5/5, Batch 5900, Loss: 2617553.2500\n",
            "Epoch 5/5, Batch 5950, Loss: 4652657.0000\n",
            "Epoch 5/5, Batch 6000, Loss: 20486480.0000\n",
            "Epoch 5/5, Batch 6050, Loss: 8079748.5000\n",
            "Epoch 5/5, Batch 6100, Loss: 26437786.0000\n",
            "Epoch 5/5, Batch 6150, Loss: 12934404.0000\n",
            "Epoch 5/5, Batch 6200, Loss: 14502963.0000\n",
            "Epoch 5/5, Batch 6250, Loss: 17146258.0000\n",
            "Epoch 5/5, Batch 6300, Loss: 5607840.0000\n",
            "Epoch 5/5, Batch 6350, Loss: 4897530.5000\n",
            "Epoch 5/5, Batch 6400, Loss: 10941372.0000\n",
            "Epoch 5/5, Batch 6450, Loss: 9177148.0000\n",
            "Epoch 5/5, Batch 6500, Loss: 2947395.0000\n",
            "Epoch 5/5:\n",
            "  Train Loss: 11505109.6802\n",
            "  Val Loss: 6602375.4913\n",
            "  Val MAE: 1083.82\n",
            "  Val RMSE: 2569.68\n",
            "  Val MAPE: 491.57%\n",
            "  Val R²: 0.9824\n",
            "\n",
            "✓ Cross validation completed and logged to wandb\n",
            "Final validation metrics: MAE=1083.82, RMSE=2569.68, MAPE=491.57%, R²=0.9824\n"
          ]
        }
      ],
      "execution_count": 51,
      "id": "cross_validation_code"
    },
    {
      "metadata": {
        "id": "final_training_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Final Training & Model Registry"
      ],
      "id": "final_training_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "final_training_code",
        "outputId": "9ff6aecb-5f37-429e-d939-5a283b40afd6"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for final training\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"NBEATS_Final_Training\",\n",
        "    config={\n",
        "        \"stage\": \"final_training\",\n",
        "        \"model_config\": model_config,\n",
        "        \"num_epochs\": 20,\n",
        "        \"batch_size\": 32\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\n=== FINAL TRAINING ===\")\n",
        "\n",
        "# IMPORTANT: Re-process the data to ensure we have clean numpy arrays\n",
        "print(\"Re-processing data for final training...\")\n",
        "ts_processor_final = TimeSeriesDataProcessor(lookback_window=52, forecast_horizon=1)\n",
        "ts_processor_final.fit(train_selected)\n",
        "processed_data_final = ts_processor_final.transform(train_selected)\n",
        "\n",
        "sequences_final = processed_data_final['sequences']\n",
        "targets_final = processed_data_final['targets']\n",
        "metadata_final = processed_data_final['metadata']\n",
        "\n",
        "print(f\"Re-generated {len(sequences_final)} sequences for final training\")\n",
        "\n",
        "if len(sequences_final) == 0:\n",
        "    print(\"❌ No sequences generated for final training. Check data processing.\")\n",
        "    wandb.log({\"final_training_failed\": True, \"reason\": \"no_sequences\"})\n",
        "else:\n",
        "    # Convert to consistent numpy arrays - ENSURE THEY STAY ON CPU\n",
        "    max_features = max([seq.shape[1] if len(seq.shape) > 1 else 1 for seq in sequences_final])\n",
        "    lookback_length = sequences_final[0].shape[0]\n",
        "\n",
        "    padded_sequences_final = []\n",
        "    valid_targets_final = []\n",
        "\n",
        "    for i, (seq, tgt) in enumerate(zip(sequences_final, targets_final)):\n",
        "        if len(seq.shape) == 1:\n",
        "            seq = seq.reshape(-1, 1)\n",
        "\n",
        "        if seq.shape[1] < max_features:\n",
        "            padding = np.zeros((seq.shape[0], max_features - seq.shape[1]), dtype=np.float32)\n",
        "            seq = np.column_stack([seq, padding]).astype(np.float32)\n",
        "        else:\n",
        "            seq = seq.astype(np.float32)\n",
        "\n",
        "        padded_sequences_final.append(seq)\n",
        "        valid_targets_final.append(tgt.astype(np.float32))\n",
        "\n",
        "    # Ensure these are numpy arrays on CPU\n",
        "    sequences_final_np = np.array(padded_sequences_final, dtype=np.float32)\n",
        "    targets_final_np = np.array(valid_targets_final, dtype=np.float32)\n",
        "\n",
        "    print(f\"Final training data shape: {sequences_final_np.shape}\")\n",
        "    print(f\"Final training targets shape: {targets_final_np.shape}\")\n",
        "\n",
        "    # Create dataset with CPU numpy arrays\n",
        "    final_dataset = WalmartDataset(sequences_final_np, targets_final_np)\n",
        "    final_loader = DataLoader(final_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize fresh model for final training\n",
        "    final_model = NBEATSModel(**model_config).to(device)\n",
        "    final_optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "    final_criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"Training on {len(sequences_final_np)} sequences...\")\n",
        "\n",
        "    # Extended training\n",
        "    num_epochs_final = 20\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs_final):\n",
        "        final_model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (sequences_batch, targets_batch) in enumerate(final_loader):\n",
        "            sequences_batch = sequences_batch.to(device)\n",
        "            targets_batch = targets_batch.to(device)\n",
        "\n",
        "            final_optimizer.zero_grad()\n",
        "            outputs = final_model(sequences_batch)\n",
        "            loss = final_criterion(outputs, targets_batch)\n",
        "            loss.backward()\n",
        "            final_optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(final_loader)\n",
        "\n",
        "        if avg_epoch_loss < best_loss:\n",
        "            best_loss = avg_epoch_loss\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Final Training Epoch {epoch+1}/{num_epochs_final}, Loss: {avg_epoch_loss:.4f}')\n",
        "\n",
        "            # Log training progress\n",
        "            wandb.log({\n",
        "                \"final_epoch\": epoch + 1,\n",
        "                \"final_train_loss\": avg_epoch_loss,\n",
        "                \"best_loss\": best_loss\n",
        "            })\n",
        "\n",
        "    # Final evaluation on training data\n",
        "    print(\"\\nEvaluating final model...\")\n",
        "    final_model.eval()\n",
        "    all_final_predictions = []\n",
        "    all_final_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences_batch, targets_batch in final_loader:\n",
        "            sequences_batch = sequences_batch.to(device)\n",
        "            outputs = final_model(sequences_batch)\n",
        "            all_final_predictions.extend(outputs.cpu().numpy())\n",
        "            all_final_targets.extend(targets_batch.numpy())\n",
        "\n",
        "    all_final_predictions = np.array(all_final_predictions).flatten()\n",
        "    all_final_targets = np.array(all_final_targets).flatten()\n",
        "\n",
        "    # Calculate final metrics\n",
        "    final_mae = mean_absolute_error(all_final_targets, all_final_predictions)\n",
        "    final_rmse = np.sqrt(mean_squared_error(all_final_targets, all_final_predictions))\n",
        "    final_r2 = r2_score(all_final_targets, all_final_predictions)\n",
        "\n",
        "    # Safe MAPE calculation\n",
        "    def safe_mape(y_true, y_pred):\n",
        "        mask = y_true != 0\n",
        "        if mask.sum() == 0:\n",
        "            return float('inf')\n",
        "        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "    final_mape = safe_mape(all_final_targets, all_final_predictions)\n",
        "\n",
        "    print(f\"\\nFinal Training Metrics:\")\n",
        "    print(f\"MAE: {final_mae:.2f}\")\n",
        "    print(f\"RMSE: {final_rmse:.2f}\")\n",
        "    print(f\"MAPE: {final_mape:.2f}%\")\n",
        "    print(f\"R²: {final_r2:.4f}\")\n",
        "\n",
        "    # Create complete pipeline\n",
        "    class NBEATSPipeline:\n",
        "        \"\"\"Complete pipeline for N-BEATS inference\"\"\"\n",
        "\n",
        "        def __init__(self, feature_merger, missing_handler, ts_processor, model):\n",
        "            self.feature_merger = feature_merger\n",
        "            self.missing_handler = missing_handler\n",
        "            self.ts_processor = ts_processor\n",
        "            self.model = model\n",
        "            self.model.eval()\n",
        "\n",
        "        def predict(self, X_raw, stores_df=None, features_df=None):\n",
        "            \"\"\"Make predictions on raw test data\"\"\"\n",
        "            # If auxiliary data provided, update the merger\n",
        "            if stores_df is not None or features_df is not None:\n",
        "                self.feature_merger.fit(X_raw, stores_df=stores_df, features_df=features_df)\n",
        "\n",
        "            # Process through pipeline\n",
        "            merged_data = self.feature_merger.transform(X_raw)\n",
        "            cleaned_data = self.missing_handler.transform(merged_data)\n",
        "\n",
        "            # For inference, we need to create sequences from the cleaned data\n",
        "            processed = self.ts_processor.transform(cleaned_data)\n",
        "\n",
        "            if len(processed['sequences']) == 0:\n",
        "                return np.array([])\n",
        "\n",
        "            # Convert to tensor and predict\n",
        "            sequences_tensor = torch.FloatTensor(processed['sequences']).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predictions = self.model(sequences_tensor)\n",
        "\n",
        "            return predictions.cpu().numpy().flatten()\n",
        "\n",
        "    # Create final pipeline\n",
        "    final_pipeline = NBEATSPipeline(\n",
        "        feature_merger=feature_merger,\n",
        "        missing_handler=missing_handler,\n",
        "        ts_processor=ts_processor_final,  # Use the final processor\n",
        "        model=final_model\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== SAVING FINAL MODEL ===\")\n",
        "\n",
        "    # Save pipeline with cloudpickle\n",
        "    try:\n",
        "        import cloudpickle\n",
        "    except ImportError:\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', 'cloudpickle'])\n",
        "        import cloudpickle\n",
        "\n",
        "    # Create filename\n",
        "    pipeline_filename = f\"nbeats_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
        "\n",
        "    # Save with cloudpickle\n",
        "    with open(pipeline_filename, 'wb') as f:\n",
        "        cloudpickle.dump(final_pipeline, f)\n",
        "\n",
        "    print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "\n",
        "    # Try to upload to wandb with error handling\n",
        "    try:\n",
        "        # Create model artifact\n",
        "        model_artifact = wandb.Artifact(\n",
        "            name=\"NBEATS_pipeline\",\n",
        "            type=\"model\",\n",
        "            description=\"Final N-BEATS pipeline for Walmart sales forecasting\",\n",
        "            metadata={\n",
        "                \"train_mae\": float(final_mae),\n",
        "                \"train_rmse\": float(final_rmse),\n",
        "                \"train_mape\": float(final_mape) if not np.isinf(final_mape) else 0.0,\n",
        "                \"train_r2\": float(final_r2),\n",
        "                \"sequences_count\": len(sequences_final_np),\n",
        "                \"training_samples\": len(all_final_targets),\n",
        "                \"model_type\": \"NBEATS\",\n",
        "                \"lookback_window\": 52,\n",
        "                \"forecast_horizon\": 1,\n",
        "                \"num_stacks\": model_config[\"num_stacks\"],\n",
        "                \"num_blocks_per_stack\": model_config[\"num_blocks_per_stack\"]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Add model file to artifact\n",
        "        model_artifact.add_file(pipeline_filename)\n",
        "\n",
        "        # Log artifact\n",
        "        wandb.log_artifact(model_artifact)\n",
        "        print(\"✓ Model artifact logged to wandb successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error uploading to wandb: {e}\")\n",
        "        print(\"Model saved locally - you can manually upload later\")\n",
        "\n",
        "        # Log just the metrics without artifact\n",
        "        wandb.log({\n",
        "            'final_train_mae': final_mae,\n",
        "            'final_train_rmse': final_rmse,\n",
        "            'final_train_mape': final_mape if not np.isinf(final_mape) else 0.0,\n",
        "            'final_train_r2': final_r2,\n",
        "            'model_saved_locally': pipeline_filename\n",
        "        })\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL MODEL SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Model Type: N-BEATS\")\n",
        "    print(f\"Training Sequences: {len(all_final_targets):,}\")\n",
        "    print(f\"Lookback Window: 52 weeks\")\n",
        "    print(f\"Forecast Horizon: 1 week\")\n",
        "    print(f\"Training MAE: {final_mae:.2f}\")\n",
        "    print(f\"Training RMSE: {final_rmse:.2f}\")\n",
        "    print(f\"Training MAPE: {final_mape:.2f}%\")\n",
        "    print(f\"Training R²: {final_r2:.4f}\")\n",
        "    print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "wandb.finish()\n",
        "print(\"\\n✓ Final training completed and model saved!\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">NBEATS_Final_Training</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/n9d25szm' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/n9d25szm</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_235920-n9d25szm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250705_000740-ckvr2t58</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/ckvr2t58' target=\"_blank\">NBEATS_Final_Training</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/ckvr2t58' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/ckvr2t58</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL TRAINING ===\n",
            "Re-processing data for final training...\n",
            "Found 3331 store-dept combinations\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Generated 261083 valid sequences from 3331 store-dept combinations\n",
            "Re-generated 261083 sequences for final training\n",
            "Final training data shape: (261083, 52, 3)\n",
            "Final training targets shape: (261083, 1)\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock initialized - input_size: 52, layer_size: 256, theta_size: 52\n",
            "NBeatsBlock theta_b weight shape (in __init__): torch.Size([52, 256])\n",
            "NBeatsBlock theta_f weight shape (in __init__): torch.Size([52, 256])\n",
            "Training on 261083 sequences...\n",
            "Final Training Epoch 5/20, Loss: 11558248.3020\n",
            "Final Training Epoch 10/20, Loss: 9858332.7354\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "final_training_code"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKS7BCXQsf37"
      },
      "id": "LKS7BCXQsf37",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}