{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leWnOYlVBGEl",
        "outputId": "67bfaf53-c6da-4661-980f-97d0e2860b6f"
      },
      "id": "leWnOYlVBGEl",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMRWS7raBHPG",
        "outputId": "7183b7d7-e149-4693-8766-d5e6ee250c30"
      },
      "id": "TMRWS7raBHPG",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äò/root/.kaggle‚Äô: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7xX2rfmzBIJN"
      },
      "id": "7xX2rfmzBIJN",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "uCOxS2NxBJRV"
      },
      "id": "uCOxS2NxBJRV",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEvzr6khBKUM",
        "outputId": "afecae88-2e45-46aa-8029-6b6b807e6e4b"
      },
      "id": "HEvzr6khBKUM",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb -qU"
      ],
      "metadata": {
        "id": "7Nk3D0umBMzJ"
      },
      "id": "7Nk3D0umBMzJ",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# ! unzip /content/train.csv.zip\n",
        "# ! unzip /content/test.csv.zip\n",
        "# ! unzip /content/features.csv.zip\n",
        "# ! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "id": "YXsjyDYTBLfp"
      },
      "id": "YXsjyDYTBLfp",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "d16209b9-9003-4bb6-b311-1b4a83b19341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Walmart Sales Forecasting Inference...\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import wandb\n",
        "import joblib\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Starting Walmart Sales Forecasting Inference...\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "stores = pd.read_csv('/content/stores.csv')\n",
        "features = pd.read_csv('/content/features.csv')\n",
        "sample_submission = pd.read_csv('/content/sampleSubmission.csv')\n",
        "\n",
        "# Convert dates\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "print(\"Data loading completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7BTSD8e_lLg",
        "outputId": "274ac0f7-a47c-4e1f-f051-a5056a50a8ce"
      },
      "id": "g7BTSD8e_lLg",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Data loading completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOU STILL NEED THESE CLASS DEFINITIONS (KEEP THEM)\n",
        "class WalmartFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Your exact fit method\n",
        "        X_with_stores = X.merge(stores, on='Store', how='left')\n",
        "\n",
        "        if 'Type' in X_with_stores.columns:\n",
        "            le = LabelEncoder()\n",
        "            le.fit(X_with_stores['Type'].astype(str))\n",
        "            self.label_encoders['Type'] = le\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Your exact transform method\n",
        "        X = X.copy()\n",
        "\n",
        "        # Merge with additional data\n",
        "        X = X.merge(stores, on='Store', how='left')\n",
        "        X = X.merge(features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        # Handle IsHoliday conflict\n",
        "        if 'IsHoliday_x' in X.columns and 'IsHoliday_y' in X.columns:\n",
        "            X['IsHoliday'] = X['IsHoliday_y'].fillna(X['IsHoliday_x'])\n",
        "            X = X.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "\n",
        "        # Basic date features\n",
        "        X['Year'] = X['Date'].dt.year\n",
        "        X['Month'] = X['Date'].dt.month\n",
        "        X['Week'] = X['Date'].dt.isocalendar().week\n",
        "        X['Weekday'] = X['Date'].dt.dayofweek\n",
        "        X['Quarter'] = X['Date'].dt.quarter\n",
        "        X['Is_Weekend'] = (X['Weekday'] >= 5).astype(int)\n",
        "\n",
        "        # Simple holiday feature\n",
        "        X['IsHoliday'] = X['IsHoliday'].fillna(False).astype(int)\n",
        "\n",
        "        # Markdown features\n",
        "        markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        for col in markdown_cols:\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(0)\n",
        "\n",
        "        # Create total markdown\n",
        "        if any(col in X.columns for col in markdown_cols):\n",
        "            existing_cols = [col for col in markdown_cols if col in X.columns]\n",
        "            X['Total_MarkDown'] = X[existing_cols].sum(axis=1)\n",
        "            X['Num_MarkDowns'] = (X[existing_cols] > 0).sum(axis=1)\n",
        "\n",
        "        # Economic features\n",
        "        if 'Fuel_Price' in X.columns and 'CPI' in X.columns:\n",
        "            X['Fuel_Price_to_CPI'] = X['Fuel_Price'] / (X['CPI'] + 1e-8)\n",
        "\n",
        "        if 'Temperature' in X.columns:\n",
        "            X['Temperature_squared'] = X['Temperature'] ** 2\n",
        "\n",
        "        # Encode categorical variables\n",
        "        for col, le in self.label_encoders.items():\n",
        "            if col in X.columns:\n",
        "                X[col] = le.transform(X[col].astype(str))\n",
        "\n",
        "        # Fill missing values\n",
        "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        X[numeric_cols] = X[numeric_cols].fillna(0)\n",
        "\n",
        "        # Select features\n",
        "        cols_to_drop = ['Date']\n",
        "        if 'Weekly_Sales' in X.columns:\n",
        "            cols_to_drop.append('Weekly_Sales')\n",
        "        feature_cols = [col for col in X.columns if col not in cols_to_drop]\n",
        "\n",
        "        return X[feature_cols]\n",
        "\n",
        "class XGBoostPipeline(BaseEstimator):\n",
        "    def __init__(self, params, feature_engineer, selected_features):\n",
        "        self.params = params\n",
        "        self.feature_engineer = feature_engineer\n",
        "        self.selected_features = selected_features\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Transform features\n",
        "        X_transformed = self.feature_engineer.fit_transform(X)\n",
        "        X_selected = X_transformed[self.selected_features]\n",
        "\n",
        "        # Train XGBoost\n",
        "        dtrain = xgb.DMatrix(X_selected, label=y)\n",
        "        self.model = xgb.train(\n",
        "            params=self.params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=1000,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Transform features\n",
        "        X_transformed = self.feature_engineer.transform(X)\n",
        "        X_selected = X_transformed[self.selected_features]\n",
        "\n",
        "        # Make predictions\n",
        "        dtest = xgb.DMatrix(X_selected)\n",
        "        return self.model.predict(dtest)\n",
        "\n",
        "print(\"Pipeline classes defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8doMliy3JJrG",
        "outputId": "8e2bb252-1e5f-40f5-e9e3-e308d5c9d3a8"
      },
      "id": "8doMliy3JJrG",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BaseEstimator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-3473530419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOU STILL NEED THESE CLASS DEFINITIONS (KEEP THEM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mWalmartFeatureEngineer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.login()\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"Walmart_Forecasting\",\n",
        "    name=\"XGBoost_Inference_Clean\",\n",
        "    group=\"Inference\",\n",
        "    tags=[\"inference\", \"xgboost\", \"components\"]\n",
        ")\n",
        "\n",
        "print(\"Wandb initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "NbIczwsLA3aT",
        "outputId": "971db754-142a-4fb5-bdce-503c357244f7"
      },
      "id": "NbIczwsLA3aT",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250703_214958-2kg58uco</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/Walmart_Forecasting/runs/2kg58uco' target=\"_blank\">XGBoost_Inference_Clean</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/Walmart_Forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/Walmart_Forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/Walmart_Forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/Walmart_Forecasting/runs/2kg58uco' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/Walmart_Forecasting/runs/2kg58uco</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wandb initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading section in inference - replace the model loading part with:\n",
        "\n",
        "print(\"Loading trained model from wandb...\")\n",
        "\n",
        "try:\n",
        "    # Load artifact (standard approach)\n",
        "    artifact = run.use_artifact('walmart_xgboost_pipeline:latest', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    print(f\"‚úÖ Model downloaded to: {artifact_dir}\")\n",
        "\n",
        "    # Load the trained pipeline with joblib\n",
        "    model_path = f\"{artifact_dir}/model.joblib\"\n",
        "    trained_pipeline = joblib.load(model_path)\n",
        "\n",
        "    print(\"‚úÖ Trained XGBoost pipeline loaded successfully!\")\n",
        "    print(f\"Pipeline type: {type(trained_pipeline).__name__}\")\n",
        "    print(f\"Selected features: {len(trained_pipeline.selected_features)}\")\n",
        "\n",
        "    # Log model loading info\n",
        "    wandb.config.update({\n",
        "        \"model_loaded_from\": \"wandb_artifact\",\n",
        "        \"model_type\": \"XGBoostPipeline\",\n",
        "        \"n_features\": len(trained_pipeline.selected_features),\n",
        "        \"artifact_version\": artifact.version\n",
        "    })\n",
        "\n",
        "    model_loaded = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model from wandb: {e}\")\n",
        "    print(\"Full error:\", str(e))\n",
        "    model_loaded = False\n",
        "    trained_pipeline = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjtzBNvVA5Cw",
        "outputId": "672185ef-399f-4b4a-bfea-297b51032210"
      },
      "id": "gjtzBNvVA5Cw",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading trained model from wandb...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model downloaded to: /content/artifacts/walmart_xgboost_pipeline:v1\n",
            "‚ùå Error loading model from wandb: Can't get attribute 'XGBoostPipeline' on <module '__main__'>\n",
            "Full error: Can't get attribute 'XGBoostPipeline' on <module '__main__'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7K6SWSrJR49"
      },
      "id": "s7K6SWSrJR49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple prediction function\n",
        "def make_predictions(test_data):\n",
        "  \"\"\"Make predictions using loaded components\"\"\"\n",
        "  if not model_loaded:\n",
        "      print(\"‚ö†Ô∏è  Model loading failed. Creating baseline model...\")\n",
        "\n",
        "      class BaselineModel:\n",
        "          def predict(self, X):\n",
        "              print(\"Using baseline predictions...\")\n",
        "              np.random.seed(42)\n",
        "\n",
        "              predictions = []\n",
        "              store_size_map = stores.set_index('Store')['Size'].to_dict()\n",
        "\n",
        "              for idx, row in X.iterrows():\n",
        "                  store_size = store_size_map.get(row['Store'], 150000)\n",
        "                  month = pd.to_datetime(row['Date']).month\n",
        "\n",
        "                  if month in [11, 12]:  # Holiday season\n",
        "                      seasonal_mult = 1.3\n",
        "                  elif month in [1, 2]:  # Post-holiday\n",
        "                      seasonal_mult = 0.8\n",
        "                  else:\n",
        "                      seasonal_mult = 1.0\n",
        "\n",
        "                  base_pred = (store_size / 1000) * seasonal_mult\n",
        "                  noise = np.random.normal(0, base_pred * 0.1)\n",
        "                  prediction = max(base_pred + noise, 0)\n",
        "                  predictions.append(prediction)\n",
        "\n",
        "              return np.array(predictions)\n",
        "\n",
        "      trained_pipeline = BaselineModel()\n",
        "      wandb.config.update({\"model_loaded_from\": \"baseline_fallback\"})\n",
        "      print(\"‚ö†Ô∏è  Using baseline model - results may not be optimal!\")\n",
        "\n",
        "  else:\n",
        "        # Use trained components\n",
        "        print(\"Using trained model components...\")\n",
        "\n",
        "        # Transform features using loaded feature engineer\n",
        "        test_transformed = feature_engineer.transform(test_data)\n",
        "        test_selected = test_transformed[selected_features]\n",
        "\n",
        "        # Make predictions using loaded XGBoost model\n",
        "        dtest = xgb.DMatrix(test_selected)\n",
        "        predictions = xgb_model.predict(dtest)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\nGenerating predictions...\")\n",
        "predictions = make_predictions(test)\n",
        "\n",
        "print(\"‚úÖ Predictions generated!\")\n",
        "print(f\"Shape: {predictions.shape}\")\n",
        "print(f\"Range: ${predictions.min():,.2f} to ${predictions.max():,.2f}\")\n",
        "print(f\"Mean: ${predictions.mean():,.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "up9_RoxMA8vB",
        "outputId": "a8e53133-1b36-4b82-d56f-d3762daf12a2"
      },
      "id": "up9_RoxMA8vB",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating predictions...\n",
            "‚ö†Ô∏è  Model loading failed. Creating baseline model...\n",
            "‚ö†Ô∏è  Using baseline model - results may not be optimal!\n",
            "‚úÖ Predictions generated!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-58122694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Predictions generated!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape: {predictions.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Range: ${predictions.min():,.2f} to ${predictions.max():,.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean: ${predictions.mean():,.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and validate submission\n",
        "print(\"\\nCreating submission...\")\n",
        "\n",
        "submission = sample_submission.copy()\n",
        "submission['Weekly_Sales'] = predictions\n",
        "\n",
        "# Validation\n",
        "validation_passed = True\n",
        "issues = []\n",
        "\n",
        "if submission.shape != sample_submission.shape:\n",
        "    issues.append(\"Shape mismatch\")\n",
        "    validation_passed = False\n",
        "\n",
        "if submission.isnull().sum().sum() > 0:\n",
        "    issues.append(\"Missing values found\")\n",
        "    validation_passed = False\n",
        "\n",
        "negative_count = (predictions < 0).sum()\n",
        "if negative_count > 0:\n",
        "    issues.append(f\"{negative_count} negative predictions\")\n",
        "\n",
        "print(f\"Validation: {'‚úÖ PASSED' if validation_passed else '‚ö†Ô∏è ISSUES'}\")\n",
        "if issues:\n",
        "    for issue in issues:\n",
        "        print(f\"  - {issue}\")\n",
        "\n",
        "# Save submission\n",
        "submission_filename = 'walmart_submission_clean.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"\\nüíæ Submission saved: {submission_filename}\")\n",
        "\n",
        "# Log results\n",
        "wandb.log({\n",
        "    \"predictions_mean\": float(predictions.mean()),\n",
        "    \"predictions_std\": float(predictions.std()),\n",
        "    \"validation_passed\": validation_passed,\n",
        "    \"negative_predictions\": int(negative_count),\n",
        "    \"submission_ready\": validation_passed\n",
        "})\n",
        "\n",
        "print(f\"\\nüéØ Ready for Kaggle submission!\")\n",
        "print(f\"üìä Wandb: {run.url}\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "9DErV-oeA-Jy",
        "outputId": "e6b7abcc-0850-43a6-e28f-5cd503c1732d"
      },
      "id": "9DErV-oeA-Jy",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating submission...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'<' not supported between instances of 'NoneType' and 'int'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-2852671309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mvalidation_passed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mnegative_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnegative_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0missues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{negative_count} negative predictions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}