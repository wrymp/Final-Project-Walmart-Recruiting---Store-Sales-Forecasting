{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xc_xtTn6ECN",
        "outputId": "bab9b418-3b66-42ea-a6d6-1a9baab6e5cb"
      },
      "id": "0xc_xtTn6ECN",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "3Q92W4PQ6EaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb3b685-328a-4ac5-dbf3-314b5c31fc4d"
      },
      "id": "3Q92W4PQ6EaA",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "MPwqMv2z6H6S"
      },
      "id": "MPwqMv2z6H6S",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# ! unzip /content/train.csv.zip\n",
        "# ! unzip /content/test.csv.zip\n",
        "# ! unzip /content/features.csv.zip\n",
        "# ! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "id": "jQr6E5zG6KBU"
      },
      "id": "jQr6E5zG6KBU",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb -qU\n",
        "# !pip uninstall -y pmdarima numpy scipy statsmodels\n",
        "# !pip install numpy==1.24.4 scipy==1.10.1 statsmodels==0.13.5 pmdarima==2.0.3"
      ],
      "metadata": {
        "id": "myvAj7pC7CyH"
      },
      "id": "myvAj7pC7CyH",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "c5Vm5Z5I7DRW"
      },
      "id": "c5Vm5Z5I7DRW",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Enhanced Prophet Time Series Forecasting for Walmart Sales\n",
        "\n",
        "Rewritten to focus on time series analysis with Prophet only.\n",
        "\"\"\"\n",
        "\n",
        "# Setup and installations\n",
        "!pip install kaggle wandb prophet dill -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Data download (uncomment if needed)\n",
        "# !kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "# !unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# !unzip /content/train.csv.zip\n",
        "# !unzip /content/test.csv.zip\n",
        "# !unzip /content/features.csv.zip\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import dill\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings and logging\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger('prophet').setLevel(logging.ERROR)\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
        "\n",
        "# WandB setup\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_TimeSeries_Optimized\")\n",
        "\n",
        "# =============================================================================\n",
        "# Block 1: Data Loading and Initial Setup\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "features_df = pd.read_csv(\"/content/features.csv\")\n",
        "stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "\n",
        "# Convert dates\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "print(f\"Data loaded: Train {train_df.shape}, Test {test_df.shape}\")\n",
        "print(f\"Train columns: {list(train_df.columns)}\")\n",
        "print(f\"Features columns: {list(features_df.columns)}\")\n",
        "print(f\"Date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "\n",
        "# Log basic info\n",
        "wandb.log({\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"n_stores\": train_df['Store'].nunique(),\n",
        "    \"n_departments\": train_df['Dept'].nunique(),\n",
        "    \"date_range_days\": (train_df['Date'].max() - train_df['Date'].min()).days\n",
        "})\n",
        "\n",
        "# =============================================================================\n",
        "# Block 2: Time Series Feature Engineering\n",
        "# =============================================================================\n",
        "\n",
        "class TimeSeriesFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Focused time series feature engineering for Prophet\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "        print(f\"Input shape: {df.shape}\")\n",
        "        print(f\"Input columns: {list(df.columns)}\")\n",
        "\n",
        "        # Merge external features with proper suffix handling\n",
        "        print(\"Merging with features...\")\n",
        "        df = df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
        "        print(f\"After features merge: {df.shape}\")\n",
        "\n",
        "        print(\"Merging with stores...\")\n",
        "        df = df.merge(stores_df, on='Store', how='left')\n",
        "        print(f\"After stores merge: {df.shape}\")\n",
        "        print(f\"Columns after merge: {list(df.columns)}\")\n",
        "\n",
        "        # Handle IsHoliday column conflicts\n",
        "        if 'IsHoliday_feat' in df.columns:\n",
        "            # Use the original IsHoliday, fill missing with features version\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(df['IsHoliday_feat'])\n",
        "            df = df.drop('IsHoliday_feat', axis=1)\n",
        "\n",
        "        # Ensure IsHoliday exists and is properly formatted\n",
        "        if 'IsHoliday' in df.columns:\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(False).astype(int)\n",
        "        else:\n",
        "            print(\"Warning: IsHoliday column not found, creating default\")\n",
        "            df['IsHoliday'] = 0\n",
        "\n",
        "        # Fill missing values efficiently\n",
        "        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "                print(f\"Filled {col}: {df[col].isnull().sum()} missing values\")\n",
        "            else:\n",
        "                print(f\"Warning: {col} not found in data\")\n",
        "\n",
        "        # Markdown columns (promotional effects)\n",
        "        markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        for col in markdown_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(0)\n",
        "            else:\n",
        "                print(f\"Creating {col} with default values\")\n",
        "                df[col] = 0\n",
        "\n",
        "        # Store type encoding\n",
        "        if 'Type' in df.columns:\n",
        "            df['Type'] = df['Type'].fillna('A')\n",
        "            print(f\"Store types: {df['Type'].value_counts()}\")\n",
        "        else:\n",
        "            print(\"Warning: Type column not found\")\n",
        "            df['Type'] = 'A'\n",
        "\n",
        "        if 'Size' in df.columns:\n",
        "            df['Size'] = df['Size'].fillna(df['Size'].median())\n",
        "        else:\n",
        "            print(\"Warning: Size column not found\")\n",
        "            df['Size'] = 151315  # Approximate median from typical Walmart data\n",
        "\n",
        "        # Time-based features for Prophet regressors\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "        # Seasonal indicators (key for retail)\n",
        "        df['IsQ4'] = (df['Quarter'] == 4).astype(int)  # Holiday season\n",
        "        df['IsBackToSchool'] = (df['Month'] == 8).astype(int)  # August\n",
        "        df['IsSpringCleaning'] = (df['Month'] == 4).astype(int)  # April\n",
        "        df['IsSummer'] = df['Month'].isin([6, 7, 8]).astype(int)  # Summer\n",
        "\n",
        "        # Create total markdown effect\n",
        "        df['TotalMarkDown'] = sum(df[col] for col in markdown_cols if col in df.columns)\n",
        "\n",
        "        # Economic indicators\n",
        "        if 'CPI' in df.columns and 'Unemployment' in df.columns:\n",
        "            df['EconomicIndex'] = df['CPI'] * df['Unemployment']\n",
        "        else:\n",
        "            df['EconomicIndex'] = 0\n",
        "\n",
        "        # Store size category\n",
        "        if 'Size' in df.columns:\n",
        "            df['StoreSizeCategory'] = pd.cut(df['Size'], bins=3, labels=[0, 1, 2]).astype(int)\n",
        "        else:\n",
        "            df['StoreSizeCategory'] = 1\n",
        "\n",
        "        print(f\"Final processed shape: {df.shape}\")\n",
        "        print(f\"Missing values check:\")\n",
        "        for col in ['IsHoliday', 'Temperature', 'Fuel_Price', 'TotalMarkDown']:\n",
        "            if col in df.columns:\n",
        "                print(f\"  {col}: {df[col].isnull().sum()} missing\")\n",
        "\n",
        "        return df\n",
        "\n",
        "# =============================================================================\n",
        "# Block 3: Prophet Model for Time Series\n",
        "# =============================================================================\n",
        "\n",
        "class SuppressOutput:\n",
        "    \"\"\"Context manager to suppress Prophet output\"\"\"\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        self._original_stderr = sys.stderr\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "        sys.stderr = open(os.devnull, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stderr.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "        sys.stderr = self._original_stderr\n",
        "\n",
        "class WalmartProphetModel(BaseEstimator):\n",
        "    \"\"\"Optimized Prophet model for Walmart sales forecasting\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 changepoint_prior_scale=0.05,\n",
        "                 seasonality_prior_scale=10.0,\n",
        "                 holidays_prior_scale=10.0,\n",
        "                 seasonality_mode='multiplicative',\n",
        "                 min_samples=20):\n",
        "        self.changepoint_prior_scale = changepoint_prior_scale\n",
        "        self.seasonality_prior_scale = seasonality_prior_scale\n",
        "        self.holidays_prior_scale = holidays_prior_scale\n",
        "        self.seasonality_mode = seasonality_mode\n",
        "        self.min_samples = min_samples\n",
        "        self.models = {}\n",
        "        self.global_median = None\n",
        "\n",
        "    def _create_holidays(self, df):\n",
        "        \"\"\"Create holiday dataframe for Prophet\"\"\"\n",
        "        # Create base holidays\n",
        "        base_holidays = {\n",
        "            'thanksgiving': ['2010-11-25', '2011-11-24', '2012-11-22', '2013-11-28'],\n",
        "            'christmas': ['2010-12-25', '2011-12-25', '2012-12-25', '2013-12-25'],\n",
        "            'newyear': ['2011-01-01', '2012-01-01', '2013-01-01', '2014-01-01'],\n",
        "            'superbowl': ['2011-02-06', '2012-02-05', '2013-02-03', '2014-02-02'],\n",
        "            'laborday': ['2010-09-06', '2011-09-05', '2012-09-03', '2013-09-02']\n",
        "        }\n",
        "\n",
        "        holidays_list = []\n",
        "        for holiday_name, dates in base_holidays.items():\n",
        "            for date_str in dates:\n",
        "                holidays_list.append({\n",
        "                    'holiday': holiday_name,\n",
        "                    'ds': pd.to_datetime(date_str),\n",
        "                    'lower_window': 0,\n",
        "                    'upper_window': 0\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(holidays_list)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit Prophet models for each store-department combination\"\"\"\n",
        "        print(f\"Training Prophet models...\")\n",
        "\n",
        "        # Calculate global median for fallback\n",
        "        if 'Weekly_Sales' in X.columns:\n",
        "            self.global_median = X['Weekly_Sales'].median()\n",
        "        else:\n",
        "            self.global_median = 15000  # Reasonable default\n",
        "\n",
        "        # Create holidays\n",
        "        holidays = self._create_holidays(X)\n",
        "\n",
        "        # Key regressors for Prophet (only use columns that exist)\n",
        "        potential_regressors = [\n",
        "            'Temperature', 'Fuel_Price', 'IsHoliday', 'IsQ4',\n",
        "            'IsBackToSchool', 'TotalMarkDown', 'EconomicIndex', 'IsSummer'\n",
        "        ]\n",
        "\n",
        "        # Filter to only existing columns\n",
        "        regressors = [col for col in potential_regressors if col in X.columns]\n",
        "        print(f\"Using regressors: {regressors}\")\n",
        "\n",
        "        trained_count = 0\n",
        "        total_groups = len(X.groupby(['Store', 'Dept']))\n",
        "\n",
        "        for i, ((store, dept), group) in enumerate(X.groupby(['Store', 'Dept'])):\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Progress: {i}/{total_groups} ({100*i/total_groups:.1f}%)\")\n",
        "\n",
        "            # Skip if insufficient data\n",
        "            if len(group) < self.min_samples:\n",
        "                continue\n",
        "\n",
        "            # Prepare data for Prophet\n",
        "            prophet_data = pd.DataFrame({\n",
        "                'ds': group['Date'],\n",
        "                'y': group['Weekly_Sales'] if 'Weekly_Sales' in group.columns else group.iloc[:, -1]  # fallback\n",
        "            })\n",
        "\n",
        "            # Add regressors\n",
        "            for regressor in regressors:\n",
        "                if regressor in group.columns:\n",
        "                    prophet_data[regressor] = group[regressor].values\n",
        "                else:\n",
        "                    prophet_data[regressor] = 0  # Default value\n",
        "\n",
        "            # Remove any remaining NaN values\n",
        "            prophet_data = prophet_data.dropna()\n",
        "\n",
        "            if len(prophet_data) < self.min_samples:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with SuppressOutput():\n",
        "                    # Initialize Prophet model\n",
        "                    model = Prophet(\n",
        "                        changepoint_prior_scale=self.changepoint_prior_scale,\n",
        "                        seasonality_prior_scale=self.seasonality_prior_scale,\n",
        "                        holidays_prior_scale=self.holidays_prior_scale,\n",
        "                        seasonality_mode=self.seasonality_mode,\n",
        "                        holidays=holidays,\n",
        "                        daily_seasonality=False,\n",
        "                        weekly_seasonality=True,\n",
        "                        yearly_seasonality=True\n",
        "                    )\n",
        "\n",
        "                    # Add regressors\n",
        "                    for regressor in regressors:\n",
        "                        if regressor in prophet_data.columns:\n",
        "                            model.add_regressor(regressor)\n",
        "\n",
        "                    # Fit model\n",
        "                    model.fit(prophet_data)\n",
        "\n",
        "                    # Store model and info\n",
        "                    self.models[(store, dept)] = {\n",
        "                        'model': model,\n",
        "                        'regressors': regressors,\n",
        "                        'median_sales': prophet_data['y'].median()\n",
        "                    }\n",
        "                    trained_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                # Skip problematic series\n",
        "                if i < 5:  # Show first few errors for debugging\n",
        "                    print(f\"Error training model for Store {store}, Dept {dept}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Successfully trained {trained_count} models out of {total_groups} store-dept combinations\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Generate predictions using trained Prophet models\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for (store, dept), group in X.groupby(['Store', 'Dept']):\n",
        "            if (store, dept) in self.models:\n",
        "                model_info = self.models[(store, dept)]\n",
        "                model = model_info['model']\n",
        "\n",
        "                # Prepare future dataframe\n",
        "                future = pd.DataFrame({'ds': group['Date']})\n",
        "\n",
        "                # Add regressors\n",
        "                for regressor in model_info['regressors']:\n",
        "                    if regressor in group.columns:\n",
        "                        future[regressor] = group[regressor].values\n",
        "                    else:\n",
        "                        future[regressor] = 0\n",
        "\n",
        "                try:\n",
        "                    with SuppressOutput():\n",
        "                        forecast = model.predict(future)\n",
        "                    predictions.extend(forecast['yhat'].values)\n",
        "                except Exception as e:\n",
        "                    # Fallback to median\n",
        "                    median_pred = model_info['median_sales']\n",
        "                    predictions.extend([median_pred] * len(group))\n",
        "            else:\n",
        "                # Use global median for unseen store-dept combinations\n",
        "                # Apply seasonal adjustment\n",
        "                seasonal_multiplier = 1.0\n",
        "                if 'Month' in group.columns:\n",
        "                    month = group['Month'].iloc[0]\n",
        "                    if month in [11, 12]:  # Holiday season\n",
        "                        seasonal_multiplier = 1.5\n",
        "                    elif month in [1, 2]:  # Post holiday\n",
        "                        seasonal_multiplier = 0.8\n",
        "\n",
        "                pred_value = self.global_median * seasonal_multiplier\n",
        "                predictions.extend([pred_value] * len(group))\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "# =============================================================================\n",
        "# Block 4: Pipeline Training and Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "# Feature engineering\n",
        "print(\"Applying feature engineering...\")\n",
        "feature_engineer = TimeSeriesFeatureEngineer()\n",
        "feature_engineer.fit(train_df)\n",
        "\n",
        "processed_train = feature_engineer.transform(train_df)\n",
        "processed_test = feature_engineer.transform(test_df)\n",
        "\n",
        "print(f\"Features added. Train shape: {processed_train.shape}\")\n",
        "\n",
        "# Time-based train/validation split\n",
        "print(\"Creating time-based validation split...\")\n",
        "max_date = processed_train['Date'].max()\n",
        "val_split_date = max_date - timedelta(weeks=8)\n",
        "\n",
        "train_data = processed_train[processed_train['Date'] <= val_split_date].copy()\n",
        "val_data = processed_train[processed_train['Date'] > val_split_date].copy()\n",
        "\n",
        "print(f\"Train: {len(train_data)} samples, Val: {len(val_data)} samples\")\n",
        "\n",
        "# Train Prophet model\n",
        "print(\"Training Prophet model...\")\n",
        "prophet_model = WalmartProphetModel(\n",
        "    changepoint_prior_scale=0.05,\n",
        "    seasonality_prior_scale=10.0,\n",
        "    seasonality_mode='multiplicative'\n",
        ")\n",
        "\n",
        "prophet_model.fit(train_data)\n",
        "\n",
        "# Validate\n",
        "print(\"Validating model...\")\n",
        "val_predictions = prophet_model.predict(val_data)\n",
        "val_mae = mean_absolute_error(val_data['Weekly_Sales'], val_predictions)\n",
        "print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "\n",
        "# Log validation results\n",
        "wandb.log({\n",
        "    'validation_mae': val_mae,\n",
        "    'models_trained': len(prophet_model.models),\n",
        "    'val_samples': len(val_data)\n",
        "})\n",
        "\n",
        "# =============================================================================\n",
        "# Block 5: Final Training and Prediction\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Training final model on full dataset...\")\n",
        "final_model = WalmartProphetModel(\n",
        "    changepoint_prior_scale=0.05,\n",
        "    seasonality_prior_scale=10.0,\n",
        "    seasonality_mode='multiplicative'\n",
        ")\n",
        "\n",
        "final_model.fit(processed_train)\n",
        "\n",
        "# Generate test predictions\n",
        "print(\"Generating test predictions...\")\n",
        "test_predictions = final_model.predict(processed_test)\n",
        "\n",
        "# Basic sanity check\n",
        "print(f\"Test predictions stats:\")\n",
        "print(f\"  Mean: {np.mean(test_predictions):.2f}\")\n",
        "print(f\"  Std: {np.std(test_predictions):.2f}\")\n",
        "print(f\"  Min: {np.min(test_predictions):.2f}\")\n",
        "print(f\"  Max: {np.max(test_predictions):.2f}\")\n",
        "\n",
        "# Create submission\n",
        "submission = sample_submission.copy()\n",
        "submission['Weekly_Sales'] = test_predictions\n",
        "\n",
        "# Save results\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "submission_filename = f'prophet_submission_{timestamp}.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"Submission saved: {submission_filename}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Block 6: Pipeline Saving and Artifact Creation\n",
        "# =============================================================================\n",
        "\n",
        "class WalmartProphetPipeline(BaseEstimator):\n",
        "    \"\"\"Complete pipeline for Walmart sales forecasting\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_engineer = TimeSeriesFeatureEngineer()\n",
        "        self.model = WalmartProphetModel()\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"Fitting complete pipeline...\")\n",
        "        processed_data = self.feature_engineer.fit_transform(X)\n",
        "        self.model.fit(processed_data)\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Pipeline must be fitted before prediction\")\n",
        "        processed_data = self.feature_engineer.transform(X)\n",
        "        return self.model.predict(processed_data)\n",
        "\n",
        "# Create and save pipeline\n",
        "print(\"Creating complete pipeline...\")\n",
        "pipeline = WalmartProphetPipeline()\n",
        "pipeline.fit(train_df)\n",
        "\n",
        "# Save pipeline\n",
        "pipeline_filename = f'walmart_prophet_pipeline_{timestamp}.pkl'\n",
        "with open(pipeline_filename, 'wb') as f:\n",
        "    dill.dump(pipeline, f)\n",
        "\n",
        "print(f\"Pipeline saved: {pipeline_filename}\")\n",
        "\n",
        "# Create WandB artifacts\n",
        "pipeline_artifact = wandb.Artifact(\n",
        "    name=\"walmart_prophet_pipeline\",\n",
        "    type=\"model\",\n",
        "    description=\"Complete Prophet pipeline for Walmart sales forecasting\",\n",
        "    metadata={\n",
        "        \"model_type\": \"Prophet\",\n",
        "        \"validation_mae\": val_mae,\n",
        "        \"models_trained\": len(final_model.models),\n",
        "        \"timestamp\": timestamp\n",
        "    }\n",
        ")\n",
        "\n",
        "pipeline_artifact.add_file(pipeline_filename)\n",
        "wandb.log_artifact(pipeline_artifact)\n",
        "\n",
        "submission_artifact = wandb.Artifact(\n",
        "    name=\"walmart_prophet_submission\",\n",
        "    type=\"dataset\",\n",
        "    description=f\"Prophet submission for Walmart sales - {timestamp}\"\n",
        ")\n",
        "submission_artifact.add_file(submission_filename)\n",
        "wandb.log_artifact(submission_artifact)\n",
        "\n",
        "# Final logging\n",
        "wandb.log({\n",
        "    'pipeline_saved': True,\n",
        "    'submission_created': True,\n",
        "    'test_predictions_mean': np.mean(test_predictions),\n",
        "    'test_predictions_std': np.std(test_predictions),\n",
        "    'final_models_count': len(final_model.models)\n",
        "})\n",
        "\n",
        "print(\"Walmart Prophet forecasting completed successfully!\")\n",
        "print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "print(f\"Models trained: {len(final_model.models)}\")\n",
        "print(\"Pipeline and submission saved to WandB!\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yonCcvehBudv",
        "outputId": "1d8a6971-280e-42e1-b969-d61c987d60de"
      },
      "id": "yonCcvehBudv",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/119.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>n_departments</td><td>▁</td></tr><tr><td>n_stores</td><td>▁</td></tr><tr><td>test_samples</td><td>▁</td></tr><tr><td>train_samples</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>n_departments</td><td>81</td></tr><tr><td>n_stores</td><td>45</td></tr><tr><td>test_samples</td><td>115064</td></tr><tr><td>train_samples</td><td>421570</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Prophet_Experiment</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/0jh34yhh' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/0jh34yhh</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250730_203007-0jh34yhh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250730_203151-jd1uqimv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/jd1uqimv' target=\"_blank\">Prophet_TimeSeries_Optimized</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/jd1uqimv' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/jd1uqimv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Data loaded: Train (421570, 5), Test (115064, 4)\n",
            "Train columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']\n",
            "Features columns: ['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday']\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Applying feature engineering...\n",
            "Input shape: (421570, 5)\n",
            "Input columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']\n",
            "Merging with features...\n",
            "After features merge: (421570, 15)\n",
            "Merging with stores...\n",
            "After stores merge: (421570, 17)\n",
            "Columns after merge: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_feat', 'Type', 'Size']\n",
            "Filled Temperature: 0 missing values\n",
            "Filled Fuel_Price: 0 missing values\n",
            "Filled CPI: 0 missing values\n",
            "Filled Unemployment: 0 missing values\n",
            "Store types: Type\n",
            "A    215478\n",
            "B    163495\n",
            "C     42597\n",
            "Name: count, dtype: int64\n",
            "Final processed shape: (421570, 27)\n",
            "Missing values check:\n",
            "  IsHoliday: 0 missing\n",
            "  Temperature: 0 missing\n",
            "  Fuel_Price: 0 missing\n",
            "  TotalMarkDown: 0 missing\n",
            "Input shape: (115064, 4)\n",
            "Input columns: ['Store', 'Dept', 'Date', 'IsHoliday']\n",
            "Merging with features...\n",
            "After features merge: (115064, 14)\n",
            "Merging with stores...\n",
            "After stores merge: (115064, 16)\n",
            "Columns after merge: ['Store', 'Dept', 'Date', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_feat', 'Type', 'Size']\n",
            "Filled Temperature: 0 missing values\n",
            "Filled Fuel_Price: 0 missing values\n",
            "Filled CPI: 0 missing values\n",
            "Filled Unemployment: 0 missing values\n",
            "Store types: Type\n",
            "A    58713\n",
            "B    44500\n",
            "C    11851\n",
            "Name: count, dtype: int64\n",
            "Final processed shape: (115064, 26)\n",
            "Missing values check:\n",
            "  IsHoliday: 0 missing\n",
            "  Temperature: 0 missing\n",
            "  Fuel_Price: 0 missing\n",
            "  TotalMarkDown: 0 missing\n",
            "Features added. Train shape: (421570, 27)\n",
            "Creating time-based validation split...\n",
            "Train: 397841 samples, Val: 23729 samples\n",
            "Training Prophet model...\n",
            "Training Prophet models...\n",
            "Using regressors: ['Temperature', 'Fuel_Price', 'IsHoliday', 'IsQ4', 'IsBackToSchool', 'TotalMarkDown', 'EconomicIndex', 'IsSummer']\n",
            "Progress: 0/3326 (0.0%)\n",
            "Progress: 50/3326 (1.5%)\n",
            "Progress: 100/3326 (3.0%)\n",
            "Progress: 150/3326 (4.5%)\n",
            "Progress: 200/3326 (6.0%)\n",
            "Progress: 250/3326 (7.5%)\n",
            "Progress: 300/3326 (9.0%)\n",
            "Progress: 350/3326 (10.5%)\n",
            "Progress: 400/3326 (12.0%)\n",
            "Progress: 450/3326 (13.5%)\n",
            "Progress: 500/3326 (15.0%)\n",
            "Progress: 550/3326 (16.5%)\n",
            "Progress: 600/3326 (18.0%)\n",
            "Progress: 650/3326 (19.5%)\n",
            "Progress: 700/3326 (21.0%)\n",
            "Progress: 750/3326 (22.5%)\n",
            "Progress: 800/3326 (24.1%)\n",
            "Progress: 850/3326 (25.6%)\n",
            "Progress: 900/3326 (27.1%)\n",
            "Progress: 950/3326 (28.6%)\n",
            "Progress: 1000/3326 (30.1%)\n",
            "Progress: 1050/3326 (31.6%)\n",
            "Progress: 1100/3326 (33.1%)\n",
            "Progress: 1150/3326 (34.6%)\n",
            "Progress: 1200/3326 (36.1%)\n",
            "Progress: 1250/3326 (37.6%)\n",
            "Progress: 1300/3326 (39.1%)\n",
            "Progress: 1350/3326 (40.6%)\n",
            "Progress: 1400/3326 (42.1%)\n",
            "Progress: 1450/3326 (43.6%)\n",
            "Progress: 1500/3326 (45.1%)\n",
            "Progress: 1550/3326 (46.6%)\n",
            "Progress: 1600/3326 (48.1%)\n",
            "Progress: 1650/3326 (49.6%)\n",
            "Progress: 1700/3326 (51.1%)\n",
            "Progress: 1750/3326 (52.6%)\n",
            "Progress: 1800/3326 (54.1%)\n",
            "Progress: 1850/3326 (55.6%)\n",
            "Progress: 1900/3326 (57.1%)\n",
            "Progress: 1950/3326 (58.6%)\n",
            "Progress: 2000/3326 (60.1%)\n",
            "Progress: 2050/3326 (61.6%)\n",
            "Progress: 2100/3326 (63.1%)\n",
            "Progress: 2150/3326 (64.6%)\n",
            "Progress: 2200/3326 (66.1%)\n",
            "Progress: 2250/3326 (67.6%)\n",
            "Progress: 2300/3326 (69.2%)\n",
            "Progress: 2350/3326 (70.7%)\n",
            "Progress: 2400/3326 (72.2%)\n",
            "Progress: 2450/3326 (73.7%)\n",
            "Progress: 2500/3326 (75.2%)\n",
            "Progress: 2550/3326 (76.7%)\n",
            "Progress: 2600/3326 (78.2%)\n",
            "Progress: 2650/3326 (79.7%)\n",
            "Progress: 2700/3326 (81.2%)\n",
            "Progress: 2750/3326 (82.7%)\n",
            "Progress: 2800/3326 (84.2%)\n",
            "Progress: 2850/3326 (85.7%)\n",
            "Progress: 2900/3326 (87.2%)\n",
            "Progress: 2950/3326 (88.7%)\n",
            "Progress: 3000/3326 (90.2%)\n",
            "Progress: 3050/3326 (91.7%)\n",
            "Progress: 3100/3326 (93.2%)\n",
            "Progress: 3150/3326 (94.7%)\n",
            "Progress: 3200/3326 (96.2%)\n",
            "Progress: 3250/3326 (97.7%)\n",
            "Progress: 3300/3326 (99.2%)\n",
            "Successfully trained 3082 models out of 3326 store-dept combinations\n",
            "Validating model...\n",
            "Validation MAE: 1897.06\n",
            "Training final model on full dataset...\n",
            "Training Prophet models...\n",
            "Using regressors: ['Temperature', 'Fuel_Price', 'IsHoliday', 'IsQ4', 'IsBackToSchool', 'TotalMarkDown', 'EconomicIndex', 'IsSummer']\n",
            "Progress: 0/3331 (0.0%)\n",
            "Progress: 50/3331 (1.5%)\n",
            "Progress: 100/3331 (3.0%)\n",
            "Progress: 150/3331 (4.5%)\n",
            "Progress: 200/3331 (6.0%)\n",
            "Progress: 250/3331 (7.5%)\n",
            "Progress: 300/3331 (9.0%)\n",
            "Progress: 350/3331 (10.5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "mR9ELoN67Ef_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05860959-2bfd-4f75-96d7-3c7b6f47c501"
      },
      "id": "mR9ELoN67Ef_",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import pickle\n",
        "import joblib\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_Experiment\")\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "features_df = pd.read_csv(\"/content/features.csv\")\n",
        "stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"Stores shape: {stores_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "\n",
        "# Log dataset info to WandB\n",
        "wandb.log({\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"n_stores\": train_df['Store'].nunique(),\n",
        "    \"n_departments\": train_df['Dept'].nunique()\n",
        "})"
      ],
      "metadata": {
        "id": "XhuLwBp_LTqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "2c7553be-7fc3-42bf-c9f2-7579947d0e8b"
      },
      "id": "XhuLwBp_LTqw",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>n_departments</td><td>▁</td></tr><tr><td>n_stores</td><td>▁</td></tr><tr><td>test_samples</td><td>▁</td></tr><tr><td>train_samples</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>n_departments</td><td>81</td></tr><tr><td>n_stores</td><td>45</td></tr><tr><td>test_samples</td><td>115064</td></tr><tr><td>train_samples</td><td>421570</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Prophet_Experiment</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/60ddc0k0' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/60ddc0k0</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250730_202625-60ddc0k0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250730_203007-0jh34yhh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/0jh34yhh' target=\"_blank\">Prophet_Experiment</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/0jh34yhh' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/0jh34yhh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Data loaded successfully!\n",
            "Train shape: (421570, 5)\n",
            "Features shape: (8190, 12)\n",
            "Stores shape: (45, 3)\n",
            "Test shape: (115064, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================\n",
        "# Block 2: Enhanced Time Series Data Preprocessing\n",
        "# =================================================\n",
        "import logging\n",
        "\n",
        "logging.getLogger('prophet').setLevel(logging.ERROR)\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
        "logging.getLogger('prophet.forecaster').setLevel(logging.ERROR)\n",
        "logging.getLogger('prophet.plot').setLevel(logging.ERROR)\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "logging.disable(logging.DEBUG)\n",
        "logging.disable(logging.INFO)\n",
        "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class TimeSeriesFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Focused time series feature engineering for Prophet\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Merge external features\n",
        "        df = df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "        df = df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "        # Fill missing values efficiently\n",
        "        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Markdown columns (promotional effects)\n",
        "        markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        for col in markdown_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(0)\n",
        "\n",
        "        # Holiday handling\n",
        "        df['IsHoliday'] = df['IsHoliday'].fillna(False).astype(int)\n",
        "\n",
        "        # Store type encoding\n",
        "        if 'Type' in df.columns:\n",
        "            df['Type'] = df['Type'].fillna('A')\n",
        "            df['Size'] = df['Size'].fillna(df['Size'].median())\n",
        "\n",
        "        # Time-based features for Prophet regressors\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "        # Seasonal indicators (key for retail)\n",
        "        df['IsQ4'] = (df['Quarter'] == 4).astype(int)  # Holiday season\n",
        "        df['IsBackToSchool'] = (df['Month'] == 8).astype(int)  # August\n",
        "        df['IsSpringCleaning'] = (df['Month'] == 4).astype(int)  # April\n",
        "\n",
        "        # Create total markdown effect\n",
        "        df['TotalMarkDown'] = sum(df[col] for col in markdown_cols if col in df.columns)\n",
        "\n",
        "        # Economic indicators\n",
        "        df['EconomicIndex'] = df['CPI'] * df['Unemployment'] if 'CPI' in df.columns and 'Unemployment' in df.columns else 0\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "-n2AVR1TKDLC"
      },
      "id": "-n2AVR1TKDLC",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Block 3: Prophet Model for Time Series\n",
        "# =============================================================================\n",
        "\n",
        "class SuppressOutput:\n",
        "    \"\"\"Context manager to suppress Prophet output\"\"\"\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        self._original_stderr = sys.stderr\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "        sys.stderr = open(os.devnull, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stderr.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "        sys.stderr = self._original_stderr\n",
        "\n",
        "class WalmartProphetModel(BaseEstimator):\n",
        "    \"\"\"Optimized Prophet model for Walmart sales forecasting\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 changepoint_prior_scale=0.05,\n",
        "                 seasonality_prior_scale=10.0,\n",
        "                 holidays_prior_scale=10.0,\n",
        "                 seasonality_mode='multiplicative',\n",
        "                 min_samples=20):\n",
        "        self.changepoint_prior_scale = changepoint_prior_scale\n",
        "        self.seasonality_prior_scale = seasonality_prior_scale\n",
        "        self.holidays_prior_scale = holidays_prior_scale\n",
        "        self.seasonality_mode = seasonality_mode\n",
        "        self.min_samples = min_samples\n",
        "        self.models = {}\n",
        "        self.global_median = None\n",
        "\n",
        "    def _create_holidays(self, df):\n",
        "        \"\"\"Create holiday dataframe for Prophet\"\"\"\n",
        "        # Create base holidays\n",
        "        base_holidays = {\n",
        "            'thanksgiving': ['2010-11-25', '2011-11-24', '2012-11-22', '2013-11-28'],\n",
        "            'christmas': ['2010-12-25', '2011-12-25', '2012-12-25', '2013-12-25'],\n",
        "            'newyear': ['2011-01-01', '2012-01-01', '2013-01-01', '2014-01-01'],\n",
        "            'superbowl': ['2011-02-06', '2012-02-05', '2013-02-03', '2014-02-02'],\n",
        "            'laborday': ['2010-09-06', '2011-09-05', '2012-09-03', '2013-09-02']\n",
        "        }\n",
        "\n",
        "        holidays_list = []\n",
        "        for holiday_name, dates in base_holidays.items():\n",
        "            for date_str in dates:\n",
        "                holidays_list.append({\n",
        "                    'holiday': holiday_name,\n",
        "                    'ds': pd.to_datetime(date_str),\n",
        "                    'lower_window': 0,\n",
        "                    'upper_window': 0\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(holidays_list)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit Prophet models for each store-department combination\"\"\"\n",
        "        print(f\"Training Prophet models...\")\n",
        "\n",
        "        # Calculate global median for fallback\n",
        "        if 'Weekly_Sales' in X.columns:\n",
        "            self.global_median = X['Weekly_Sales'].median()\n",
        "        else:\n",
        "            self.global_median = 15000  # Reasonable default\n",
        "\n",
        "        # Create holidays\n",
        "        holidays = self._create_holidays(X)\n",
        "\n",
        "        # Key regressors for Prophet (only use columns that exist)\n",
        "        potential_regressors = [\n",
        "            'Temperature', 'Fuel_Price', 'IsHoliday', 'IsQ4',\n",
        "            'IsBackToSchool', 'TotalMarkDown', 'EconomicIndex', 'IsSummer'\n",
        "        ]\n",
        "\n",
        "        # Filter to only existing columns\n",
        "        regressors = [col for col in potential_regressors if col in X.columns]\n",
        "        print(f\"Using regressors: {regressors}\")\n",
        "\n",
        "        trained_count = 0\n",
        "        total_groups = len(X.groupby(['Store', 'Dept']))\n",
        "\n",
        "        for i, ((store, dept), group) in enumerate(X.groupby(['Store', 'Dept'])):\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Progress: {i}/{total_groups} ({100*i/total_groups:.1f}%)\")\n",
        "\n",
        "            # Skip if insufficient data\n",
        "            if len(group) < self.min_samples:\n",
        "                continue\n",
        "\n",
        "            # Prepare data for Prophet\n",
        "            prophet_data = pd.DataFrame({\n",
        "                'ds': group['Date'],\n",
        "                'y': group['Weekly_Sales'] if 'Weekly_Sales' in group.columns else group.iloc[:, -1]  # fallback\n",
        "            })\n",
        "\n",
        "            # Add regressors\n",
        "            for regressor in regressors:\n",
        "                if regressor in group.columns:\n",
        "                    prophet_data[regressor] = group[regressor].values\n",
        "                else:\n",
        "                    prophet_data[regressor] = 0  # Default value\n",
        "\n",
        "            # Remove any remaining NaN values\n",
        "            prophet_data = prophet_data.dropna()\n",
        "\n",
        "            if len(prophet_data) < self.min_samples:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with SuppressOutput():\n",
        "                    # Initialize Prophet model\n",
        "                    model = Prophet(\n",
        "                        changepoint_prior_scale=self.changepoint_prior_scale,\n",
        "                        seasonality_prior_scale=self.seasonality_prior_scale,\n",
        "                        holidays_prior_scale=self.holidays_prior_scale,\n",
        "                        seasonality_mode=self.seasonality_mode,\n",
        "                        holidays=holidays,\n",
        "                        daily_seasonality=False,\n",
        "                        weekly_seasonality=True,\n",
        "                        yearly_seasonality=True\n",
        "                    )\n",
        "\n",
        "                    # Add regressors\n",
        "                    for regressor in regressors:\n",
        "                        if regressor in prophet_data.columns:\n",
        "                            model.add_regressor(regressor)\n",
        "\n",
        "                    # Fit model\n",
        "                    model.fit(prophet_data)\n",
        "\n",
        "                    # Store model and info\n",
        "                    self.models[(store, dept)] = {\n",
        "                        'model': model,\n",
        "                        'regressors': regressors,\n",
        "                        'median_sales': prophet_data['y'].median()\n",
        "                    }\n",
        "                    trained_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                # Skip problematic series\n",
        "                if i < 5:  # Show first few errors for debugging\n",
        "                    print(f\"Error training model for Store {store}, Dept {dept}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Successfully trained {trained_count} models out of {total_groups} store-dept combinations\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Generate predictions using trained Prophet models\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for (store, dept), group in X.groupby(['Store', 'Dept']):\n",
        "            if (store, dept) in self.models:\n",
        "                model_info = self.models[(store, dept)]\n",
        "                model = model_info['model']\n",
        "\n",
        "                # Prepare future dataframe\n",
        "                future = pd.DataFrame({'ds': group['Date']})\n",
        "\n",
        "                # Add regressors\n",
        "                for regressor in model_info['regressors']:\n",
        "                    if regressor in group.columns:\n",
        "                        future[regressor] = group[regressor].values\n",
        "                    else:\n",
        "                        future[regressor] = 0\n",
        "\n",
        "                try:\n",
        "                    with SuppressOutput():\n",
        "                        forecast = model.predict(future)\n",
        "                    predictions.extend(forecast['yhat'].values)\n",
        "                except Exception as e:\n",
        "                    # Fallback to median\n",
        "                    median_pred = model_info['median_sales']\n",
        "                    predictions.extend([median_pred] * len(group))\n",
        "            else:\n",
        "                # Use global median for unseen store-dept combinations\n",
        "                # Apply seasonal adjustment\n",
        "                seasonal_multiplier = 1.0\n",
        "                if 'Month' in group.columns:\n",
        "                    month = group['Month'].iloc[0]\n",
        "                    if month in [11, 12]:  # Holiday season\n",
        "                        seasonal_multiplier = 1.5\n",
        "                    elif month in [1, 2]:  # Post holiday\n",
        "                        seasonal_multiplier = 0.8\n",
        "\n",
        "                pred_value = self.global_median * seasonal_multiplier\n",
        "                predictions.extend([pred_value] * len(group))\n",
        "\n",
        "        return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 4: Pipeline Training and Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "# Feature engineering\n",
        "print(\"Applying feature engineering...\")\n",
        "feature_engineer = TimeSeriesFeatureEngineer()\n",
        "feature_engineer.fit(train_df)\n",
        "\n",
        "processed_train = feature_engineer.transform(train_df)\n",
        "processed_test = feature_engineer.transform(test_df)\n",
        "\n",
        "print(f\"Features added. Train shape: {processed_train.shape}\")\n",
        "\n",
        "# Time-based train/validation split\n",
        "print(\"Creating time-based validation split...\")\n",
        "max_date = processed_train['Date'].max()\n",
        "val_split_date = max_date - timedelta(weeks=8)\n",
        "\n",
        "train_data = processed_train[processed_train['Date'] <= val_split_date].copy()\n",
        "val_data = processed_train[processed_train['Date'] > val_split_date].copy()\n",
        "\n",
        "print(f\"Train: {len(train_data)} samples, Val: {len(val_data)} samples\")\n",
        "\n",
        "# Train Prophet model\n",
        "print(\"Training Prophet model...\")\n",
        "prophet_model = WalmartProphetModel(\n",
        "    changepoint_prior_scale=0.05,\n",
        "    seasonality_prior_scale=10.0,\n",
        "    seasonality_mode='multiplicative'\n",
        ")\n",
        "\n",
        "prophet_model.fit(train_data)\n",
        "\n",
        "# Validate\n",
        "print(\"Validating model...\")\n",
        "val_predictions = prophet_model.predict(val_data)\n",
        "val_mae = mean_absolute_error(val_data['Weekly_Sales'], val_predictions)\n",
        "print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "\n",
        "# Log validation results\n",
        "wandb.log({\n",
        "    'validation_mae': val_mae,\n",
        "    'models_trained': len(prophet_model.models),\n",
        "    'val_samples': len(val_data)\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "THan61MPlY9z",
        "outputId": "24dd51d0-1a52-423d-f04e-aa216c6de132"
      },
      "id": "THan61MPlY9z",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying feature engineering...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'IsHoliday'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'IsHoliday'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1317907657.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeature_engineer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprocessed_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_engineer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprocessed_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_engineer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2674824517.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Holiday handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IsHoliday'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IsHoliday'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Store type encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'IsHoliday'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 5: Final Training and Prediction\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Training final model on full dataset...\")\n",
        "final_model = WalmartProphetModel(\n",
        "    changepoint_prior_scale=0.05,\n",
        "    seasonality_prior_scale=10.0,\n",
        "    seasonality_mode='multiplicative'\n",
        ")\n",
        "\n",
        "final_model.fit(processed_train)\n",
        "\n",
        "# Generate test predictions\n",
        "print(\"Generating test predictions...\")\n",
        "test_predictions = final_model.predict(processed_test)\n",
        "\n",
        "# Create submission\n",
        "submission = sample_submission.copy()\n",
        "submission['Weekly_Sales'] = test_predictions\n",
        "\n",
        "# Save results\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "submission_filename = f'prophet_submission_{timestamp}.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"Submission saved: {submission_filename}\")\n"
      ],
      "metadata": {
        "id": "ngfqnQDaNQRC",
        "collapsed": true
      },
      "id": "ngfqnQDaNQRC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 6: Pipeline Saving and Artifact Creation\n",
        "# =============================================================================\n",
        "\n",
        "class WalmartProphetPipeline(BaseEstimator):\n",
        "    \"\"\"Complete pipeline for Walmart sales forecasting\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_engineer = TimeSeriesFeatureEngineer()\n",
        "        self.model = WalmartProphetModel()\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"Fitting complete pipeline...\")\n",
        "        processed_data = self.feature_engineer.fit_transform(X)\n",
        "        self.model.fit(processed_data)\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Pipeline must be fitted before prediction\")\n",
        "        processed_data = self.feature_engineer.transform(X)\n",
        "        return self.model.predict(processed_data)\n",
        "\n",
        "# Create and save pipeline\n",
        "print(\"Creating complete pipeline...\")\n",
        "pipeline = WalmartProphetPipeline()\n",
        "pipeline.fit(train_df)\n",
        "\n",
        "# Save pipeline\n",
        "pipeline_filename = f'walmart_prophet_pipeline_{timestamp}.pkl'\n",
        "with open(pipeline_filename, 'wb') as f:\n",
        "    dill.dump(pipeline, f)\n",
        "\n",
        "print(f\"Pipeline saved: {pipeline_filename}\")\n",
        "\n",
        "# Create WandB artifacts\n",
        "pipeline_artifact = wandb.Artifact(\n",
        "    name=\"walmart_prophet_pipeline\",\n",
        "    type=\"model\",\n",
        "    description=\"Complete Prophet pipeline for Walmart sales forecasting\",\n",
        "    metadata={\n",
        "        \"model_type\": \"Prophet\",\n",
        "        \"validation_mae\": val_mae,\n",
        "        \"models_trained\": len(final_model.models),\n",
        "        \"timestamp\": timestamp\n",
        "    }\n",
        ")\n",
        "\n",
        "pipeline_artifact.add_file(pipeline_filename)\n",
        "wandb.log_artifact(pipeline_artifact)\n",
        "\n",
        "submission_artifact = wandb.Artifact(\n",
        "    name=\"walmart_prophet_submission\",\n",
        "    type=\"dataset\",\n",
        "    description=f\"Prophet submission for Walmart sales - {timestamp}\"\n",
        ")\n",
        "submission_artifact.add_file(submission_filename)\n",
        "wandb.log_artifact(submission_artifact)\n",
        "\n",
        "# Final logging\n",
        "wandb.log({\n",
        "    'pipeline_saved': True,\n",
        "    'submission_created': True,\n",
        "    'test_predictions_mean': np.mean(test_predictions),\n",
        "    'test_predictions_std': np.std(test_predictions),\n",
        "    'final_models_count': len(final_model.models)\n",
        "})\n",
        "\n",
        "print(\"Walmart Prophet forecasting completed successfully!\")\n",
        "print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "print(f\"Models trained: {len(final_model.models)}\")\n",
        "print(\"Pipeline and submission saved to WandB!\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "me3EXIWyWEul"
      },
      "id": "me3EXIWyWEul",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}