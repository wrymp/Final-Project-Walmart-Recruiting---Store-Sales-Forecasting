{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xc_xtTn6ECN",
        "outputId": "dc8f82e7-f9bf-43a2-a9af-30c1e23324ff"
      },
      "id": "0xc_xtTn6ECN",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "3Q92W4PQ6EaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c356ccf4-a1ec-4359-ee79-e75e3ec81a98"
      },
      "id": "3Q92W4PQ6EaA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "MPwqMv2z6H6S"
      },
      "id": "MPwqMv2z6H6S",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# ! unzip /content/train.csv.zip\n",
        "# ! unzip /content/test.csv.zip\n",
        "# ! unzip /content/features.csv.zip\n",
        "# ! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "id": "jQr6E5zG6KBU"
      },
      "id": "jQr6E5zG6KBU",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb -qU\n",
        "# !pip uninstall -y pmdarima numpy scipy statsmodels\n",
        "# !pip install numpy==1.24.4 scipy==1.10.1 statsmodels==0.13.5 pmdarima==2.0.3"
      ],
      "metadata": {
        "id": "myvAj7pC7CyH"
      },
      "id": "myvAj7pC7CyH",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "c5Vm5Z5I7DRW"
      },
      "id": "c5Vm5Z5I7DRW",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "mR9ELoN67Ef_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "34fd27ff-d2ce-4065-ecba-300362bc325b"
      },
      "id": "mR9ELoN67Ef_",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdshan21\u001b[0m (\u001b[33mdshan21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import pickle\n",
        "import joblib\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_Experiment\")\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "features_df = pd.read_csv(\"/content/features.csv\")\n",
        "stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"Stores shape: {stores_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "\n",
        "# Log dataset info to WandB\n",
        "wandb.log({\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"n_stores\": train_df['Store'].nunique(),\n",
        "    \"n_departments\": train_df['Dept'].nunique()\n",
        "})"
      ],
      "metadata": {
        "id": "XhuLwBp_LTqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "7a9fae10-a757-4ab0-d18a-f0f37bab7521"
      },
      "id": "XhuLwBp_LTqw",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250729_171628-84v12kkf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/84v12kkf' target=\"_blank\">Prophet_Experiment</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/84v12kkf' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/84v12kkf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Data loaded successfully!\n",
            "Train shape: (421570, 5)\n",
            "Features shape: (8190, 12)\n",
            "Stores shape: (45, 3)\n",
            "Test shape: (115064, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedTimeSeriesPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.store_encodings = {}\n",
        "        self.dept_encodings = {}\n",
        "        self.seasonal_components = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def _add_lag_features(self, df, group_cols=['Store', 'Dept']):\n",
        "        \"\"\"Add lag features for time series analysis\"\"\"\n",
        "        print(\"Adding lag features...\")\n",
        "\n",
        "        # Sort by date to ensure proper lag calculation\n",
        "        df = df.sort_values(group_cols + ['Date'])\n",
        "\n",
        "        # Add lag features for each store-dept combination\n",
        "        for lag in [1, 2, 3, 7, 14, 30]:\n",
        "            df[f'lag_{lag}'] = df.groupby(group_cols)['Weekly_Sales'].shift(lag)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_rolling_features(self, df, group_cols=['Store', 'Dept']):\n",
        "        \"\"\"Add rolling statistics features\"\"\"\n",
        "        print(\"Adding rolling features...\")\n",
        "\n",
        "        # Add rolling means and stds\n",
        "        for window in [7, 14, 30]:\n",
        "            df[f'rolling_mean_{window}'] = df.groupby(group_cols)['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "            )\n",
        "            df[f'rolling_std_{window}'] = df.groupby(group_cols)['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window=window, min_periods=1).std()\n",
        "            )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_fourier_features(self, df):\n",
        "        \"\"\"Add Fourier features for cyclical patterns\"\"\"\n",
        "        print(\"Adding Fourier features...\")\n",
        "\n",
        "        # Add day of year for annual patterns\n",
        "        df['dayofyear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "        # Add Fourier components for annual seasonality\n",
        "        for i in range(1, 6):\n",
        "            df[f'sin_annual_{i}'] = np.sin(2 * np.pi * i * df['dayofyear'] / 365.25)\n",
        "            df[f'cos_annual_{i}'] = np.cos(2 * np.pi * i * df['dayofyear'] / 365.25)\n",
        "\n",
        "        # Add Fourier components for weekly seasonality\n",
        "        df['dayofweek'] = df['Date'].dt.dayofweek\n",
        "        for i in range(1, 4):\n",
        "            df[f'sin_weekly_{i}'] = np.sin(2 * np.pi * i * df['dayofweek'] / 7)\n",
        "            df[f'cos_weekly_{i}'] = np.cos(2 * np.pi * i * df['dayofweek'] / 7)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_seasonal_decomposition(self, df, group_cols=['Store', 'Dept']):\n",
        "        \"\"\"Add seasonal decomposition components where possible\"\"\"\n",
        "        print(\"Adding seasonal decomposition features...\")\n",
        "\n",
        "        df['trend_component'] = np.nan\n",
        "        df['seasonal_component'] = np.nan\n",
        "        df['residual_component'] = np.nan\n",
        "\n",
        "        # Apply decomposition for groups with sufficient data\n",
        "        for name, group in df.groupby(group_cols):\n",
        "            if len(group) >= 52:  # At least one year of data\n",
        "                try:\n",
        "                    # Create weekly aggregation for decomposition\n",
        "                    weekly_data = group.set_index('Date')['Weekly_Sales'].resample('W').mean()\n",
        "                    if len(weekly_data) >= 52:\n",
        "                        decomposition = seasonal_decompose(weekly_data, model='additive', period=52)\n",
        "\n",
        "                        # Map back to original data\n",
        "                        for idx in group.index:\n",
        "                            date = group.loc[idx, 'Date']\n",
        "                            week_start = date - timedelta(days=date.weekday())\n",
        "                            closest_week = weekly_data.index[weekly_data.index.get_loc(week_start, method='nearest')]\n",
        "\n",
        "                            df.loc[idx, 'trend_component'] = decomposition.trend.loc[closest_week]\n",
        "                            df.loc[idx, 'seasonal_component'] = decomposition.seasonal.loc[closest_week]\n",
        "                            df.loc[idx, 'residual_component'] = decomposition.resid.loc[closest_week]\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # Fill missing values with median\n",
        "        df['trend_component'] = df['trend_component'].fillna(df['trend_component'].median())\n",
        "        df['seasonal_component'] = df['seasonal_component'].fillna(0)\n",
        "        df['residual_component'] = df['residual_component'].fillna(0)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def transform(self, X, is_train=True):\n",
        "        # Make a copy to avoid modifying original data\n",
        "        df = X.copy()\n",
        "\n",
        "        # Convert Date to datetime first\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Also convert Date in features_df and stores_df to datetime\n",
        "        features_df_copy = features_df.copy()\n",
        "        features_df_copy['Date'] = pd.to_datetime(features_df_copy['Date'])\n",
        "\n",
        "        # Merge with features data (matching on Store and Date)\n",
        "        df = df.merge(features_df_copy, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
        "\n",
        "        # Merge with stores data (only matching on Store, no Date column in stores)\n",
        "        df = df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "        # Handle missing values in numeric columns\n",
        "        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Fill markdown columns with 0 (these are promotional markdowns)\n",
        "        markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        for col in markdown_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(0)\n",
        "\n",
        "        # Handle IsHoliday column - take the one from main data, fill missing with features data\n",
        "        if 'IsHoliday_feat' in df.columns:\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(df['IsHoliday_feat'])\n",
        "            df = df.drop('IsHoliday_feat', axis=1)\n",
        "\n",
        "        # Enhanced time-based features\n",
        "        print(\"Creating enhanced time-based features...\")\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "        # Season indicators based on findings (fall to early spring sales increase)\n",
        "        df['IsFallWinterSeason'] = ((df['Month'] >= 9) | (df['Month'] <= 3)).astype(int)\n",
        "        df['IsHolidaySeason'] = ((df['Month'] == 11) | (df['Month'] == 12) | (df['Month'] == 1)).astype(int)\n",
        "\n",
        "        # Create holiday features\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "        # Create store type features\n",
        "        if 'Type' in df.columns:\n",
        "            df['Type_A'] = (df['Type'] == 'A').astype(int)\n",
        "            df['Type_B'] = (df['Type'] == 'B').astype(int)\n",
        "            df['Type_C'] = (df['Type'] == 'C').astype(int)\n",
        "\n",
        "        # Handle Size column\n",
        "        if 'Size' in df.columns:\n",
        "            df['Size'] = df['Size'].fillna(df['Size'].median())\n",
        "\n",
        "        # Add enhanced time series features only for training data\n",
        "        if is_train and 'Weekly_Sales' in df.columns:\n",
        "            # Add lag features\n",
        "            df = self._add_lag_features(df)\n",
        "\n",
        "            # Add rolling features\n",
        "            df = self._add_rolling_features(df)\n",
        "\n",
        "            # Add seasonal decomposition\n",
        "            df = self._add_seasonal_decomposition(df)\n",
        "\n",
        "        # Add Fourier features for both train and test\n",
        "        df = self._add_fourier_features(df)\n",
        "\n",
        "        return df\n",
        "\n",
        "# Initialize enhanced preprocessor\n",
        "enhanced_preprocessor = EnhancedTimeSeriesPreprocessor()\n",
        "\n",
        "# Preprocess data\n",
        "print(\"Preprocessing data with enhanced time series features...\")\n",
        "processed_train = enhanced_preprocessor.transform(train_df, is_train=True)\n",
        "processed_test = enhanced_preprocessor.transform(test_df, is_train=False)\n",
        "\n",
        "print(\"Enhanced data preprocessing completed!\")\n",
        "print(f\"Processed train shape: {processed_train.shape}\")\n",
        "print(f\"Processed test shape: {processed_test.shape}\")\n",
        "\n",
        "# Check for any remaining missing values\n",
        "print(f\"Missing values in train: {processed_train.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in test: {processed_test.isnull().sum().sum()}\")\n",
        "\n",
        "# Display new feature columns\n",
        "new_features = [col for col in processed_train.columns if any(x in col.lower() for x in ['lag_', 'rolling_', 'sin_', 'cos_', 'trend_', 'seasonal_', 'residual_'])]\n",
        "print(f\"New time series features added: {len(new_features)}\")\n",
        "print(\"Sample features:\", new_features[:10])\n",
        "\n",
        "# Log preprocessing info to WandB\n",
        "wandb.log({\n",
        "    \"processed_train_shape\": processed_train.shape,\n",
        "    \"processed_test_shape\": processed_test.shape,\n",
        "    \"missing_values_train\": processed_train.isnull().sum().sum(),\n",
        "    \"missing_values_test\": processed_test.isnull().sum().sum(),\n",
        "    \"train_date_range\": f\"{processed_train['Date'].min()} to {processed_train['Date'].max()}\",\n",
        "    \"test_date_range\": f\"{processed_test['Date'].min()} to {processed_test['Date'].max()}\",\n",
        "    \"new_features_count\": len(new_features)\n",
        "})"
      ],
      "metadata": {
        "id": "-n2AVR1TKDLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326a9dd6-0788-4642-9ebd-252be6ea9ccb"
      },
      "id": "-n2AVR1TKDLC",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing data with enhanced time series features...\n",
            "Creating enhanced time-based features...\n",
            "Adding lag features...\n",
            "Adding rolling features...\n",
            "Adding seasonal decomposition features...\n",
            "Adding Fourier features...\n",
            "Creating enhanced time-based features...\n",
            "Adding Fourier features...\n",
            "Enhanced data preprocessing completed!\n",
            "Processed train shape: (421570, 61)\n",
            "Processed test shape: (115064, 45)\n",
            "Missing values in train: 613195\n",
            "Missing values in test: 0\n",
            "New time series features added: 31\n",
            "Sample features: ['lag_1', 'lag_2', 'lag_3', 'lag_7', 'lag_14', 'lag_30', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b40229-1bf8-4cc2-968e-47587f746761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating time-based train/validation split...\n",
            "Time-based split created:\n",
            "Train split: 386007 samples (up to 2012-08-03)\n",
            "Validation split: 35563 samples (after 2012-08-03)\n",
            "Train date range: 2010-02-05 to 2012-08-03\n",
            "Val date range: 2012-08-10 to 2012-10-26\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating time-based train/validation split...\")\n",
        "\n",
        "# Sort by date to ensure proper time series split\n",
        "processed_train = processed_train.sort_values('Date')\n",
        "\n",
        "# Use time-based split instead of random split (maintaining temporal order)\n",
        "max_date = processed_train['Date'].max()\n",
        "split_date = max_date - timedelta(weeks=12)  # Last 12 weeks for validation\n",
        "\n",
        "train_split = processed_train[processed_train['Date'] <= split_date].copy()\n",
        "val_split = processed_train[processed_train['Date'] > split_date].copy()\n",
        "\n",
        "print(f\"Time-based split created:\")\n",
        "print(f\"Train split: {len(train_split)} samples (up to {split_date.date()})\")\n",
        "print(f\"Validation split: {len(val_split)} samples (after {split_date.date()})\")\n",
        "print(f\"Train date range: {train_split['Date'].min().date()} to {train_split['Date'].max().date()}\")\n",
        "print(f\"Val date range: {val_split['Date'].min().date()} to {val_split['Date'].max().date()}\")\n",
        "\n",
        "# Log split information\n",
        "wandb.log({\n",
        "    \"train_split_samples\": len(train_split),\n",
        "    \"val_split_samples\": len(val_split),\n",
        "    \"split_date\": split_date.strftime('%Y-%m-%d'),\n",
        "    \"time_series_split\": True\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating time-based train/validation split...\")\n",
        "\n",
        "# Sort by date to ensure proper time series split\n",
        "processed_train = processed_train.sort_values('Date')\n",
        "\n",
        "# Use time-based split instead of random split (maintaining temporal order)\n",
        "max_date = processed_train['Date'].max()\n",
        "split_date = max_date - timedelta(weeks=12)  # Last 12 weeks for validation\n",
        "\n",
        "train_split = processed_train[processed_train['Date'] <= split_date].copy()\n",
        "val_split = processed_train[processed_train['Date'] > split_date].copy()\n",
        "\n",
        "print(f\"Time-based split created:\")\n",
        "print(f\"Train split: {len(train_split)} samples (up to {split_date.date()})\")\n",
        "print(f\"Validation split: {len(val_split)} samples (after {split_date.date()})\")\n",
        "print(f\"Train date range: {train_split['Date'].min().date()} to {train_split['Date'].max().date()}\")\n",
        "print(f\"Val date range: {val_split['Date'].min().date()} to {val_split['Date'].max().date()}\")\n",
        "\n",
        "# Log split information\n",
        "wandb.log({\n",
        "    \"train_split_samples\": len(train_split),\n",
        "    \"val_split_samples\": len(val_split),\n",
        "    \"split_date\": split_date.strftime('%Y-%m-%d'),\n",
        "    \"time_series_split\": True\n",
        "})"
      ],
      "metadata": {
        "id": "54PZZRYuLhju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3167a110-dd69-45fc-f931-6c0d7958686e"
      },
      "id": "54PZZRYuLhju",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating time-based train/validation split...\n",
            "Time-based split created:\n",
            "Train split: 386007 samples (up to 2012-08-03)\n",
            "Validation split: 35563 samples (after 2012-08-03)\n",
            "Train date range: 2010-02-05 to 2012-08-03\n",
            "Val date range: 2012-08-10 to 2012-10-26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedWalmartProphetPipeline(BaseEstimator):\n",
        "    def __init__(self, prophet_params=None):\n",
        "        self.preprocessor = EnhancedTimeSeriesPreprocessor()\n",
        "        self.prophet_params = prophet_params or best_params\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"Fitting enhanced pipeline...\")\n",
        "        # Preprocess data\n",
        "        processed_data = self.preprocessor.transform(X, is_train=True)\n",
        "\n",
        "        # Train Enhanced Prophet model\n",
        "        self.model = EnhancedProphetModel(**self.prophet_params)\n",
        "        self.model.fit(processed_data)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Pipeline must be fitted before making predictions\")\n",
        "\n",
        "        # Preprocess data\n",
        "        processed_data = self.preprocessor.transform(X, is_train=False)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(processed_data)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Create and fit the enhanced pipeline\n",
        "print(\"Creating Enhanced Prophet pipeline...\")\n",
        "enhanced_prophet_pipeline = EnhancedWalmartProphetPipeline(prophet_params=best_params)\n",
        "enhanced_prophet_pipeline.fit(train_df)\n",
        "\n",
        "# Test pipeline predictions\n",
        "print(\"Testing enhanced pipeline predictions...\")\n",
        "test_predictions = enhanced_prophet_pipeline.predict(test_df)\n",
        "\n",
        "# Create submission\n",
        "submission_df = sample_submission.copy()\n",
        "submission_df['Weekly_Sales'] = test_predictions\n",
        "\n",
        "# Save submission\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "submission_filename = f'enhanced_prophet_submission_{timestamp}.csv'\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"Submission saved as: {submission_filename}\")\n",
        "\n",
        "# Save the enhanced pipeline with dill\n",
        "import dill\n",
        "pipeline_filename = f'enhanced_prophet_pipeline_{timestamp}.pkl'\n",
        "with open(pipeline_filename, 'wb') as f:\n",
        "    dill.dump(enhanced_prophet_pipeline, f)\n",
        "\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_save_model\")\n",
        "\n",
        "print(f\"Enhanced pipeline saved locally as: {pipeline_filename}\")\n",
        "\n",
        "# Create WandB Artifact BEFORE logging\n",
        "print(\"Creating WandB artifact for the enhanced pipeline...\")\n",
        "pipeline_artifact = wandb.Artifact(\n",
        "    name=\"prophet_pipeline\",\n",
        "    type=\"model\",\n",
        "    description=\"Enhanced Prophet model pipeline with time series features for Walmart sales forecasting\",\n",
        "    metadata={\n",
        "        \"model_type\": \"Enhanced_Prophet\",\n",
        "        \"train_mae\": train_mae,\n",
        "        \"train_rmse\": train_rmse,\n",
        "        \"n_models\": len(final_model.models),\n",
        "        \"best_params\": best_params,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"enhanced_features\": True,\n",
        "        \"time_series_split\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add the pipeline file to the artifact BEFORE logging\n",
        "pipeline_artifact.add_file(pipeline_filename)\n",
        "\n",
        "# NOW log the artifact (this finalizes it)\n",
        "wandb.log_artifact(pipeline_artifact)\n",
        "\n",
        "# Continue with submission artifact...\n",
        "submission_artifact = wandb.Artifact(\n",
        "    name=\"prophet_submission\",\n",
        "    type=\"dataset\",\n",
        "    description=f\"Enhanced Prophet model submission for Kaggle - {timestamp}\"\n",
        ")\n",
        "submission_artifact.add_file(submission_filename)\n",
        "wandb.log_artifact(submission_artifact)\n",
        "\n",
        "# Log final summary\n",
        "wandb.log({\n",
        "    'pipeline_created': True,\n",
        "    'pipeline_artifact_name': \"prophet_pipeline\",\n",
        "    'submission_artifact_name': \"prophet_submission\",\n",
        "    'test_predictions_mean': np.mean(test_predictions),\n",
        "    'test_predictions_std': np.std(test_predictions),\n",
        "    'model_registry_success': True\n",
        "})\n",
        "\n",
        "print(\"Enhanced Prophet experiment completed successfully!\")\n",
        "print(\"Enhanced pipeline and submission saved to WandB artifacts!\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "ngfqnQDaNQRC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "collapsed": true,
        "outputId": "b92bc5c3-4f1d-4cea-a1b7-471e601726ee"
      },
      "id": "ngfqnQDaNQRC",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Enhanced Prophet pipeline...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_params' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-3548248104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Create and fit the enhanced pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating Enhanced Prophet pipeline...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0menhanced_prophet_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnhancedWalmartProphetPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprophet_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0menhanced_prophet_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_params' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "pipeline_filename = f'enhanced_prophet_pipeline_{timestamp}.pkl'\n",
        "with open(pipeline_filename, 'wb') as f:\n",
        "    dill.dump(enhanced_prophet_pipeline, f)\n",
        "\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_save_model\")\n",
        "\n",
        "print(f\"Enhanced pipeline saved locally as: {pipeline_filename}\")\n",
        "\n",
        "# Create WandB Artifact BEFORE logging\n",
        "print(\"Creating WandB artifact for the enhanced pipeline...\")\n",
        "pipeline_artifact = wandb.Artifact(\n",
        "    name=\"prophet_pipeline\",\n",
        "    type=\"model\",\n",
        "    description=\"Enhanced Prophet model pipeline with time series features for Walmart sales forecasting\",\n",
        "    metadata={\n",
        "        \"model_type\": \"Enhanced_Prophet\",\n",
        "        \"train_mae\": train_mae,\n",
        "        \"train_rmse\": train_rmse,\n",
        "        \"n_models\": len(final_model.models),\n",
        "        \"best_params\": best_params,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"enhanced_features\": True,\n",
        "        \"time_series_split\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add the pipeline file to the artifact BEFORE logging\n",
        "pipeline_artifact.add_file(pipeline_filename)\n",
        "\n",
        "# NOW log the artifact (this finalizes it)\n",
        "wandb.log_artifact(pipeline_artifact)\n",
        "\n",
        "# Continue with submission artifact...\n",
        "submission_artifact = wandb.Artifact(\n",
        "    name=\"prophet_submission\",\n",
        "    type=\"dataset\",\n",
        "    description=f\"Enhanced Prophet model submission for Kaggle - {timestamp}\"\n",
        ")\n",
        "submission_artifact.add_file(submission_filename)\n",
        "wandb.log_artifact(submission_artifact)\n",
        "\n",
        "# Log final summary\n",
        "wandb.log({\n",
        "    'pipeline_created': True,\n",
        "    'pipeline_artifact_name': \"prophet_pipeline\",\n",
        "    'submission_artifact_name': \"prophet_submission\",\n",
        "    'test_predictions_mean': np.mean(test_predictions),\n",
        "    'test_predictions_std': np.std(test_predictions),\n",
        "    'model_registry_success': True\n",
        "})\n",
        "\n",
        "print(\"Enhanced Prophet experiment completed successfully!\")\n",
        "print(\"Enhanced pipeline and submission saved to WandB artifacts!\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "me3EXIWyWEul"
      },
      "id": "me3EXIWyWEul",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}