{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "0xc_xtTn6ECN",
        "outputId": "707cb002-7b97-4cb9-b110-4e94501585e0"
      },
      "id": "0xc_xtTn6ECN",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-39-1527773442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install kaggle wandb onnx -Uq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "3Q92W4PQ6EaA"
      },
      "id": "3Q92W4PQ6EaA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "cDQaBGLX6FfU"
      },
      "id": "cDQaBGLX6FfU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "HkcOB55i6G8d"
      },
      "id": "HkcOB55i6G8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "MPwqMv2z6H6S"
      },
      "id": "MPwqMv2z6H6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# ! unzip /content/train.csv.zip\n",
        "# ! unzip /content/test.csv.zip\n",
        "# ! unzip /content/features.csv.zip\n",
        "# ! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "id": "jQr6E5zG6KBU"
      },
      "id": "jQr6E5zG6KBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb -qU\n",
        "\n",
        "# # Clean up all related packages\n",
        "# !pip uninstall -y pmdarima numpy scipy statsmodels\n",
        "\n",
        "# # Reinstall pinned, compatible versions\n",
        "# !pip install numpy==1.24.4 scipy==1.10.1 statsmodels==0.13.5 pmdarima==2.0.3"
      ],
      "metadata": {
        "id": "myvAj7pC7CyH"
      },
      "id": "myvAj7pC7CyH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "c5Vm5Z5I7DRW"
      },
      "id": "c5Vm5Z5I7DRW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "mR9ELoN67Ef_"
      },
      "id": "mR9ELoN67Ef_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import pickle\n",
        "import joblib\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_Experiment\")\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "features_df = pd.read_csv(\"/content/features.csv\")\n",
        "stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"Stores shape: {stores_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "\n",
        "# Log dataset info to WandB\n",
        "wandb.log({\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"n_stores\": train_df['Store'].nunique(),\n",
        "    \"n_departments\": train_df['Dept'].nunique()\n",
        "})"
      ],
      "metadata": {
        "id": "XhuLwBp_LTqw"
      },
      "id": "XhuLwBp_LTqw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Ultra-Simple SARIMA Training (Guaranteed to Work)\n",
        "class WalmartDataPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.store_encodings = {}\n",
        "        self.dept_encodings = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, is_train=True):\n",
        "        # Make a copy to avoid modifying original data\n",
        "        df = X.copy()\n",
        "\n",
        "        # Convert Date to datetime first\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Also convert Date in features_df and stores_df to datetime\n",
        "        features_df_copy = features_df.copy()\n",
        "        features_df_copy['Date'] = pd.to_datetime(features_df_copy['Date'])\n",
        "\n",
        "        # Merge with features data (matching on Store and Date)\n",
        "        df = df.merge(features_df_copy, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
        "\n",
        "        # Merge with stores data (only matching on Store, no Date column in stores)\n",
        "        df = df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "        # Handle missing values in numeric columns\n",
        "        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Fill markdown columns with 0 (these are promotional markdowns)\n",
        "        markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        for col in markdown_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(0)\n",
        "\n",
        "        # Handle IsHoliday column - take the one from main data, fill missing with features data\n",
        "        if 'IsHoliday_feat' in df.columns:\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(df['IsHoliday_feat'])\n",
        "            df = df.drop('IsHoliday_feat', axis=1)\n",
        "\n",
        "        # Create time-based features\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "        # Create holiday features\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "        # Create store type features\n",
        "        if 'Type' in df.columns:\n",
        "            df['Type_A'] = (df['Type'] == 'A').astype(int)\n",
        "            df['Type_B'] = (df['Type'] == 'B').astype(int)\n",
        "            df['Type_C'] = (df['Type'] == 'C').astype(int)\n",
        "\n",
        "        # Handle Size column\n",
        "        if 'Size' in df.columns:\n",
        "            df['Size'] = df['Size'].fillna(df['Size'].median())\n",
        "\n",
        "        return df\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = WalmartDataPreprocessor()\n",
        "\n",
        "# Preprocess data\n",
        "print(\"Preprocessing data...\")\n",
        "try:\n",
        "    processed_train = preprocessor.transform(train_df, is_train=True)\n",
        "    processed_test = preprocessor.transform(test_df, is_train=False)\n",
        "\n",
        "    print(\"Data preprocessing completed!\")\n",
        "    print(f\"Processed train shape: {processed_train.shape}\")\n",
        "    print(f\"Processed test shape: {processed_test.shape}\")\n",
        "\n",
        "    # Check for any remaining missing values\n",
        "    print(f\"Missing values in train: {processed_train.isnull().sum().sum()}\")\n",
        "    print(f\"Missing values in test: {processed_test.isnull().sum().sum()}\")\n",
        "\n",
        "    # Display column info\n",
        "    print(f\"Train columns: {list(processed_train.columns)}\")\n",
        "    print(f\"Test columns: {list(processed_test.columns)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in preprocessing: {e}\")\n",
        "\n",
        "    # Debug: Check data types\n",
        "    print(\"\\nDebugging - Data types:\")\n",
        "    print(\"Train Date type:\", train_df['Date'].dtype)\n",
        "    print(\"Features Date type:\", features_df['Date'].dtype)\n",
        "    print(\"Stores columns:\", stores_df.columns.tolist())\n",
        "\n",
        "    # Show sample data\n",
        "    print(\"\\nSample data:\")\n",
        "    print(\"Train sample:\")\n",
        "    print(train_df.head())\n",
        "    print(\"\\nFeatures sample:\")\n",
        "    print(features_df.head())\n",
        "    print(\"\\nStores sample:\")\n",
        "    print(stores_df.head())\n",
        "\n",
        "# Log preprocessing info to WandB\n",
        "if 'processed_train' in locals() and 'processed_test' in locals():\n",
        "    wandb.log({\n",
        "        \"processed_train_shape\": processed_train.shape,\n",
        "        \"processed_test_shape\": processed_test.shape,\n",
        "        \"missing_values_train\": processed_train.isnull().sum().sum(),\n",
        "        \"missing_values_test\": processed_test.isnull().sum().sum(),\n",
        "        \"train_date_range\": f\"{processed_train['Date'].min()} to {processed_train['Date'].max()}\",\n",
        "        \"test_date_range\": f\"{processed_test['Date'].min()} to {processed_test['Date'].max()}\"\n",
        "    })\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = WalmartDataPreprocessor()\n",
        "\n",
        "# Preprocess data\n",
        "print(\"Preprocessing data...\")\n",
        "processed_train = preprocessor.transform(train_df, is_train=True)\n",
        "processed_test = preprocessor.transform(test_df, is_train=False)\n",
        "\n",
        "print(\"Data preprocessing completed!\")\n",
        "print(f\"Processed train shape: {processed_train.shape}\")\n",
        "print(f\"Processed test shape: {processed_test.shape}\")\n",
        "\n",
        "# Log preprocessing info\n",
        "wandb.log({\n",
        "    \"processed_train_shape\": processed_train.shape,\n",
        "    \"processed_test_shape\": processed_test.shape,\n",
        "    \"missing_values_train\": processed_train.isnull().sum().sum(),\n",
        "    \"missing_values_test\": processed_test.isnull().sum().sum()\n",
        "})"
      ],
      "metadata": {
        "id": "-n2AVR1TKDLC"
      },
      "id": "-n2AVR1TKDLC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "# Block 3: Robust Department-Level SARIMA (Will Actually Work)\n",
        "import logging\n",
        "import sys\n",
        "from contextlib import redirect_stdout, redirect_stderr\n",
        "import os\n",
        "\n",
        "# Suppress Prophet's verbose output\n",
        "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
        "\n",
        "# Also suppress other verbose loggers\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "# Suppress stdout from Prophet\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        self._original_stderr = sys.stderr\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "        sys.stderr = open(os.devnull, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stderr.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "        sys.stderr = self._original_stderr\n",
        "\n",
        "class ProphetModel(BaseEstimator):\n",
        "    def __init__(self,\n",
        "                 changepoint_prior_scale=0.05,\n",
        "                 seasonality_prior_scale=10.0,\n",
        "                 holidays_prior_scale=10.0,\n",
        "                 seasonality_mode='additive',\n",
        "                 yearly_seasonality=True,\n",
        "                 weekly_seasonality=True,\n",
        "                 daily_seasonality=False):\n",
        "\n",
        "        self.changepoint_prior_scale = changepoint_prior_scale\n",
        "        self.seasonality_prior_scale = seasonality_prior_scale\n",
        "        self.holidays_prior_scale = holidays_prior_scale\n",
        "        self.seasonality_mode = seasonality_mode\n",
        "        self.yearly_seasonality = yearly_seasonality\n",
        "        self.weekly_seasonality = weekly_seasonality\n",
        "        self.daily_seasonality = daily_seasonality\n",
        "        self.models = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Group by Store and Dept for individual models\n",
        "        grouped = X.groupby(['Store', 'Dept'])\n",
        "\n",
        "        print(f\"Training Prophet models for {len(grouped)} store-department combinations...\")\n",
        "\n",
        "        total_combinations = len(grouped)\n",
        "        trained_models = 0\n",
        "\n",
        "        for i, ((store, dept), group_data) in enumerate(grouped):\n",
        "            if len(group_data) < 10:  # Skip if too few data points\n",
        "                continue\n",
        "\n",
        "            # Show progress every 50 models\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Progress: {i}/{total_combinations} combinations processed...\")\n",
        "\n",
        "            # Prepare data for Prophet\n",
        "            prophet_data = pd.DataFrame({\n",
        "                'ds': group_data['Date'],\n",
        "                'y': group_data['Weekly_Sales']\n",
        "            })\n",
        "\n",
        "            # Initialize Prophet model with suppressed output\n",
        "            with SuppressOutput():\n",
        "                model = Prophet(\n",
        "                    changepoint_prior_scale=self.changepoint_prior_scale,\n",
        "                    seasonality_prior_scale=self.seasonality_prior_scale,\n",
        "                    holidays_prior_scale=self.holidays_prior_scale,\n",
        "                    seasonality_mode=self.seasonality_mode,\n",
        "                    yearly_seasonality=self.yearly_seasonality,\n",
        "                    weekly_seasonality=self.weekly_seasonality,\n",
        "                    daily_seasonality=self.daily_seasonality\n",
        "                )\n",
        "\n",
        "                # Add regressors\n",
        "                regressors = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday']\n",
        "                for col in regressors:\n",
        "                    if col in group_data.columns:\n",
        "                        model.add_regressor(col)\n",
        "\n",
        "                # Add regressor data\n",
        "                for col in regressors:\n",
        "                    if col in group_data.columns:\n",
        "                        prophet_data[col] = group_data[col].values\n",
        "\n",
        "                # Fit model\n",
        "                try:\n",
        "                    model.fit(prophet_data)\n",
        "                    self.models[(store, dept)] = model\n",
        "                    trained_models += 1\n",
        "                except Exception as e:\n",
        "                    if i % 100 == 0:  # Only print occasional errors\n",
        "                        print(f\"Error fitting model for Store {store}, Dept {dept}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        print(f\"Successfully trained {trained_models} Prophet models out of {total_combinations} combinations\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "\n",
        "        print(f\"Making predictions for {len(X.groupby(['Store', 'Dept']))} store-department combinations...\")\n",
        "\n",
        "        for i, ((store, dept), group_data) in enumerate(X.groupby(['Store', 'Dept'])):\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Prediction progress: {i} combinations processed...\")\n",
        "\n",
        "            if (store, dept) not in self.models:\n",
        "                # Use mean prediction if model doesn't exist\n",
        "                mean_sales = 15000  # Default fallback\n",
        "                predictions.extend([mean_sales] * len(group_data))\n",
        "                continue\n",
        "\n",
        "            model = self.models[(store, dept)]\n",
        "\n",
        "            # Prepare future dataframe\n",
        "            future_df = pd.DataFrame({\n",
        "                'ds': group_data['Date']\n",
        "            })\n",
        "\n",
        "            # Add regressors\n",
        "            regressors = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday']\n",
        "            for col in regressors:\n",
        "                if col in group_data.columns:\n",
        "                    future_df[col] = group_data[col].values\n",
        "\n",
        "            # Make predictions with suppressed output\n",
        "            try:\n",
        "                with SuppressOutput():\n",
        "                    forecast = model.predict(future_df)\n",
        "                predictions.extend(forecast['yhat'].values)\n",
        "            except Exception as e:\n",
        "                if i % 100 == 0:  # Only print occasional errors\n",
        "                    print(f\"Error predicting for Store {store}, Dept {dept}: {e}\")\n",
        "                mean_sales = 15000\n",
        "                predictions.extend([mean_sales] * len(group_data))\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Hyperparameter tuning with progress tracking\n",
        "hyperparams_grid = [\n",
        "    {\n",
        "        'changepoint_prior_scale': 0.05,\n",
        "        'seasonality_prior_scale': 10.0,\n",
        "        'seasonality_mode': 'additive'\n",
        "    },\n",
        "    {\n",
        "        'changepoint_prior_scale': 0.5,\n",
        "        'seasonality_prior_scale': 10.0,\n",
        "        'seasonality_mode': 'multiplicative'\n",
        "    },\n",
        "    {\n",
        "        'changepoint_prior_scale': 0.1,\n",
        "        'seasonality_prior_scale': 5.0,\n",
        "        'seasonality_mode': 'additive'\n",
        "    }\n",
        "]\n",
        "\n",
        "best_params = None\n",
        "best_mae = float('inf')\n",
        "\n",
        "print(\"Starting hyperparameter tuning...\")\n",
        "print(\"This may take a while, please be patient...\")\n",
        "\n",
        "for i, params in enumerate(hyperparams_grid):\n",
        "    print(f\"\\n=== Testing hyperparameter set {i+1}/{len(hyperparams_grid)} ===\")\n",
        "    print(f\"Parameters: {params}\")\n",
        "\n",
        "    # Create train/validation split (last 8 weeks for validation)\n",
        "    max_date = processed_train['Date'].max()\n",
        "    val_start_date = max_date - timedelta(weeks=8)\n",
        "\n",
        "    train_split = processed_train[processed_train['Date'] < val_start_date]\n",
        "    val_split = processed_train[processed_train['Date'] >= val_start_date]\n",
        "\n",
        "    print(f\"Train split: {len(train_split)} samples\")\n",
        "    print(f\"Validation split: {len(val_split)} samples\")\n",
        "\n",
        "    # Train model with current hyperparameters\n",
        "    model = ProphetModel(**params)\n",
        "    model.fit(train_split)\n",
        "\n",
        "    # Make predictions on validation set\n",
        "    val_predictions = model.predict(val_split)\n",
        "\n",
        "    # Calculate MAE\n",
        "    mae = mean_absolute_error(val_split['Weekly_Sales'], val_predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(val_split['Weekly_Sales'], val_predictions))\n",
        "\n",
        "    print(f\"Results - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
        "\n",
        "    # Log to WandB\n",
        "    wandb.log({\n",
        "        f'hp_set_{i+1}_mae': mae,\n",
        "        f'hp_set_{i+1}_rmse': rmse,\n",
        "        f'hp_set_{i+1}_changepoint_prior_scale': params['changepoint_prior_scale'],\n",
        "        f'hp_set_{i+1}_seasonality_prior_scale': params['seasonality_prior_scale'],\n",
        "        f'hp_set_{i+1}_seasonality_mode': params['seasonality_mode'],\n",
        "        f'hp_set_{i+1}_n_models': len(model.models)\n",
        "    })\n",
        "\n",
        "    if mae < best_mae:\n",
        "        best_mae = mae\n",
        "        best_params = params\n",
        "        print(f\"New best MAE found: {mae:.2f}\")\n",
        "\n",
        "print(f\"\\n=== HYPERPARAMETER TUNING COMPLETE ===\")\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Best MAE: {best_mae:.2f}\")\n",
        "\n",
        "# Log best parameters\n",
        "wandb.log({\n",
        "    'best_mae': best_mae,\n",
        "    'best_params': best_params,\n",
        "    'hyperparameter_tuning_complete': True\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Simple Predictions Fallback\n",
        "print(\"Training final Prophet model with best parameters...\")\n",
        "\n",
        "# Train final model on full dataset\n",
        "final_model = ProphetModel(**best_params)\n",
        "final_model.fit(processed_train)\n",
        "\n",
        "# Cross-validation evaluation (on a subset due to computational constraints)\n",
        "print(\"Performing cross-validation...\")\n",
        "\n",
        "# Sample some store-dept combinations for CV\n",
        "sample_combinations = list(final_model.models.keys())[:5]  # Use first 5 for speed\n",
        "\n",
        "cv_maes = []\n",
        "for store, dept in sample_combinations:\n",
        "    if (store, dept) in final_model.models:\n",
        "        # Get data for this combination\n",
        "        combo_data = processed_train[(processed_train['Store'] == store) &\n",
        "                                   (processed_train['Dept'] == dept)]\n",
        "\n",
        "        if len(combo_data) > 30:  # Only if enough data points\n",
        "            prophet_data = pd.DataFrame({\n",
        "                'ds': combo_data['Date'],\n",
        "                'y': combo_data['Weekly_Sales']\n",
        "            })\n",
        "\n",
        "            # Add regressors\n",
        "            regressors = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday']\n",
        "            for col in regressors:\n",
        "                if col in combo_data.columns:\n",
        "                    prophet_data[col] = combo_data[col].values\n",
        "\n",
        "            try:\n",
        "                model = final_model.models[(store, dept)]\n",
        "                df_cv = cross_validation(model, initial='365 days', period='30 days', horizon='30 days')\n",
        "                df_p = performance_metrics(df_cv)\n",
        "                cv_maes.append(df_p['mae'].mean())\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "if cv_maes:\n",
        "    avg_cv_mae = np.mean(cv_maes)\n",
        "    print(f\"Cross-validation MAE: {avg_cv_mae:.2f}\")\n",
        "    wandb.log({'cv_mae': avg_cv_mae})\n",
        "\n",
        "# Final evaluation metrics\n",
        "print(\"Calculating final metrics...\")\n",
        "train_predictions = final_model.predict(processed_train)\n",
        "train_mae = mean_absolute_error(processed_train['Weekly_Sales'], train_predictions)\n",
        "train_rmse = np.sqrt(mean_squared_error(processed_train['Weekly_Sales'], train_predictions))\n",
        "\n",
        "print(f\"Final Training MAE: {train_mae:.2f}\")\n",
        "print(f\"Final Training RMSE: {train_rmse:.2f}\")\n",
        "\n",
        "# Log final metrics\n",
        "wandb.log({\n",
        "    'final_train_mae': train_mae,\n",
        "    'final_train_rmse': train_rmse,\n",
        "    'n_models_trained': len(final_model.models)\n",
        "})\n",
        "\n",
        "# Create residual plots\n",
        "sample_idx = np.random.choice(len(train_predictions), min(10000, len(train_predictions)), replace=False)\n",
        "residuals = processed_train['Weekly_Sales'].iloc[sample_idx] - train_predictions[sample_idx]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(train_predictions[sample_idx], residuals, alpha=0.5)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Predicted')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.hist(residuals, bins=50, alpha=0.7)\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Residual Distribution')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.scatter(processed_train['Weekly_Sales'].iloc[sample_idx], train_predictions[sample_idx], alpha=0.5)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.plot([processed_train['Weekly_Sales'].min(), processed_train['Weekly_Sales'].max()],\n",
        "         [processed_train['Weekly_Sales'].min(), processed_train['Weekly_Sales'].max()], 'r--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('prophet_evaluation_plots.png', dpi=300, bbox_inches='tight')\n",
        "wandb.log({\"evaluation_plots\": wandb.Image('prophet_evaluation_plots.png')})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "54PZZRYuLhju"
      },
      "id": "54PZZRYuLhju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Save Final SARIMA Pipeline to Wandb\n",
        "class WalmartProphetPipeline(BaseEstimator):\n",
        "    def __init__(self, prophet_params=None):\n",
        "        self.preprocessor = WalmartDataPreprocessor()\n",
        "        self.prophet_params = prophet_params or best_params\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"Fitting pipeline...\")\n",
        "        # Preprocess data\n",
        "        processed_data = self.preprocessor.transform(X, is_train=True)\n",
        "\n",
        "        # Train Prophet model\n",
        "        self.model = ProphetModel(**self.prophet_params)\n",
        "        self.model.fit(processed_data)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Pipeline must be fitted before making predictions\")\n",
        "\n",
        "        # Preprocess data\n",
        "        processed_data = self.preprocessor.transform(X, is_train=False)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(processed_data)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Create and fit the pipeline\n",
        "print(\"Creating Prophet pipeline...\")\n",
        "prophet_pipeline = WalmartProphetPipeline(prophet_params=best_params)\n",
        "prophet_pipeline.fit(train_df)\n",
        "\n",
        "# Test pipeline predictions\n",
        "print(\"Testing pipeline predictions...\")\n",
        "test_predictions = prophet_pipeline.predict(test_df)\n",
        "\n",
        "# Create submission\n",
        "submission_df = sample_submission.copy()\n",
        "submission_df['Weekly_Sales'] = test_predictions\n",
        "\n",
        "# Save submission\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "submission_filename = f'prophet_submission_{timestamp}.csv'\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"Submission saved as: {submission_filename}\")\n",
        "\n",
        "# Save the pipeline locally first\n",
        "pipeline_filename = f'prophet_pipeline_{timestamp}.pkl'\n",
        "joblib.dump(prophet_pipeline, pipeline_filename)\n",
        "print(f\"Pipeline saved locally as: {pipeline_filename}\")\n",
        "\n",
        "# Create WandB Artifact for the model pipeline\n",
        "print(\"Creating WandB artifact for the pipeline...\")\n",
        "pipeline_artifact = wandb.Artifact(\n",
        "    name=\"prophet_pipeline\",\n",
        "    type=\"model\",\n",
        "    description=\"Prophet model pipeline for Walmart sales forecasting\",\n",
        "    metadata={\n",
        "        \"model_type\": \"Prophet\",\n",
        "        \"train_mae\": train_mae,\n",
        "        \"train_rmse\": train_rmse,\n",
        "        \"n_models\": len(final_model.models),\n",
        "        \"best_params\": best_params,\n",
        "        \"timestamp\": timestamp\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add the pipeline file to the artifact\n",
        "pipeline_artifact.add_file(pipeline_filename)\n",
        "\n",
        "# Log the artifact\n",
        "wandb.log_artifact(pipeline_artifact)\n",
        "\n",
        "# Also create artifact for submission\n",
        "submission_artifact = wandb.Artifact(\n",
        "    name=\"prophet_submission\",\n",
        "    type=\"dataset\",\n",
        "    description=f\"Prophet model submission for Kaggle - {timestamp}\"\n",
        ")\n",
        "submission_artifact.add_file(submission_filename)\n",
        "wandb.log_artifact(submission_artifact)\n",
        "\n",
        "# Log final summary\n",
        "wandb.log({\n",
        "    'pipeline_created': True,\n",
        "    'pipeline_artifact_name': \"prophet_pipeline\",\n",
        "    'submission_artifact_name': \"prophet_submission\",\n",
        "    'test_predictions_mean': np.mean(test_predictions),\n",
        "    'test_predictions_std': np.std(test_predictions),\n",
        "    'model_registry_success': True\n",
        "})\n",
        "\n",
        "print(\"Prophet experiment completed successfully!\")\n",
        "print(\"Pipeline and submission saved to WandB artifacts!\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "ngfqnQDaNQRC"
      },
      "id": "ngfqnQDaNQRC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this in a cell to clear all widget metadata\n",
        "import json\n",
        "\n",
        "# For Colab/Jupyter\n",
        "from IPython.display import Javascript\n",
        "Javascript(\"\"\"\n",
        "    Jupyter.notebook.clear_all_output();\n",
        "    Jupyter.notebook.metadata.widgets = {};\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "me3EXIWyWEul"
      },
      "id": "me3EXIWyWEul",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}