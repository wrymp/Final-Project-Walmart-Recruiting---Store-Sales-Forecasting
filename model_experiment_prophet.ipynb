{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xc_xtTn6ECN",
        "outputId": "29f0ba76-81bd-41f6-cbf8-9e76b2b55781"
      },
      "id": "0xc_xtTn6ECN",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "3Q92W4PQ6EaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37676fd1-c660-478a-f0d4-0a3a018edaed"
      },
      "id": "3Q92W4PQ6EaA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "MPwqMv2z6H6S"
      },
      "id": "MPwqMv2z6H6S",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# ! unzip /content/train.csv.zip\n",
        "# ! unzip /content/test.csv.zip\n",
        "# ! unzip /content/features.csv.zip\n",
        "# ! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "id": "jQr6E5zG6KBU"
      },
      "id": "jQr6E5zG6KBU",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb -qU\n",
        "# !pip uninstall -y pmdarima numpy scipy statsmodels\n",
        "# !pip install numpy==1.24.4 scipy==1.10.1 statsmodels==0.13.5 pmdarima==2.0.3"
      ],
      "metadata": {
        "id": "myvAj7pC7CyH"
      },
      "id": "myvAj7pC7CyH",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "c5Vm5Z5I7DRW"
      },
      "id": "c5Vm5Z5I7DRW",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "mR9ELoN67Ef_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763890b2-27f8-4013-b0cf-2552e3b11ebc"
      },
      "id": "mR9ELoN67Ef_",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdshan21\u001b[0m (\u001b[33mdshan21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import pickle\n",
        "import joblib\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_Experiment\")\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "features_df = pd.read_csv(\"/content/features.csv\")\n",
        "stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"Stores shape: {stores_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "\n",
        "# Log dataset info to WandB\n",
        "wandb.log({\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"n_stores\": train_df['Store'].nunique(),\n",
        "    \"n_departments\": train_df['Dept'].nunique()\n",
        "})"
      ],
      "metadata": {
        "id": "XhuLwBp_LTqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "a02c0beb-4632-4e3c-e20d-46352c212037"
      },
      "id": "XhuLwBp_LTqw",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250729_191747-weorvo5i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/weorvo5i' target=\"_blank\">Prophet_Experiment</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/weorvo5i' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/weorvo5i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Data loaded successfully!\n",
            "Train shape: (421570, 5)\n",
            "Features shape: (8190, 12)\n",
            "Stores shape: (45, 3)\n",
            "Test shape: (115064, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================\n",
        "# Block 2: Enhanced Time Series Data Preprocessing\n",
        "# =================================================\n",
        "import logging\n",
        "\n",
        "logging.getLogger('prophet').setLevel(logging.ERROR)\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
        "logging.getLogger('prophet.forecaster').setLevel(logging.ERROR)\n",
        "logging.getLogger('prophet.plot').setLevel(logging.ERROR)\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "logging.disable(logging.DEBUG)\n",
        "logging.disable(logging.INFO)\n",
        "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        self._original_stderr = sys.stderr\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "        sys.stderr = open(os.devnull, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stderr.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "        sys.stderr = self._original_stderr\n",
        "\n",
        "class EnhancedTimeSeriesPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.store_encodings = {}\n",
        "        self.dept_encodings = {}\n",
        "        self.seasonal_components = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def _add_lag_features(self, df, group_cols=['Store', 'Dept']):\n",
        "        \"\"\"Add lag features for time series analysis\"\"\"\n",
        "        print(\"Adding lag features...\")\n",
        "\n",
        "        # Sort by date to ensure proper lag calculation\n",
        "        df = df.sort_values(group_cols + ['Date'])\n",
        "\n",
        "        # Add lag features for each store-dept combination\n",
        "        for lag in [1, 2, 3, 7, 14, 30]:\n",
        "            df[f'lag_{lag}'] = df.groupby(group_cols)['Weekly_Sales'].shift(lag)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_rolling_features(self, df, group_cols=['Store', 'Dept']):\n",
        "        \"\"\"Add rolling statistics features\"\"\"\n",
        "        print(\"Adding rolling features...\")\n",
        "\n",
        "        # Add rolling means and stds\n",
        "        for window in [7, 14, 30]:\n",
        "            df[f'rolling_mean_{window}'] = df.groupby(group_cols)['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "            )\n",
        "            df[f'rolling_std_{window}'] = df.groupby(group_cols)['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window=window, min_periods=1).std()\n",
        "            )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_fourier_features(self, df):\n",
        "        \"\"\"Add Fourier features for cyclical patterns\"\"\"\n",
        "        print(\"Adding Fourier features...\")\n",
        "\n",
        "        # Add day of year for annual patterns\n",
        "        df['dayofyear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "        # Add Fourier components for annual seasonality\n",
        "        for i in range(1, 6):\n",
        "            df[f'sin_annual_{i}'] = np.sin(2 * np.pi * i * df['dayofyear'] / 365.25)\n",
        "            df[f'cos_annual_{i}'] = np.cos(2 * np.pi * i * df['dayofyear'] / 365.25)\n",
        "\n",
        "        # Add Fourier components for weekly seasonality\n",
        "        df['dayofweek'] = df['Date'].dt.dayofweek\n",
        "        for i in range(1, 4):\n",
        "            df[f'sin_weekly_{i}'] = np.sin(2 * np.pi * i * df['dayofweek'] / 7)\n",
        "            df[f'cos_weekly_{i}'] = np.cos(2 * np.pi * i * df['dayofweek'] / 7)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _add_seasonal_decomposition(self, df, group_cols=['Store', 'Dept']):\n",
        "        \"\"\"Add seasonal decomposition components where possible\"\"\"\n",
        "        print(\"Adding seasonal decomposition features...\")\n",
        "\n",
        "        df['trend_component'] = np.nan\n",
        "        df['seasonal_component'] = np.nan\n",
        "        df['residual_component'] = np.nan\n",
        "\n",
        "        # Apply decomposition for groups with sufficient data\n",
        "        for name, group in df.groupby(group_cols):\n",
        "            if len(group) >= 52:  # At least one year of data\n",
        "                try:\n",
        "                    # Create weekly aggregation for decomposition\n",
        "                    weekly_data = group.set_index('Date')['Weekly_Sales'].resample('W').mean()\n",
        "                    if len(weekly_data) >= 52:\n",
        "                        decomposition = seasonal_decompose(weekly_data, model='additive', period=52)\n",
        "\n",
        "                        # Map back to original data\n",
        "                        for idx in group.index:\n",
        "                            date = group.loc[idx, 'Date']\n",
        "                            week_start = date - timedelta(days=date.weekday())\n",
        "                            try:\n",
        "                                closest_week = weekly_data.index[weekly_data.index.get_loc(week_start, method='nearest')]\n",
        "\n",
        "                                if not pd.isna(decomposition.trend.loc[closest_week]):\n",
        "                                    df.loc[idx, 'trend_component'] = decomposition.trend.loc[closest_week]\n",
        "                                if not pd.isna(decomposition.seasonal.loc[closest_week]):\n",
        "                                    df.loc[idx, 'seasonal_component'] = decomposition.seasonal.loc[closest_week]\n",
        "                                if not pd.isna(decomposition.resid.loc[closest_week]):\n",
        "                                    df.loc[idx, 'residual_component'] = decomposition.resid.loc[closest_week]\n",
        "                            except:\n",
        "                                continue\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # Fill missing values with median/zero\n",
        "        df['trend_component'] = df['trend_component'].fillna(df['trend_component'].median() if not df['trend_component'].isnull().all() else 0)\n",
        "        df['seasonal_component'] = df['seasonal_component'].fillna(0)\n",
        "        df['residual_component'] = df['residual_component'].fillna(0)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def transform(self, X, is_train=True):\n",
        "        # Make a copy to avoid modifying original data\n",
        "        df = X.copy()\n",
        "\n",
        "        # Convert Date to datetime first\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Also convert Date in features_df and stores_df to datetime\n",
        "        features_df_copy = features_df.copy()\n",
        "        features_df_copy['Date'] = pd.to_datetime(features_df_copy['Date'])\n",
        "\n",
        "        # Merge with features data (matching on Store and Date)\n",
        "        df = df.merge(features_df_copy, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
        "\n",
        "        # Merge with stores data (only matching on Store, no Date column in stores)\n",
        "        df = df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "        # Handle missing values in numeric columns\n",
        "        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Fill markdown columns with 0 (these are promotional markdowns)\n",
        "        markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        for col in markdown_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(0)\n",
        "\n",
        "        # Handle IsHoliday column - take the one from main data, fill missing with features data\n",
        "        if 'IsHoliday_feat' in df.columns:\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(df['IsHoliday_feat'])\n",
        "            df = df.drop('IsHoliday_feat', axis=1)\n",
        "\n",
        "        # Enhanced time-based features\n",
        "        print(\"Creating enhanced time-based features...\")\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "        # Season indicators based on findings (fall to early spring sales increase)\n",
        "        df['IsFallWinterSeason'] = ((df['Month'] >= 9) | (df['Month'] <= 3)).astype(int)\n",
        "        df['IsHolidaySeason'] = ((df['Month'] == 11) | (df['Month'] == 12) | (df['Month'] == 1)).astype(int)\n",
        "\n",
        "        # Create holiday features\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "        # Create store type features\n",
        "        if 'Type' in df.columns:\n",
        "            df['Type_A'] = (df['Type'] == 'A').astype(int)\n",
        "            df['Type_B'] = (df['Type'] == 'B').astype(int)\n",
        "            df['Type_C'] = (df['Type'] == 'C').astype(int)\n",
        "\n",
        "        # Handle Size column\n",
        "        if 'Size' in df.columns:\n",
        "            df['Size'] = df['Size'].fillna(df['Size'].median())\n",
        "\n",
        "        # Add enhanced time series features only for training data\n",
        "        if is_train and 'Weekly_Sales' in df.columns:\n",
        "            # Add lag features\n",
        "            df = self._add_lag_features(df)\n",
        "\n",
        "            # Add rolling features\n",
        "            df = self._add_rolling_features(df)\n",
        "\n",
        "            # Add seasonal decomposition\n",
        "            df = self._add_seasonal_decomposition(df)\n",
        "\n",
        "        # Add Fourier features for both train and test\n",
        "        df = self._add_fourier_features(df)\n",
        "\n",
        "        return df\n",
        "\n",
        "# Initialize enhanced preprocessor\n",
        "enhanced_preprocessor = EnhancedTimeSeriesPreprocessor()\n",
        "\n",
        "# Preprocess data\n",
        "print(\"Preprocessing data with enhanced time series features...\")\n",
        "processed_train = enhanced_preprocessor.transform(train_df, is_train=True)\n",
        "processed_test = enhanced_preprocessor.transform(test_df, is_train=False)\n",
        "\n",
        "print(\"Enhanced data preprocessing completed!\")\n",
        "print(f\"Processed train shape: {processed_train.shape}\")\n",
        "print(f\"Processed test shape: {processed_test.shape}\")\n",
        "\n",
        "# Check for any remaining missing values\n",
        "print(f\"Missing values in train: {processed_train.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in test: {processed_test.isnull().sum().sum()}\")\n",
        "\n",
        "# Display new feature columns\n",
        "new_features = [col for col in processed_train.columns if any(x in col.lower() for x in ['lag_', 'rolling_', 'sin_', 'cos_', 'trend_', 'seasonal_', 'residual_'])]\n",
        "print(f\"New time series features added: {len(new_features)}\")\n",
        "print(\"Sample features:\", new_features[:10])\n",
        "\n",
        "# Log preprocessing info to WandB\n",
        "wandb.log({\n",
        "    \"processed_train_shape\": processed_train.shape,\n",
        "    \"processed_test_shape\": processed_test.shape,\n",
        "    \"missing_values_train\": processed_train.isnull().sum().sum(),\n",
        "    \"missing_values_test\": processed_test.isnull().sum().sum(),\n",
        "    \"train_date_range\": f\"{processed_train['Date'].min()} to {processed_train['Date'].max()}\",\n",
        "    \"test_date_range\": f\"{processed_test['Date'].min()} to {processed_test['Date'].max()}\",\n",
        "    \"new_features_count\": len(new_features)\n",
        "})"
      ],
      "metadata": {
        "id": "-n2AVR1TKDLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bf2ff3-7d4b-4837-b88d-821d28a73f4f"
      },
      "id": "-n2AVR1TKDLC",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing data with enhanced time series features...\n",
            "Creating enhanced time-based features...\n",
            "Adding lag features...\n",
            "Adding rolling features...\n",
            "Adding seasonal decomposition features...\n",
            "Adding Fourier features...\n",
            "Creating enhanced time-based features...\n",
            "Adding Fourier features...\n",
            "Enhanced data preprocessing completed!\n",
            "Processed train shape: (421570, 61)\n",
            "Processed test shape: (115064, 45)\n",
            "Missing values in train: 191625\n",
            "Missing values in test: 0\n",
            "New time series features added: 31\n",
            "Sample features: ['lag_1', 'lag_2', 'lag_3', 'lag_7', 'lag_14', 'lag_30', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98b588c-4aad-41e7-ec31-7386a8c9487f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating time-based train/validation split...\n",
            "Time-based split created:\n",
            "Train split: 386007 samples (up to 2012-08-03)\n",
            "Validation split: 35563 samples (after 2012-08-03)\n",
            "Train date range: 2010-02-05 to 2012-08-03\n",
            "Val date range: 2012-08-10 to 2012-10-26\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# Block 3: Time Series Train/Validation Split\n",
        "# =============================================\n",
        "\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "\n",
        "# Sort by date to ensure proper time series split\n",
        "processed_train = processed_train.sort_values('Date')\n",
        "\n",
        "# Use time-based split instead of random split (maintaining temporal order)\n",
        "max_date = processed_train['Date'].max()\n",
        "split_date = max_date - timedelta(weeks=12)  # Last 12 weeks for validation\n",
        "\n",
        "train_split = processed_train[processed_train['Date'] <= split_date].copy()\n",
        "val_split = processed_train[processed_train['Date'] > split_date].copy()\n",
        "\n",
        "print(f\"Time-based split created:\")\n",
        "print(f\"Train split: {len(train_split)} samples (up to {split_date.date()})\")\n",
        "print(f\"Validation split: {len(val_split)} samples (after {split_date.date()})\")\n",
        "print(f\"Train date range: {train_split['Date'].min().date()} to {train_split['Date'].max().date()}\")\n",
        "print(f\"Val date range: {val_split['Date'].min().date()} to {val_split['Date'].max().date()}\")\n",
        "\n",
        "# Log split information\n",
        "wandb.log({\n",
        "    \"train_split_samples\": len(train_split),\n",
        "    \"val_split_samples\": len(val_split),\n",
        "    \"split_date\": split_date.strftime('%Y-%m-%d'),\n",
        "    \"time_series_split\": True\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "# Block 4: Enhanced Prophet Training (Shortened)\n",
        "# ===================================================\n",
        "\n",
        "class EnhancedProphetModel(BaseEstimator):\n",
        "    def __init__(self, changepoint_prior_scale=0.05, seasonality_prior_scale=10.0, seasonality_mode='additive'):\n",
        "        self.changepoint_prior_scale = changepoint_prior_scale\n",
        "        self.seasonality_prior_scale = seasonality_prior_scale\n",
        "        self.seasonality_mode = seasonality_mode\n",
        "        self.models = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(f\"Training Prophet models for {len(X.groupby(['Store', 'Dept']))} store-dept combinations...\")\n",
        "        trained_models = 0\n",
        "\n",
        "        for i, ((store, dept), group_data) in enumerate(X.groupby(['Store', 'Dept'])):\n",
        "            if len(group_data) < 15: continue\n",
        "            if i % 100 == 0: print(f\"Progress: {i} models...\")\n",
        "\n",
        "            clean_data = group_data.dropna(subset=['Weekly_Sales'])\n",
        "            if len(clean_data) < 10: continue\n",
        "\n",
        "            prophet_data = pd.DataFrame({'ds': clean_data['Date'], 'y': clean_data['Weekly_Sales']})\n",
        "\n",
        "            with SuppressOutput():\n",
        "                model = Prophet(\n",
        "                    changepoint_prior_scale=self.changepoint_prior_scale,\n",
        "                    seasonality_prior_scale=self.seasonality_prior_scale,\n",
        "                    seasonality_mode=self.seasonality_mode\n",
        "                )\n",
        "\n",
        "                # Add key regressors\n",
        "                regressors = ['Temperature', 'Fuel_Price', 'IsHoliday', 'IsFallWinterSeason', 'IsHolidaySeason']\n",
        "                for col in regressors:\n",
        "                    if col in clean_data.columns and not clean_data[col].isnull().all():\n",
        "                        model.add_regressor(col)\n",
        "                        prophet_data[col] = clean_data[col].values\n",
        "\n",
        "                prophet_data = prophet_data.dropna()\n",
        "                if len(prophet_data) < 10: continue\n",
        "\n",
        "                try:\n",
        "                    model.fit(prophet_data)\n",
        "                    self.models[(store, dept)] = {'model': model, 'regressors': regressors}\n",
        "                    trained_models += 1\n",
        "                except: continue\n",
        "\n",
        "        print(f\"Trained {trained_models} models\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for (store, dept), group_data in X.groupby(['Store', 'Dept']):\n",
        "            if (store, dept) not in self.models:\n",
        "                month = group_data['Month'].iloc[0] if 'Month' in group_data.columns else 6\n",
        "                pred = 15000 * (1.3 if month in [11, 12, 1, 2] else 1.0)\n",
        "                predictions.extend([pred] * len(group_data))\n",
        "                continue\n",
        "\n",
        "            model_info = self.models[(store, dept)]\n",
        "            future_df = pd.DataFrame({'ds': group_data['Date']})\n",
        "\n",
        "            for col in model_info['regressors']:\n",
        "                if col in group_data.columns:\n",
        "                    future_df[col] = group_data[col].values\n",
        "                else:\n",
        "                    future_df[col] = 0\n",
        "\n",
        "            try:\n",
        "                with SuppressOutput():\n",
        "                    forecast = model.predict(future_df)\n",
        "                predictions.extend(forecast['yhat'].values)\n",
        "            except:\n",
        "                month = group_data['Month'].iloc[0] if 'Month' in group_data.columns else 6\n",
        "                pred = 15000 * (1.3 if month in [11, 12, 1, 2] else 1.0)\n",
        "                predictions.extend([pred] * len(group_data))\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Quick hyperparameter test\n",
        "default_params = {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'seasonality_mode': 'additive'}\n",
        "hyperparams_grid = [\n",
        "    {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'seasonality_mode': 'additive'},\n",
        "    {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 15.0, 'seasonality_mode': 'multiplicative'}\n",
        "]\n",
        "\n",
        "best_params, best_mae = default_params, float('inf')\n",
        "\n",
        "print(\"Quick hyperparameter tuning...\")\n",
        "for i, params in enumerate(hyperparams_grid):\n",
        "    print(f\"Testing set {i+1}: {params}\")\n",
        "    try:\n",
        "        model = EnhancedProphetModel(**params)\n",
        "        model.fit(train_split)\n",
        "        predictions = model.predict(val_split)\n",
        "        mae = mean_absolute_error(val_split['Weekly_Sales'], predictions)\n",
        "        print(f\"MAE: {mae:.2f}\")\n",
        "\n",
        "        wandb.log({f'hp_set_{i+1}_mae': mae, f'hp_set_{i+1}_n_models': len(model.models)})\n",
        "\n",
        "        if mae < best_mae:\n",
        "            best_mae, best_params = mae, params\n",
        "            print(f\"New best MAE: {mae:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "print(f\"Best params: {best_params}, Best MAE: {best_mae:.2f}\")\n",
        "\n",
        "# Train final model\n",
        "print(\"Training final model...\")\n",
        "final_model = EnhancedProphetModel(**best_params)\n",
        "final_model.fit(processed_train)\n",
        "\n",
        "# Quick evaluation\n",
        "train_predictions = final_model.predict(processed_train)\n",
        "train_mae = mean_absolute_error(processed_train['Weekly_Sales'], train_predictions)\n",
        "print(f\"Final Training MAE: {train_mae:.2f}\")\n",
        "\n",
        "wandb.log({'final_train_mae': train_mae, 'best_mae': best_mae, 'n_models_trained': len(final_model.models)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "THan61MPlY9z",
        "outputId": "cb25ed2f-eb02-41b8-b36e-cce2663b9355"
      },
      "id": "THan61MPlY9z",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BaseEstimator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2636587831.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ===================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEnhancedProphetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchangepoint_prior_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseasonality_prior_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseasonality_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'additive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchangepoint_prior_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchangepoint_prior_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Block 5: Save Enhanced Prophet Pipeline to WandB\n",
        "# ================================================================\n",
        "\n",
        "class EnhancedWalmartProphetPipeline(BaseEstimator):\n",
        "    def __init__(self, prophet_params=None):\n",
        "        self.preprocessor = EnhancedTimeSeriesPreprocessor()\n",
        "        self.prophet_params = prophet_params or default_params\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"Fitting enhanced pipeline...\")\n",
        "        # Preprocess data\n",
        "        processed_data = self.preprocessor.transform(X, is_train=True)\n",
        "\n",
        "        # Train Enhanced Prophet model\n",
        "        self.model = EnhancedProphetModel(**self.prophet_params)\n",
        "        self.model.fit(processed_data)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Pipeline must be fitted before making predictions\")\n",
        "\n",
        "        # Preprocess data\n",
        "        processed_data = self.preprocessor.transform(X, is_train=False)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(processed_data)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Create and fit the enhanced pipeline\n",
        "print(\"Creating Enhanced Prophet pipeline...\")\n",
        "enhanced_prophet_pipeline = EnhancedWalmartProphetPipeline(prophet_params=best_params)\n",
        "enhanced_prophet_pipeline.fit(train_df)\n",
        "\n",
        "# Test pipeline predictions\n",
        "print(\"Testing enhanced pipeline predictions...\")\n",
        "test_predictions = enhanced_prophet_pipeline.predict(test_df)\n",
        "\n",
        "# Create submission\n",
        "submission_df = sample_submission.copy()\n",
        "submission_df['Weekly_Sales'] = test_predictions\n",
        "\n",
        "# Save submission\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "submission_filename = f'enhanced_prophet_submission_{timestamp}.csv'\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"Submission saved as: {submission_filename}\")\n",
        "\n",
        "# Save the enhanced pipeline with dill\n",
        "import dill\n",
        "pipeline_filename = f'enhanced_prophet_pipeline_{timestamp}.pkl'\n",
        "with open(pipeline_filename, 'wb') as f:\n",
        "    dill.dump(enhanced_prophet_pipeline, f)\n",
        "\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_save_model\")\n",
        "\n",
        "print(f\"Enhanced pipeline saved locally as: {pipeline_filename}\")\n",
        "\n",
        "# Create WandB Artifact BEFORE logging\n",
        "print(\"Creating WandB artifact for the enhanced pipeline...\")\n",
        "pipeline_artifact = wandb.Artifact(\n",
        "    name=\"prophet_pipeline\",\n",
        "    type=\"model\",\n",
        "    description=\"Enhanced Prophet model pipeline with time series features for Walmart sales forecasting\",\n",
        "    metadata={\n",
        "        \"model_type\": \"Enhanced_Prophet\",\n",
        "        \"train_mae\": train_mae,\n",
        "        \"train_rmse\": train_rmse,\n",
        "        \"n_models\": len(final_model.models),\n",
        "        \"best_params\": best_params,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"enhanced_features\": True,\n",
        "        \"time_series_split\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add the pipeline file to the artifact BEFORE logging\n",
        "pipeline_artifact.add_file(pipeline_filename)\n",
        "\n",
        "# NOW log the artifact (this finalizes it)\n",
        "wandb.log_artifact(pipeline_artifact)\n",
        "\n",
        "# Continue with submission artifact...\n",
        "submission_artifact = wandb.Artifact(\n",
        "    name=\"prophet_submission\",\n",
        "    type=\"dataset\",\n",
        "    description=f\"Enhanced Prophet model submission for Kaggle - {timestamp}\"\n",
        ")\n",
        "submission_artifact.add_file(submission_filename)\n",
        "wandb.log_artifact(submission_artifact)\n",
        "\n",
        "# Log final summary\n",
        "wandb.log({\n",
        "    'pipeline_created': True,\n",
        "    'pipeline_artifact_name': \"prophet_pipeline\",\n",
        "    'submission_artifact_name': \"prophet_submission\",\n",
        "    'test_predictions_mean': np.mean(test_predictions),\n",
        "    'test_predictions_std': np.std(test_predictions),\n",
        "    'model_registry_success': True\n",
        "})\n",
        "\n",
        "print(\"Enhanced Prophet experiment completed successfully!\")\n",
        "print(\"Enhanced pipeline and submission saved to WandB artifacts!\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "ngfqnQDaNQRC",
        "collapsed": true
      },
      "id": "ngfqnQDaNQRC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "pipeline_filename = f'enhanced_prophet_pipeline_{timestamp}.pkl'\n",
        "with open(pipeline_filename, 'wb') as f:\n",
        "    dill.dump(enhanced_prophet_pipeline, f)\n",
        "\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"Prophet_save_model\")\n",
        "\n",
        "print(f\"Enhanced pipeline saved locally as: {pipeline_filename}\")\n",
        "\n",
        "# Create WandB Artifact BEFORE logging\n",
        "print(\"Creating WandB artifact for the enhanced pipeline...\")\n",
        "pipeline_artifact = wandb.Artifact(\n",
        "    name=\"prophet_pipeline\",\n",
        "    type=\"model\",\n",
        "    description=\"Enhanced Prophet model pipeline with time series features for Walmart sales forecasting\",\n",
        "    metadata={\n",
        "        \"model_type\": \"Enhanced_Prophet\",\n",
        "        \"train_mae\": train_mae,\n",
        "        \"train_rmse\": train_rmse,\n",
        "        \"n_models\": len(final_model.models),\n",
        "        \"best_params\": best_params,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"enhanced_features\": True,\n",
        "        \"time_series_split\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add the pipeline file to the artifact BEFORE logging\n",
        "pipeline_artifact.add_file(pipeline_filename)\n",
        "\n",
        "# NOW log the artifact (this finalizes it)\n",
        "wandb.log_artifact(pipeline_artifact)\n",
        "\n",
        "# Continue with submission artifact...\n",
        "submission_artifact = wandb.Artifact(\n",
        "    name=\"prophet_submission\",\n",
        "    type=\"dataset\",\n",
        "    description=f\"Enhanced Prophet model submission for Kaggle - {timestamp}\"\n",
        ")\n",
        "submission_artifact.add_file(submission_filename)\n",
        "wandb.log_artifact(submission_artifact)\n",
        "\n",
        "# Log final summary\n",
        "wandb.log({\n",
        "    'pipeline_created': True,\n",
        "    'pipeline_artifact_name': \"prophet_pipeline\",\n",
        "    'submission_artifact_name': \"prophet_submission\",\n",
        "    'test_predictions_mean': np.mean(test_predictions),\n",
        "    'test_predictions_std': np.std(test_predictions),\n",
        "    'model_registry_success': True\n",
        "})\n",
        "\n",
        "print(\"Enhanced Prophet experiment completed successfully!\")\n",
        "print(\"Enhanced pipeline and submission saved to WandB artifacts!\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "me3EXIWyWEul"
      },
      "id": "me3EXIWyWEul",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}