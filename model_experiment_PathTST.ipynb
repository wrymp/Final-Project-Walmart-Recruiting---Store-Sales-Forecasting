{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrymp/Final-Project-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_PathTST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# PatchTST Implementation for Walmart Sales Forecasting\n",
        "\n",
        "This notebook implements PatchTST (Patch Time Series Transformer) for Walmart sales forecasting following the exact pipeline structure from NBEATS experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "setup_cell"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# !pip install wandb -q\n",
        "# !pip install kaggle -q\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "data_download",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fed7f10-169a-4fa9-930e-550d53d42ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "100% 2.70M/2.70M [00:00<00:00, 875MB/s]\n",
            "Archive:  /content/walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n",
            "Archive:  /content/train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  /content/test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  /content/features.csv.zip\n",
            "  inflating: features.csv            \n",
            "unzip:  cannot find or open /content/stores.csv.zip, /content/stores.csv.zip.zip or /content/stores.csv.zip.ZIP.\n",
            "✓ Data extraction completed\n"
          ]
        }
      ],
      "source": [
        "# Download and extract Walmart dataset\n",
        "# ! kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "# ! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# ! unzip /content/train.csv.zip\n",
        "# ! unzip /content/test.csv.zip\n",
        "# ! unzip /content/features.csv.zip\n",
        "# ! unzip /content/stores.csv.zip\n",
        "# print(\"✓ Data extraction completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports_cell"
      },
      "source": [
        "# Essential Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports_code",
        "outputId": "28bca0a7-f5dd-49ce-c072-a39ea91ab679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA device: Tesla T4\n",
            "CUDA memory: 14.7GB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "wandb_init",
        "outputId": "9b3bd0cf-423d-4cf7-e364-2b87eead1651"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqitiashvili13\u001b[0m (\u001b[33mdshan21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_174404-9qysysuc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/9qysysuc' target=\"_blank\">PatchTST_Initial_Setup</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/9qysysuc' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/9qysysuc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Wandb initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize Wandb project\n",
        "wandb.login()\n",
        "try:\n",
        "    wandb.init(\n",
        "        project=\"walmart-sales-forecasting\",\n",
        "        name=\"PatchTST_Initial_Setup\",\n",
        "        config={\n",
        "            \"model_type\": \"PatchTST\",\n",
        "            \"framework\": \"PyTorch\",\n",
        "            \"device\": str(device),\n",
        "            \"random_seed\": 42\n",
        "        }\n",
        "    )\n",
        "    print(\"✓ Wandb initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Wandb initialization failed: {e}\")\n",
        "    print(\"Continuing without wandb logging...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading_section"
      },
      "source": [
        "# Data Loading and Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "data_loading_code",
        "outputId": "86fdc316-9f14-4442-b254-bc4c18acd4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Walmart datasets...\n",
            "Train data shape: (421570, 5)\n",
            "Test data shape: (115064, 4)\n",
            "Stores data shape: (45, 3)\n",
            "Features data shape: (8190, 12)\n",
            "\n",
            "=== DATA SAMPLES ===\n",
            "\n",
            "Train Data:\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday\n",
            "0      1     1 2010-02-05      24924.50      False\n",
            "1      1     1 2010-02-12      46039.49       True\n",
            "2      1     1 2010-02-19      41595.55      False\n",
            "3      1     1 2010-02-26      19403.54      False\n",
            "4      1     1 2010-03-05      21827.90      False\n",
            "\n",
            "Stores Data:\n",
            "   Store Type    Size\n",
            "0      1    A  151315\n",
            "1      2    A  202307\n",
            "2      3    B   37392\n",
            "3      4    A  205863\n",
            "4      5    B   34875\n",
            "\n",
            "Features Data:\n",
            "   Store       Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  \\\n",
            "0      1 2010-02-05        42.31       2.572        NaN        NaN        NaN   \n",
            "1      1 2010-02-12        38.51       2.548        NaN        NaN        NaN   \n",
            "2      1 2010-02-19        39.93       2.514        NaN        NaN        NaN   \n",
            "3      1 2010-02-26        46.63       2.561        NaN        NaN        NaN   \n",
            "4      1 2010-03-05        46.50       2.625        NaN        NaN        NaN   \n",
            "\n",
            "   MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
            "0        NaN        NaN  211.096358         8.106      False  \n",
            "1        NaN        NaN  211.242170         8.106       True  \n",
            "2        NaN        NaN  211.289143         8.106      False  \n",
            "3        NaN        NaN  211.319643         8.106      False  \n",
            "4        NaN        NaN  211.350143         8.106      False  \n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "print(\"Loading Walmart datasets...\")\n",
        "\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "stores_df = pd.read_csv('/content/stores.csv')\n",
        "features_df = pd.read_csv('/content/features.csv')\n",
        "\n",
        "print(f\"Train data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(f\"Stores data shape: {stores_df.shape}\")\n",
        "print(f\"Features data shape: {features_df.shape}\")\n",
        "\n",
        "# Convert date columns\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "print(\"\\n=== DATA SAMPLES ===\")\n",
        "print(\"\\nTrain Data:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nStores Data:\")\n",
        "print(stores_df.head())\n",
        "print(\"\\nFeatures Data:\")\n",
        "print(features_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exploration_section"
      },
      "source": [
        "# Data Exploration and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exploration_code",
        "outputId": "adabe13c-52c8-46f3-f2f6-f425cc6313c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA EXPLORATION ===\n",
            "\n",
            "Train Data Info:\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Unique stores: 45\n",
            "Unique departments: 81\n",
            "Total store-dept combinations: 3331\n",
            "\n",
            "Sales Statistics:\n",
            "Mean weekly sales: $15,981.26\n",
            "Median weekly sales: $7,612.03\n",
            "Min weekly sales: $-4,988.94\n",
            "Max weekly sales: $693,099.36\n",
            "\n",
            "Holiday Impact:\n",
            "                   mean   count\n",
            "IsHoliday                      \n",
            "False      15901.445069  391909\n",
            "True       17035.823187   29661\n",
            "\n",
            "Store Types:\n",
            "Type\n",
            "A    22\n",
            "B    17\n",
            "C     6\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Missing Values in Features:\n",
            "MarkDown2       64.334554\n",
            "MarkDown4       57.704518\n",
            "MarkDown3       55.885226\n",
            "MarkDown1       50.769231\n",
            "MarkDown5       50.549451\n",
            "CPI              7.142857\n",
            "Unemployment     7.142857\n",
            "dtype: float64\n",
            "\n",
            "✓ Exploration completed and logged to wandb\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== DATA EXPLORATION ===\")\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nTrain Data Info:\")\n",
        "print(f\"Date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "print(f\"Unique stores: {train_df['Store'].nunique()}\")\n",
        "print(f\"Unique departments: {train_df['Dept'].nunique()}\")\n",
        "print(f\"Total store-dept combinations: {train_df[['Store', 'Dept']].drop_duplicates().shape[0]}\")\n",
        "\n",
        "# Sales statistics\n",
        "print(\"\\nSales Statistics:\")\n",
        "print(f\"Mean weekly sales: ${train_df['Weekly_Sales'].mean():,.2f}\")\n",
        "print(f\"Median weekly sales: ${train_df['Weekly_Sales'].median():,.2f}\")\n",
        "print(f\"Min weekly sales: ${train_df['Weekly_Sales'].min():,.2f}\")\n",
        "print(f\"Max weekly sales: ${train_df['Weekly_Sales'].max():,.2f}\")\n",
        "\n",
        "# Holiday impact\n",
        "print(\"\\nHoliday Impact:\")\n",
        "holiday_stats = train_df.groupby('IsHoliday')['Weekly_Sales'].agg(['mean', 'count'])\n",
        "print(holiday_stats)\n",
        "\n",
        "# Store types\n",
        "print(\"\\nStore Types:\")\n",
        "print(stores_df['Type'].value_counts())\n",
        "\n",
        "# Missing values\n",
        "print(\"\\nMissing Values in Features:\")\n",
        "missing_pct = features_df.isnull().sum() / len(features_df) * 100\n",
        "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
        "print(missing_pct)\n",
        "\n",
        "# Log to wandb\n",
        "try:\n",
        "    wandb.log({\n",
        "        \"data_exploration/train_samples\": len(train_df),\n",
        "        \"data_exploration/test_samples\": len(test_df),\n",
        "        \"data_exploration/unique_stores\": train_df['Store'].nunique(),\n",
        "        \"data_exploration/unique_departments\": train_df['Dept'].nunique(),\n",
        "        \"data_exploration/mean_weekly_sales\": train_df['Weekly_Sales'].mean(),\n",
        "        \"data_exploration/median_weekly_sales\": train_df['Weekly_Sales'].median(),\n",
        "        \"data_exploration/holiday_vs_regular_ratio\": holiday_stats.loc[True, 'mean'] / holiday_stats.loc[False, 'mean']\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"\\n✓ Exploration completed and logged to wandb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transformers_section"
      },
      "source": [
        "# Custom Transformers for PatchTST Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "transformers_code",
        "outputId": "1469a08c-9090-46e7-c1d5-3b5f515007c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Custom transformers defined\n"
          ]
        }
      ],
      "source": [
        "class PatchTimeSeriesDataProcessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Processes raw Walmart data into patch-based time-series format for PatchTST\"\"\"\n",
        "\n",
        "    def __init__(self, lookback_window=52, forecast_horizon=1, patch_length=13, stride=13):\n",
        "        self.lookback_window = lookback_window  # 52 weeks = 1 year\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.patch_length = patch_length  # 13 weeks = quarterly patches\n",
        "        self.stride = stride  # Non-overlapping patches\n",
        "        self.store_dept_combinations = None\n",
        "        self.date_range = None\n",
        "        self.n_patches = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Learn the structure of the time series data\"\"\"\n",
        "        # Get all unique store-department combinations\n",
        "        self.store_dept_combinations = X[['Store', 'Dept']].drop_duplicates().sort_values(['Store', 'Dept'])\n",
        "\n",
        "        # Get date range\n",
        "        self.date_range = pd.date_range(\n",
        "            start=X['Date'].min(),\n",
        "            end=X['Date'].max(),\n",
        "            freq='W'\n",
        "        )\n",
        "\n",
        "        # Calculate number of patches\n",
        "        self.n_patches = (self.lookback_window - self.patch_length) // self.stride + 1\n",
        "\n",
        "        print(f\"PatchTimeSeriesDataProcessor fitted:\")\n",
        "        print(f\"- Store-Dept combinations: {len(self.store_dept_combinations)}\")\n",
        "        print(f\"- Date range: {len(self.date_range)} weeks\")\n",
        "        print(f\"- Lookback window: {self.lookback_window} weeks\")\n",
        "        print(f\"- Patch length: {self.patch_length} weeks\")\n",
        "        print(f\"- Number of patches per sequence: {self.n_patches}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Convert data to patch-based sequences\"\"\"\n",
        "        if self.store_dept_combinations is None:\n",
        "            raise ValueError(\"Must call fit() before transform()\")\n",
        "\n",
        "        sequences = []\n",
        "        targets = []\n",
        "        metadata = []\n",
        "\n",
        "        print(f\"Creating patch-based sequences for {len(self.store_dept_combinations)} store-dept combinations...\")\n",
        "\n",
        "        for idx, (_, row) in enumerate(self.store_dept_combinations.iterrows()):\n",
        "            store_id = row['Store']\n",
        "            dept_id = row['Dept']\n",
        "\n",
        "            # Get time series for this store-department\n",
        "            ts_data = X[(X['Store'] == store_id) & (X['Dept'] == dept_id)].copy()\n",
        "\n",
        "            if len(ts_data) < self.lookback_window + self.forecast_horizon:\n",
        "                continue\n",
        "\n",
        "            # Sort by date\n",
        "            ts_data = ts_data.sort_values('Date')\n",
        "\n",
        "            # Create sequences with sliding window\n",
        "            values = ts_data['Weekly_Sales'].values\n",
        "            features = ts_data.drop(['Store', 'Dept', 'Date', 'Weekly_Sales'], axis=1, errors='ignore')\n",
        "\n",
        "            # Additional features (if available)\n",
        "            if len(features.columns) > 0:\n",
        "                feature_values = features.values\n",
        "            else:\n",
        "                feature_values = np.zeros((len(values), 1))  # Dummy feature\n",
        "\n",
        "            # Create sequences\n",
        "            for i in range(len(values) - self.lookback_window - self.forecast_horizon + 1):\n",
        "                # Sales sequence\n",
        "                sales_seq = values[i:i + self.lookback_window]\n",
        "\n",
        "                # Features sequence\n",
        "                feat_seq = feature_values[i:i + self.lookback_window]\n",
        "\n",
        "                # Combine sales and features\n",
        "                combined_seq = np.column_stack([sales_seq.reshape(-1, 1), feat_seq])\n",
        "\n",
        "                # Convert to patches\n",
        "                patches = self._create_patches(combined_seq)\n",
        "\n",
        "                # Target\n",
        "                target = values[i + self.lookback_window:i + self.lookback_window + self.forecast_horizon]\n",
        "\n",
        "                sequences.append(patches)\n",
        "                targets.append(target[0] if self.forecast_horizon == 1 else target)\n",
        "                metadata.append({\n",
        "                    'store': store_id,\n",
        "                    'dept': dept_id,\n",
        "                    'start_date': ts_data.iloc[i]['Date'],\n",
        "                    'end_date': ts_data.iloc[i + self.lookback_window - 1]['Date']\n",
        "                })\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                print(f\"Processed {idx}/{len(self.store_dept_combinations)} combinations\")\n",
        "\n",
        "        print(f\"Created {len(sequences)} patch-based sequences\")\n",
        "\n",
        "        return {\n",
        "            'sequences': np.array(sequences) if sequences else np.array([]),\n",
        "            'targets': np.array(targets) if targets else np.array([]),\n",
        "            'metadata': metadata,\n",
        "            'n_patches': self.n_patches,\n",
        "            'patch_length': self.patch_length,\n",
        "            'n_features': sequences[0].shape[-1] if sequences else 0\n",
        "        }\n",
        "\n",
        "    def _create_patches(self, sequence):\n",
        "        \"\"\"Convert sequence to patches\"\"\"\n",
        "        patches = []\n",
        "\n",
        "        for i in range(0, len(sequence) - self.patch_length + 1, self.stride):\n",
        "            if i + self.patch_length <= len(sequence):\n",
        "                patch = sequence[i:i + self.patch_length]\n",
        "                patches.append(patch)\n",
        "\n",
        "        # Handle remaining data if sequence length is not divisible by patch_length\n",
        "        remaining = len(sequence) % self.patch_length\n",
        "        if remaining > 0 and len(patches) < self.n_patches:\n",
        "            # Pad the last patch\n",
        "            last_patch = sequence[-self.patch_length:]\n",
        "            patches.append(last_patch)\n",
        "\n",
        "        # Ensure we have exactly n_patches\n",
        "        while len(patches) < self.n_patches:\n",
        "            patches.append(np.zeros((self.patch_length, sequence.shape[1])))\n",
        "\n",
        "        return np.array(patches[:self.n_patches])\n",
        "\n",
        "\n",
        "class WalmartFeatureMerger(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Merges train/test data with stores and features data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stores_df = None\n",
        "        self.features_df = None\n",
        "\n",
        "    def fit(self, X, y=None, stores_df=None, features_df=None):\n",
        "        \"\"\"Store reference data for merging\"\"\"\n",
        "        if stores_df is not None:\n",
        "            self.stores_df = stores_df.copy()\n",
        "        if features_df is not None:\n",
        "            self.features_df = features_df.copy()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Merge with stores and features data\"\"\"\n",
        "        result = X.copy()\n",
        "\n",
        "        # Merge with stores data\n",
        "        if self.stores_df is not None:\n",
        "            result = result.merge(self.stores_df, on='Store', how='left')\n",
        "\n",
        "        # Merge with features data\n",
        "        if self.features_df is not None:\n",
        "            result = result.merge(self.features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class WalmartMissingValueHandler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Handles missing values in Walmart dataset\"\"\"\n",
        "\n",
        "    def __init__(self, strategy='forward_fill'):\n",
        "        self.strategy = strategy\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Learn fill values for missing data\"\"\"\n",
        "        # Store median values for numerical columns\n",
        "        numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        self.fill_values = X[numerical_cols].median().to_dict()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Fill missing values\"\"\"\n",
        "        result = X.copy()\n",
        "\n",
        "        # Forward fill for time series data\n",
        "        if self.strategy == 'forward_fill':\n",
        "            # Ensure 'Store', 'Dept', 'Date' are present before grouping\n",
        "            if not all(col in result.columns for col in ['Store', 'Dept', 'Date']):\n",
        "                 raise ValueError(\"Input DataFrame must contain 'Store', 'Dept', and 'Date' columns for forward fill.\")\n",
        "            result = result.sort_values(['Store', 'Dept', 'Date'])\n",
        "            # Identify columns to fill (excluding grouping keys)\n",
        "            cols_to_fill = [col for col in result.columns if col not in ['Store', 'Dept', 'Date']]\n",
        "            for col in cols_to_fill:\n",
        "                result[col] = result.groupby(['Store', 'Dept'])[col].fillna(method='ffill')\n",
        "\n",
        "\n",
        "        # Fill remaining missing values with median\n",
        "        for col, fill_val in self.fill_values.items():\n",
        "            if col in result.columns:\n",
        "                result[col] = result[col].fillna(fill_val)\n",
        "\n",
        "        return result\n",
        "\n",
        "print(\"✓ Custom transformers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patchtst_model"
      },
      "source": [
        "# PatchTST Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "patchtst_model_code",
        "outputId": "1a4b9511-7a71-4e25-bed0-436864b71a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ PatchTST model components defined\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for transformer\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Patch embedding layer for PatchTST\"\"\"\n",
        "\n",
        "    def __init__(self, patch_length, n_features, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_length = patch_length\n",
        "        self.n_features = n_features\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Linear projection for patches\n",
        "        self.patch_projection = nn.Linear(patch_length * n_features, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        print(f\"PatchEmbedding initialized:\")\n",
        "        print(f\"- patch_length: {patch_length}\")\n",
        "        print(f\"- n_features: {n_features}\")\n",
        "        print(f\"- d_model: {d_model}\")\n",
        "        print(f\"- input_dim: {patch_length * n_features}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, n_patches, patch_length, n_features)\n",
        "        batch_size, n_patches, patch_length, n_features = x.shape\n",
        "\n",
        "        # Flatten patches\n",
        "        x = x.reshape(batch_size, n_patches, -1)  # (batch_size, n_patches, patch_length * n_features)\n",
        "\n",
        "        # Project to d_model\n",
        "        x = self.patch_projection(x)  # (batch_size, n_patches, d_model)\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Single transformer encoder block\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + ff_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchTSTModel(nn.Module):\n",
        "    \"\"\"PatchTST model for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self, patch_length, n_patches, n_features, forecast_horizon=1,\n",
        "                 d_model=256, n_heads=8, n_layers=4, d_ff=512, dropout=0.1,\n",
        "                 channel_independent=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_length = patch_length\n",
        "        self.n_patches = n_patches\n",
        "        self.n_features = n_features\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.d_model = d_model\n",
        "        self.channel_independent = channel_independent\n",
        "\n",
        "        # Channel independence: separate model for each feature\n",
        "        if channel_independent:\n",
        "            self.patch_embeddings = nn.ModuleList([\n",
        "                PatchEmbedding(patch_length, 1, d_model, dropout)\n",
        "                for _ in range(n_features)\n",
        "            ])\n",
        "        else:\n",
        "            self.patch_embedding = PatchEmbedding(patch_length, n_features, d_model, dropout)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len=n_patches)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        if channel_independent:\n",
        "            # Only predict the first feature (sales)\n",
        "            self.output_projection = nn.Linear(d_model, forecast_horizon)\n",
        "        else:\n",
        "            self.output_projection = nn.Linear(d_model * n_features, forecast_horizon)\n",
        "\n",
        "        print(f\"PatchTSTModel initialized:\")\n",
        "        print(f\"- patch_length: {patch_length}\")\n",
        "        print(f\"- n_patches: {n_patches}\")\n",
        "        print(f\"- n_features: {n_features}\")\n",
        "        print(f\"- d_model: {d_model}\")\n",
        "        print(f\"- n_heads: {n_heads}\")\n",
        "        print(f\"- n_layers: {n_layers}\")\n",
        "        print(f\"- channel_independent: {channel_independent}\")\n",
        "        print(f\"- forecast_horizon: {forecast_horizon}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, n_patches, patch_length, n_features)\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        if self.channel_independent:\n",
        "            # Process each feature independently\n",
        "            channel_outputs = []\n",
        "\n",
        "            for i in range(self.n_features):\n",
        "                # Extract single feature\n",
        "                x_channel = x[:, :, :, i:i+1]  # (batch_size, n_patches, patch_length, 1)\n",
        "\n",
        "                # Patch embedding\n",
        "                embedded = self.patch_embeddings[i](x_channel)  # (batch_size, n_patches, d_model)\n",
        "\n",
        "                # Add positional encoding\n",
        "                embedded = embedded.transpose(0, 1)  # (n_patches, batch_size, d_model)\n",
        "                embedded = self.positional_encoding(embedded)\n",
        "                embedded = embedded.transpose(0, 1)  # (batch_size, n_patches, d_model)\n",
        "\n",
        "                # Transformer layers\n",
        "                for layer in self.transformer_layers:\n",
        "                    embedded = layer(embedded)\n",
        "\n",
        "                # Global average pooling over patches\n",
        "                pooled = embedded.mean(dim=1)  # (batch_size, d_model)\n",
        "\n",
        "                channel_outputs.append(pooled)\n",
        "\n",
        "            # For sales forecasting, we only use the first feature (sales)\n",
        "            output = self.output_projection(channel_outputs[0])  # (batch_size, forecast_horizon)\n",
        "\n",
        "        else:\n",
        "            # Process all features together\n",
        "            embedded = self.patch_embedding(x)  # (batch_size, n_patches, d_model)\n",
        "\n",
        "            # Add positional encoding\n",
        "            embedded = embedded.transpose(0, 1)  # (n_patches, batch_size, d_model)\n",
        "            embedded = self.positional_encoding(embedded)\n",
        "            embedded = embedded.transpose(0, 1)  # (batch_size, n_patches, d_model)\n",
        "\n",
        "            # Transformer layers\n",
        "            for layer in self.transformer_layers:\n",
        "                embedded = layer(embedded)\n",
        "\n",
        "            # Global average pooling\n",
        "            pooled = embedded.mean(dim=1)  # (batch_size, d_model)\n",
        "\n",
        "            # Output projection\n",
        "            output = self.output_projection(pooled)  # (batch_size, forecast_horizon)\n",
        "\n",
        "        return output.squeeze(-1) if self.forecast_horizon == 1 else output\n",
        "\n",
        "print(\"✓ PatchTST model components defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_class"
      },
      "source": [
        "# Dataset and Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dataset_code",
        "outputId": "592cd871-4797-4ec9-babf-4c0291d7959e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dataset classes defined\n"
          ]
        }
      ],
      "source": [
        "class PatchTSTDataset(Dataset):\n",
        "    \"\"\"Dataset class for PatchTST\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = torch.FloatTensor(sequences)\n",
        "        self.targets = torch.FloatTensor(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "\n",
        "def create_dataloaders(sequences, targets, train_ratio=0.8, batch_size=32, random_seed=42):\n",
        "    \"\"\"Create train and validation dataloaders\"\"\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Split data\n",
        "    n_samples = len(sequences)\n",
        "    n_train = int(n_samples * train_ratio)\n",
        "\n",
        "    # Random shuffle\n",
        "    indices = np.random.permutation(n_samples)\n",
        "    train_indices = indices[:n_train]\n",
        "    val_indices = indices[n_train:]\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = PatchTSTDataset(sequences[train_indices], targets[train_indices])\n",
        "    val_dataset = PatchTSTDataset(sequences[val_indices], targets[val_indices])\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"Created dataloaders:\")\n",
        "    print(f\"- Training samples: {len(train_dataset)}\")\n",
        "    print(f\"- Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"- Batch size: {batch_size}\")\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "print(\"✓ Dataset classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipeline_section"
      },
      "source": [
        "# Complete PatchTST Pipeline Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pipeline_code",
        "outputId": "b9a26274-1466-4c53-9742-74bc3aa72fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ PatchTST experiment function defined\n"
          ]
        }
      ],
      "source": [
        "def run_patchtst_experiment():\n",
        "    \"\"\"Run complete PatchTST experiment\"\"\"\n",
        "\n",
        "    print(\"\\n=== STARTING PATCHTST EXPERIMENT ===\")\n",
        "\n",
        "    # Initialize new wandb run for PatchTST training\n",
        "    try:\n",
        "        wandb.finish()  # Close previous run\n",
        "        wandb.init(\n",
        "            project=\"walmart-sales-forecasting\",\n",
        "            name=\"PatchTST_Training\",\n",
        "            config={\n",
        "                \"model_type\": \"PatchTST\",\n",
        "                \"patch_length\": 13,\n",
        "                \"stride\": 13,\n",
        "                \"lookback_window\": 52,\n",
        "                \"forecast_horizon\": 1,\n",
        "                \"d_model\": 256,\n",
        "                \"n_heads\": 8,\n",
        "                \"n_layers\": 4,\n",
        "                \"channel_independent\": True,\n",
        "                \"batch_size\": 64,\n",
        "                \"learning_rate\": 0.001,\n",
        "                \"epochs\": 10\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Data preprocessing pipeline\n",
        "    print(\"\\n=== DATA PREPROCESSING ===\")\n",
        "\n",
        "    # Initialize transformers\n",
        "    feature_merger = WalmartFeatureMerger()\n",
        "    missing_handler = WalmartMissingValueHandler()\n",
        "    ts_processor = PatchTimeSeriesDataProcessor(\n",
        "        lookback_window=52,\n",
        "        forecast_horizon=1,\n",
        "        patch_length=13,\n",
        "        stride=13\n",
        "    )\n",
        "\n",
        "    # Fit transformers\n",
        "    feature_merger.fit(train_df, stores_df=stores_df, features_df=features_df)\n",
        "\n",
        "    # Transform data\n",
        "    merged_data = feature_merger.transform(train_df)\n",
        "    print(f\"After merging: {merged_data.shape}\")\n",
        "\n",
        "    missing_handler.fit(merged_data)\n",
        "    cleaned_data = missing_handler.transform(merged_data)\n",
        "    print(f\"After cleaning: {cleaned_data.shape}\")\n",
        "\n",
        "    # Handle categorical features (one-hot encode 'Type')\n",
        "    if 'Type' in cleaned_data.columns:\n",
        "        cleaned_data = pd.get_dummies(cleaned_data, columns=['Type'], prefix='StoreType', dtype=float)\n",
        "        print(f\"After one-hot encoding 'Type': {cleaned_data.shape}\")\n",
        "\n",
        "    # Drop any remaining non-numerical columns except 'Date', 'Store', 'Dept'\n",
        "    non_numerical_cols = cleaned_data.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    cols_to_drop = [col for col in non_numerical_cols if col not in ['Date', 'Store', 'Dept']]\n",
        "    if cols_to_drop:\n",
        "        cleaned_data.drop(columns=cols_to_drop, inplace=True)\n",
        "        print(f\"Dropped non-numerical columns: {cols_to_drop}\")\n",
        "        print(f\"After dropping non-numerical: {cleaned_data.shape}\")\n",
        "\n",
        "    # Ensure all columns are numerical before processing, keeping Date, Store, Dept for time series processing\n",
        "    # Start with the required ID columns\n",
        "    cleaned_data_prepared_cols = ['Store', 'Dept', 'Date']\n",
        "\n",
        "    # Add all numerical columns from cleaned_data, excluding the ID columns already added\n",
        "    numerical_cols = cleaned_data.select_dtypes(include=np.number).columns.tolist()\n",
        "    for col in numerical_cols:\n",
        "        if col not in cleaned_data_prepared_cols:\n",
        "            cleaned_data_prepared_cols.append(col)\n",
        "\n",
        "    # Ensure all columns in cleaned_data_prepared_cols actually exist in cleaned_data\n",
        "    cleaned_data_prepared_cols = [col for col in cleaned_data_prepared_cols if col in cleaned_data.columns]\n",
        "\n",
        "    # Create the prepared DataFrame with the correct columns and order\n",
        "    cleaned_data_prepared = cleaned_data[cleaned_data_prepared_cols].copy()\n",
        "\n",
        "\n",
        "    # Drop any rows that might have introduced NaNs during feature engineering if necessary\n",
        "    cleaned_data_prepared.dropna(inplace=True)\n",
        "    print(f\"After dropping rows with NaNs: {cleaned_data_prepared.shape}\")\n",
        "\n",
        "    # Check for duplicate column names before passing to ts_processor\n",
        "    if cleaned_data_prepared.columns.duplicated().any():\n",
        "        print(\"❌ Duplicate column names found in cleaned_data_prepared:\")\n",
        "        print(cleaned_data_prepared.columns[cleaned_data_prepared.columns.duplicated()])\n",
        "        return None\n",
        "\n",
        "\n",
        "    # Create time series sequences\n",
        "    ts_processor.fit(cleaned_data_prepared)\n",
        "    processed_data = ts_processor.transform(cleaned_data_prepared)\n",
        "\n",
        "    sequences = processed_data['sequences']\n",
        "    targets = processed_data['targets']\n",
        "    n_patches = processed_data['n_patches']\n",
        "    patch_length = processed_data['patch_length']\n",
        "    n_features = processed_data['n_features']\n",
        "\n",
        "    print(f\"\\nSequences shape: {sequences.shape}\")\n",
        "    print(f\"Targets shape: {targets.shape}\")\n",
        "    print(f\"Number of patches: {n_patches}\")\n",
        "    print(f\"Patch length: {patch_length}\")\n",
        "    print(f\"Number of features: {n_features}\")\n",
        "\n",
        "\n",
        "    if sequences.size == 0:\n",
        "        print(\"❌ No sequences created. Check data preprocessing.\")\n",
        "        return None\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader, val_loader = create_dataloaders(\n",
        "        sequences, targets,\n",
        "        train_ratio=0.8,\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\n=== MODEL INITIALIZATION ===\")\n",
        "\n",
        "    model_config = {\n",
        "        \"patch_length\": patch_length,\n",
        "        \"n_patches\": n_patches,\n",
        "        \"n_features\": n_features,\n",
        "        \"forecast_horizon\": 1,\n",
        "        \"d_model\": 256,\n",
        "        \"n_heads\": 8,\n",
        "        \"n_layers\": 4,\n",
        "        \"d_ff\": 512,\n",
        "        \"dropout\": 0.1,\n",
        "        \"channel_independent\": True\n",
        "    }\n",
        "\n",
        "    model = PatchTSTModel(**model_config).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"\\nModel initialized with config: {model_config}\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n=== TRAINING ===\")\n",
        "\n",
        "    num_epochs = 10\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (sequences_batch, targets_batch) in enumerate(train_loader):\n",
        "            sequences_batch = sequences_batch.to(device)\n",
        "            targets_batch = targets_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences_batch)\n",
        "            loss = criterion(outputs, targets_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences_batch, targets_batch in val_loader:\n",
        "                sequences_batch = sequences_batch.to(device)\n",
        "                targets_batch = targets_batch.to(device)\n",
        "\n",
        "                outputs = model(sequences_batch)\n",
        "                loss = criterion(outputs, targets_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                all_predictions.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(targets_batch.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(all_targets, all_predictions)\n",
        "        rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
        "        r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "        # MAPE calculation with safety check\n",
        "        def safe_mape(y_true, y_pred):\n",
        "            mask = np.abs(y_true) > 1e-8\n",
        "            if mask.sum() == 0:\n",
        "                return float('inf')\n",
        "            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "        mape = safe_mape(np.array(all_targets), np.array(all_predictions))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"  Val MAE: {mae:.2f}\")\n",
        "        print(f\"  Val RMSE: {rmse:.2f}\")\n",
        "        print(f\"  Val MAPE: {mape:.2f}%\")\n",
        "        print(f\"  Val R²: {r2:.4f}\")\n",
        "\n",
        "        # Log to wandb\n",
        "        try:\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": avg_train_loss,\n",
        "                \"val_loss\": avg_val_loss,\n",
        "                \"val_mae\": mae,\n",
        "                \"val_rmse\": rmse,\n",
        "                \"val_mape\": mape if not np.isinf(mape) else 0.0,\n",
        "                \"val_r2\": r2\n",
        "            })\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Final evaluation on full validation set\n",
        "    print(\"\\n=== FINAL EVALUATION ===\")\n",
        "\n",
        "    model.eval()\n",
        "    final_predictions = []\n",
        "    final_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences_batch, targets_batch in val_loader:\n",
        "            sequences_batch = sequences_batch.to(device)\n",
        "            outputs = model(sequences_batch)\n",
        "            final_predictions.extend(outputs.cpu().numpy())\n",
        "            final_targets.extend(targets_batch.cpu().numpy())\n",
        "\n",
        "    # Calculate final metrics\n",
        "    final_mae = mean_absolute_error(final_targets, final_predictions)\n",
        "    final_rmse = np.sqrt(mean_squared_error(final_targets, final_predictions))\n",
        "    final_r2 = r2_score(final_targets, final_predictions)\n",
        "    final_mape = safe_mape(np.array(final_targets), np.array(final_predictions))\n",
        "\n",
        "    print(f\"\\nFinal Validation Metrics:\")\n",
        "    print(f\"MAE: {final_mae:.2f}\")\n",
        "    print(f\"RMSE: {final_rmse:.2f}\")\n",
        "    print(f\"MAPE: {final_mape:.2f}%\")\n",
        "    print(f\"R²: {final_r2:.4f}\")\n",
        "\n",
        "    # Create complete pipeline\n",
        "    class PatchTSTPipeline:\n",
        "        \"\"\"Complete pipeline for PatchTST inference\"\"\"\n",
        "\n",
        "        def __init__(self, feature_merger, missing_handler, ts_processor, model):\n",
        "            self.feature_merger = feature_merger\n",
        "            self.missing_handler = missing_handler\n",
        "            self.ts_processor = ts_processor\n",
        "            self.model = model\n",
        "            self.model.eval()\n",
        "\n",
        "        def predict(self, X_raw, stores_df=None, features_df=None):\n",
        "            \"\"\"Make predictions on raw test data\"\"\"\n",
        "            # If auxiliary data provided, update the merger\n",
        "            if stores_df is not None or features_df is not None:\n",
        "                self.feature_merger.fit(X_raw, stores_df=stores_df, features_df=features_df)\n",
        "\n",
        "            # Process through pipeline\n",
        "            merged_data = self.feature_merger.transform(X_raw)\n",
        "            cleaned_data = self.missing_handler.transform(merged_data)\n",
        "\n",
        "            # Handle categorical features (one-hot encode 'Type')\n",
        "            if 'Type' in cleaned_data.columns:\n",
        "                cleaned_data = pd.get_dummies(cleaned_data, columns=['Type'], prefix='StoreType', dtype=float)\n",
        "\n",
        "            # Drop any remaining non-numerical columns except 'Date', 'Store', 'Dept'\n",
        "            non_numerical_cols = cleaned_data.select_dtypes(exclude=np.number).columns.tolist()\n",
        "            cols_to_drop = [col for col in non_numerical_cols if col not in ['Date', 'Store', 'Dept']]\n",
        "            if cols_to_drop:\n",
        "                cleaned_data.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "            # Ensure all columns are numerical before processing, keeping Date, Store, Dept for time series processing\n",
        "            cleaned_data_prepared_cols = ['Store', 'Dept', 'Date']\n",
        "            numerical_cols = cleaned_data.select_dtypes(include=np.number).columns.tolist()\n",
        "            for col in numerical_cols:\n",
        "                if col not in cleaned_data_prepared_cols:\n",
        "                    cleaned_data_prepared_cols.append(col)\n",
        "\n",
        "            cleaned_data_prepared_cols = [col for col in cleaned_data_prepared_cols if col in cleaned_data.columns]\n",
        "\n",
        "            cleaned_data_prepared = cleaned_data[cleaned_data_prepared_cols].copy()\n",
        "\n",
        "            # Drop any rows that might have introduced NaNs during feature engineering if necessary\n",
        "            cleaned_data_prepared.dropna(inplace=True)\n",
        "\n",
        "\n",
        "            # For inference, we need to create sequences from the cleaned data\n",
        "            processed = self.ts_processor.transform(cleaned_data_prepared)\n",
        "\n",
        "            if processed['sequences'].size == 0:\n",
        "                return np.array([])\n",
        "\n",
        "            # Convert to tensor and predict\n",
        "            sequences_tensor = torch.FloatTensor(processed['sequences']).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predictions = self.model(sequences_tensor)\n",
        "\n",
        "            return predictions.cpu().numpy().flatten()\n",
        "\n",
        "    # Create final pipeline\n",
        "    final_pipeline = PatchTSTPipeline(\n",
        "        feature_merger=feature_merger,\n",
        "        missing_handler=missing_handler,\n",
        "        ts_processor=ts_processor,\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    print(\"\\n=== SAVING MODEL ===\")\n",
        "\n",
        "    try:\n",
        "        import cloudpickle\n",
        "    except ImportError:\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', 'cloudpickle'])\n",
        "        import cloudpickle\n",
        "\n",
        "    # Create filename\n",
        "    pipeline_filename = f\"patchtst_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
        "\n",
        "    # Save with cloudpickle\n",
        "    with open(pipeline_filename, 'wb') as f:\n",
        "        cloudpickle.dump(final_pipeline, f)\n",
        "\n",
        "    print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "\n",
        "    # Log to wandb\n",
        "    try:\n",
        "        # Create model artifact\n",
        "        model_artifact = wandb.Artifact(\n",
        "            name=\"PatchTST_pipeline\",\n",
        "            type=\"model\",\n",
        "            description=\"Final PatchTST pipeline for Walmart sales forecasting\",\n",
        "            metadata={\n",
        "                \"val_mae\": float(final_mae),\n",
        "                \"val_rmse\": float(final_rmse),\n",
        "                \"val_mape\": float(final_mape) if not np.isinf(final_mape) else 0.0,\n",
        "                \"val_r2\": float(final_r2),\n",
        "                \"sequences_count\": len(sequences),\n",
        "                \"validation_samples\": len(final_targets),\n",
        "                \"model_type\": \"PatchTST\",\n",
        "                \"patch_length\": patch_length,\n",
        "                \"n_patches\": n_patches,\n",
        "                \"channel_independent\": True,\n",
        "                \"d_model\": model_config[\"d_model\"],\n",
        "                \"n_heads\": model_config[\"n_heads\"],\n",
        "                \"n_layers\": model_config[\"n_layers\"]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Add model file to artifact\n",
        "        model_artifact.add_file(pipeline_filename)\n",
        "\n",
        "        # Log artifact\n",
        "        wandb.log_artifact(model_artifact)\n",
        "\n",
        "        print(\"✓ Model logged to wandb\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed to log to wandb: {e}\")\n",
        "\n",
        "    print(\"\\n=== EXPERIMENT COMPLETED ===\")\n",
        "\n",
        "    return {\n",
        "        'pipeline': final_pipeline,\n",
        "        'model': model,\n",
        "        'metrics': {\n",
        "            'mae': final_mae,\n",
        "            'rmse': final_rmse,\n",
        "            'mape': final_mape,\n",
        "            'r2': final_r2\n",
        "        },\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'filename': pipeline_filename\n",
        "    }\n",
        "\n",
        "print(\"✓ PatchTST experiment function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cross_validation"
      },
      "source": [
        "# Cross-Validation Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cross_validation_code",
        "outputId": "dc34feda-9b10-413d-9ac8-e427e8358437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PATCHTST CROSS-VALIDATION (3 splits) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">PatchTST_CrossValidation</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/brf1prfz' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/brf1prfz</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_183527-brf1prfz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_183724-siz1aqbh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/siz1aqbh' target=\"_blank\">PatchTST_CrossValidation</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/siz1aqbh' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/siz1aqbh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After merging: (421570, 17)\n",
            "After cleaning: (421570, 17)\n",
            "After one-hot encoding 'Type': (421570, 19)\n",
            "Dropped non-numerical columns: ['IsHoliday_x', 'IsHoliday_y']\n",
            "After dropping non-numerical: (421570, 17)\n",
            "After dropping rows with NaNs: (421570, 17)\n",
            "PatchTimeSeriesDataProcessor fitted:\n",
            "- Store-Dept combinations: 3331\n",
            "- Date range: 142 weeks\n",
            "- Lookback window: 52 weeks\n",
            "- Patch length: 13 weeks\n",
            "- Number of patches per sequence: 4\n",
            "Creating patch-based sequences for 3331 store-dept combinations...\n",
            "Processed 0/3331 combinations\n",
            "Processed 100/3331 combinations\n",
            "Processed 200/3331 combinations\n",
            "Processed 300/3331 combinations\n",
            "Processed 400/3331 combinations\n",
            "Processed 500/3331 combinations\n",
            "Processed 600/3331 combinations\n",
            "Processed 700/3331 combinations\n",
            "Processed 900/3331 combinations\n",
            "Processed 1000/3331 combinations\n",
            "Processed 1200/3331 combinations\n",
            "Processed 1300/3331 combinations\n",
            "Processed 1400/3331 combinations\n",
            "Processed 1600/3331 combinations\n",
            "Processed 1700/3331 combinations\n",
            "Processed 1800/3331 combinations\n",
            "Processed 1900/3331 combinations\n",
            "Processed 2000/3331 combinations\n",
            "Processed 2100/3331 combinations\n",
            "Processed 2200/3331 combinations\n",
            "Processed 2300/3331 combinations\n",
            "Processed 2400/3331 combinations\n",
            "Processed 2500/3331 combinations\n",
            "Processed 2600/3331 combinations\n",
            "Processed 2700/3331 combinations\n",
            "Processed 2800/3331 combinations\n",
            "Processed 3000/3331 combinations\n",
            "Processed 3200/3331 combinations\n",
            "Processed 3300/3331 combinations\n",
            "Created 261083 patch-based sequences\n",
            "\n",
            "--- Fold 1/3 ---\n",
            "Train: 65273, Val: 65270\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchTSTModel initialized:\n",
            "- patch_length: 13\n",
            "- n_patches: 4\n",
            "- n_features: 14\n",
            "- d_model: 128\n",
            "- n_heads: 4\n",
            "- n_layers: 2\n",
            "- channel_independent: True\n",
            "- forecast_horizon: 1\n",
            "Fold 1 Results:\n",
            "  MAE: 14786.34\n",
            "  RMSE: 26389.53\n",
            "  R²: -0.1107\n",
            "\n",
            "--- Fold 2/3 ---\n",
            "Train: 130543, Val: 65270\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchTSTModel initialized:\n",
            "- patch_length: 13\n",
            "- n_patches: 4\n",
            "- n_features: 14\n",
            "- d_model: 128\n",
            "- n_heads: 4\n",
            "- n_layers: 2\n",
            "- channel_independent: True\n",
            "- forecast_horizon: 1\n",
            "Fold 2 Results:\n",
            "  MAE: 15714.00\n",
            "  RMSE: 21723.76\n",
            "  R²: -0.0069\n",
            "\n",
            "--- Fold 3/3 ---\n",
            "Train: 195813, Val: 65270\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 128\n",
            "- input_dim: 13\n",
            "PatchTSTModel initialized:\n",
            "- patch_length: 13\n",
            "- n_patches: 4\n",
            "- n_features: 14\n",
            "- d_model: 128\n",
            "- n_heads: 4\n",
            "- n_layers: 2\n",
            "- channel_independent: True\n",
            "- forecast_horizon: 1\n",
            "Fold 3 Results:\n",
            "  MAE: 12046.66\n",
            "  RMSE: 16633.52\n",
            "  R²: 0.1868\n",
            "\n",
            "=== CROSS-VALIDATION SUMMARY ===\n",
            "MAE: 14182.33 ± 1556.91\n",
            "RMSE: 21582.27 ± 3984.13\n",
            "R²: 0.0231 ± 0.1233\n",
            "\n",
            "✓ Cross-validation completed successfully!\n"
          ]
        }
      ],
      "source": [
        "def run_patchtst_cross_validation(n_splits=3):\n",
        "    \"\"\"Run time series cross-validation for PatchTST\"\"\"\n",
        "\n",
        "    print(f\"\\n=== PATCHTST CROSS-VALIDATION ({n_splits} splits) ===\")\n",
        "\n",
        "    # Initialize new wandb run\n",
        "    try:\n",
        "        wandb.finish()\n",
        "        wandb.init(\n",
        "            project=\"walmart-sales-forecasting\",\n",
        "            name=\"PatchTST_CrossValidation\",\n",
        "            config={\n",
        "                \"model_type\": \"PatchTST\",\n",
        "                \"cv_splits\": n_splits,\n",
        "                \"experiment_type\": \"cross_validation\"\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Prepare data (reuse preprocessing from main experiment)\n",
        "    feature_merger = WalmartFeatureMerger()\n",
        "    missing_handler = WalmartMissingValueHandler()\n",
        "    ts_processor = PatchTimeSeriesDataProcessor(\n",
        "        lookback_window=52,\n",
        "        forecast_horizon=1,\n",
        "        patch_length=13,\n",
        "        stride=13\n",
        "    )\n",
        "\n",
        "    # Fit transformers\n",
        "    feature_merger.fit(train_df, stores_df=stores_df, features_df=features_df)\n",
        "\n",
        "    # Transform data\n",
        "    merged_data = feature_merger.transform(train_df)\n",
        "    print(f\"After merging: {merged_data.shape}\")\n",
        "\n",
        "    missing_handler.fit(merged_data)\n",
        "    cleaned_data = missing_handler.transform(merged_data)\n",
        "    print(f\"After cleaning: {cleaned_data.shape}\")\n",
        "\n",
        "    # Handle categorical features (one-hot encode 'Type')\n",
        "    if 'Type' in cleaned_data.columns:\n",
        "        cleaned_data = pd.get_dummies(cleaned_data, columns=['Type'], prefix='StoreType', dtype=float)\n",
        "        print(f\"After one-hot encoding 'Type': {cleaned_data.shape}\")\n",
        "\n",
        "    # Drop any remaining non-numerical columns except 'Date', 'Store', 'Dept'\n",
        "    non_numerical_cols = cleaned_data.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    cols_to_drop = [col for col in non_numerical_cols if col not in ['Date', 'Store', 'Dept']]\n",
        "    if cols_to_drop:\n",
        "        cleaned_data.drop(columns=cols_to_drop, inplace=True)\n",
        "        print(f\"Dropped non-numerical columns: {cols_to_drop}\")\n",
        "        print(f\"After dropping non-numerical: {cleaned_data.shape}\")\n",
        "\n",
        "    # Ensure all columns are numerical before processing, keeping Date, Store, Dept for time series processing\n",
        "    # Start with the required ID columns\n",
        "    cleaned_data_prepared_cols = ['Store', 'Dept', 'Date']\n",
        "\n",
        "    # Add all numerical columns from cleaned_data, excluding the ID columns already added\n",
        "    numerical_cols = cleaned_data.select_dtypes(include=np.number).columns.tolist()\n",
        "    for col in numerical_cols:\n",
        "        if col not in cleaned_data_prepared_cols:\n",
        "            cleaned_data_prepared_cols.append(col)\n",
        "\n",
        "    # Ensure all columns in cleaned_data_prepared_cols actually exist in cleaned_data\n",
        "    cleaned_data_prepared_cols = [col for col in cleaned_data_prepared_cols if col in cleaned_data.columns]\n",
        "\n",
        "    # Create the prepared DataFrame with the correct columns and order\n",
        "    cleaned_data_prepared = cleaned_data[cleaned_data_prepared_cols].copy()\n",
        "\n",
        "    # Drop any rows that might have introduced NaNs during feature engineering if necessary\n",
        "    cleaned_data_prepared.dropna(inplace=True)\n",
        "    print(f\"After dropping rows with NaNs: {cleaned_data_prepared.shape}\")\n",
        "\n",
        "    # Create time series sequences\n",
        "    ts_processor.fit(cleaned_data_prepared)\n",
        "    processed_data = ts_processor.transform(cleaned_data_prepared)\n",
        "\n",
        "    sequences = processed_data['sequences']\n",
        "    targets = processed_data['targets']\n",
        "\n",
        "    if len(sequences) == 0:\n",
        "        print(\"❌ No sequences for cross-validation\")\n",
        "        return None\n",
        "\n",
        "    # Time series split\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    cv_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(sequences)):\n",
        "        print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "        # Split data\n",
        "        train_seq, val_seq = sequences[train_idx], sequences[val_idx]\n",
        "        train_tgt, val_tgt = targets[train_idx], targets[val_idx]\n",
        "\n",
        "        print(f\"Train: {len(train_seq)}, Val: {len(val_seq)}\")\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = PatchTSTDataset(train_seq, train_tgt)\n",
        "        val_dataset = PatchTSTDataset(val_seq, val_tgt)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Initialize model\n",
        "        model = PatchTSTModel(\n",
        "            patch_length=processed_data['patch_length'],\n",
        "            n_patches=processed_data['n_patches'],\n",
        "            n_features=processed_data['n_features'],\n",
        "            forecast_horizon=1,\n",
        "            d_model=128,  # Smaller for faster CV\n",
        "            n_heads=4,\n",
        "            n_layers=2,\n",
        "            channel_independent=True\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Quick training (fewer epochs for CV)\n",
        "        num_epochs = 5\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch_idx, (seq_batch, tgt_batch) in enumerate(train_loader):\n",
        "                seq_batch = seq_batch.to(device)\n",
        "                tgt_batch = tgt_batch.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(seq_batch)\n",
        "                loss = criterion(outputs, tgt_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        actuals = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for seq_batch, tgt_batch in val_loader:\n",
        "                seq_batch = seq_batch.to(device)\n",
        "                outputs = model(seq_batch)\n",
        "                predictions.extend(outputs.cpu().numpy())\n",
        "                actuals.extend(tgt_batch.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(actuals, predictions)\n",
        "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
        "        r2 = r2_score(actuals, predictions)\n",
        "\n",
        "        fold_results = {\n",
        "            'fold': fold + 1,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'r2': r2\n",
        "        }\n",
        "\n",
        "        cv_results.append(fold_results)\n",
        "\n",
        "        print(f\"Fold {fold + 1} Results:\")\n",
        "        print(f\"  MAE: {mae:.2f}\")\n",
        "        print(f\"  RMSE: {rmse:.2f}\")\n",
        "        print(f\"  R²: {r2:.4f}\")\n",
        "\n",
        "        # Log to wandb\n",
        "        try:\n",
        "            wandb.log({\n",
        "                f\"cv_fold_{fold + 1}_mae\": mae,\n",
        "                f\"cv_fold_{fold + 1}_rmse\": rmse,\n",
        "                f\"cv_fold_{fold + 1}_r2\": r2\n",
        "            })\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Calculate overall CV statistics\n",
        "    cv_mae = [r['mae'] for r in cv_results]\n",
        "    cv_rmse = [r['rmse'] for r in cv_results]\n",
        "    cv_r2 = [r['r2'] for r in cv_results]\n",
        "\n",
        "    summary = {\n",
        "        'mean_mae': np.mean(cv_mae),\n",
        "        'std_mae': np.std(cv_mae),\n",
        "        'mean_rmse': np.mean(cv_rmse),\n",
        "        'std_rmse': np.std(cv_rmse),\n",
        "        'mean_r2': np.mean(cv_r2),\n",
        "        'std_r2': np.std(cv_r2)\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== CROSS-VALIDATION SUMMARY ===\")\n",
        "    print(f\"MAE: {summary['mean_mae']:.2f} ± {summary['std_mae']:.2f}\")\n",
        "    print(f\"RMSE: {summary['mean_rmse']:.2f} ± {summary['std_rmse']:.2f}\")\n",
        "    print(f\"R²: {summary['mean_r2']:.4f} ± {summary['std_r2']:.4f}\")\n",
        "\n",
        "    # Log summary to wandb\n",
        "    try:\n",
        "        wandb.log({\n",
        "            \"cv_mean_mae\": summary['mean_mae'],\n",
        "            \"cv_std_mae\": summary['std_mae'],\n",
        "            \"cv_mean_rmse\": summary['mean_rmse'],\n",
        "            \"cv_std_rmse\": summary['std_rmse'],\n",
        "            \"cv_mean_r2\": summary['mean_r2'],\n",
        "            \"cv_std_r2\": summary['std_r2']\n",
        "        })\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        'fold_results': cv_results,\n",
        "        'summary': summary\n",
        "    }\n",
        "\n",
        "# Run cross-validation\n",
        "cv_results = run_patchtst_cross_validation(n_splits=3)\n",
        "\n",
        "if cv_results:\n",
        "    print(f\"\\n✓ Cross-validation completed successfully!\")\n",
        "else:\n",
        "    print(\"❌ Cross-validation failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_experiment"
      },
      "source": [
        "# Run PatchTST Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "run_experiment_code",
        "outputId": "5a6e3ba9-2b5e-42b5-8233-6777969ee8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== STARTING PATCHTST EXPERIMENT ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">PatchTST_Training</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/dkfufoki' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/dkfufoki</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_174726-dkfufoki/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_174913-xhs89piv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/xhs89piv' target=\"_blank\">PatchTST_Training</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/xhs89piv' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/xhs89piv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA PREPROCESSING ===\n",
            "After merging: (421570, 17)\n",
            "After cleaning: (421570, 17)\n",
            "After one-hot encoding 'Type': (421570, 19)\n",
            "Dropped non-numerical columns: ['IsHoliday_x', 'IsHoliday_y']\n",
            "After dropping non-numerical: (421570, 17)\n",
            "After dropping rows with NaNs: (421570, 17)\n",
            "PatchTimeSeriesDataProcessor fitted:\n",
            "- Store-Dept combinations: 3331\n",
            "- Date range: 142 weeks\n",
            "- Lookback window: 52 weeks\n",
            "- Patch length: 13 weeks\n",
            "- Number of patches per sequence: 4\n",
            "Creating patch-based sequences for 3331 store-dept combinations...\n",
            "Processed 0/3331 combinations\n",
            "Processed 100/3331 combinations\n",
            "Processed 200/3331 combinations\n",
            "Processed 300/3331 combinations\n",
            "Processed 400/3331 combinations\n",
            "Processed 500/3331 combinations\n",
            "Processed 600/3331 combinations\n",
            "Processed 700/3331 combinations\n",
            "Processed 900/3331 combinations\n",
            "Processed 1000/3331 combinations\n",
            "Processed 1200/3331 combinations\n",
            "Processed 1300/3331 combinations\n",
            "Processed 1400/3331 combinations\n",
            "Processed 1600/3331 combinations\n",
            "Processed 1700/3331 combinations\n",
            "Processed 1800/3331 combinations\n",
            "Processed 1900/3331 combinations\n",
            "Processed 2000/3331 combinations\n",
            "Processed 2100/3331 combinations\n",
            "Processed 2200/3331 combinations\n",
            "Processed 2300/3331 combinations\n",
            "Processed 2400/3331 combinations\n",
            "Processed 2500/3331 combinations\n",
            "Processed 2600/3331 combinations\n",
            "Processed 2700/3331 combinations\n",
            "Processed 2800/3331 combinations\n",
            "Processed 3000/3331 combinations\n",
            "Processed 3200/3331 combinations\n",
            "Processed 3300/3331 combinations\n",
            "Created 261083 patch-based sequences\n",
            "\n",
            "Sequences shape: (261083, 4, 13, 14)\n",
            "Targets shape: (261083,)\n",
            "Number of patches: 4\n",
            "Patch length: 13\n",
            "Number of features: 14\n",
            "Created dataloaders:\n",
            "- Training samples: 208866\n",
            "- Validation samples: 52217\n",
            "- Batch size: 64\n",
            "\n",
            "=== MODEL INITIALIZATION ===\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchEmbedding initialized:\n",
            "- patch_length: 13\n",
            "- n_features: 1\n",
            "- d_model: 256\n",
            "- input_dim: 13\n",
            "PatchTSTModel initialized:\n",
            "- patch_length: 13\n",
            "- n_patches: 4\n",
            "- n_features: 14\n",
            "- d_model: 256\n",
            "- n_heads: 8\n",
            "- n_layers: 4\n",
            "- channel_independent: True\n",
            "- forecast_horizon: 1\n",
            "\n",
            "Model initialized with config: {'patch_length': 13, 'n_patches': 4, 'n_features': 14, 'forecast_horizon': 1, 'd_model': 256, 'n_heads': 8, 'n_layers': 4, 'd_ff': 512, 'dropout': 0.1, 'channel_independent': True}\n",
            "Total parameters: 2,158,849\n",
            "\n",
            "=== TRAINING ===\n",
            "Epoch 1/10, Batch 0, Loss: 705046656.0000\n",
            "Epoch 1/10, Batch 50, Loss: 1209797376.0000\n",
            "Epoch 1/10, Batch 100, Loss: 797104576.0000\n",
            "Epoch 1/10, Batch 150, Loss: 368516448.0000\n",
            "Epoch 1/10, Batch 200, Loss: 513506176.0000\n",
            "Epoch 1/10, Batch 250, Loss: 687717376.0000\n",
            "Epoch 1/10, Batch 300, Loss: 924121856.0000\n",
            "Epoch 1/10, Batch 350, Loss: 1103468416.0000\n",
            "Epoch 1/10, Batch 400, Loss: 531979552.0000\n",
            "Epoch 1/10, Batch 450, Loss: 707333632.0000\n",
            "Epoch 1/10, Batch 500, Loss: 1671169536.0000\n",
            "Epoch 1/10, Batch 550, Loss: 485436160.0000\n",
            "Epoch 1/10, Batch 600, Loss: 1279914880.0000\n",
            "Epoch 1/10, Batch 650, Loss: 588074624.0000\n",
            "Epoch 1/10, Batch 700, Loss: 405082336.0000\n",
            "Epoch 1/10, Batch 750, Loss: 744582016.0000\n",
            "Epoch 1/10, Batch 800, Loss: 1160316672.0000\n",
            "Epoch 1/10, Batch 850, Loss: 429636736.0000\n",
            "Epoch 1/10, Batch 900, Loss: 917302400.0000\n",
            "Epoch 1/10, Batch 950, Loss: 677925312.0000\n",
            "Epoch 1/10, Batch 1000, Loss: 647581888.0000\n",
            "Epoch 1/10, Batch 1050, Loss: 552667392.0000\n",
            "Epoch 1/10, Batch 1100, Loss: 1369196544.0000\n",
            "Epoch 1/10, Batch 1150, Loss: 688170688.0000\n",
            "Epoch 1/10, Batch 1200, Loss: 1368198016.0000\n",
            "Epoch 1/10, Batch 1250, Loss: 1002456512.0000\n",
            "Epoch 1/10, Batch 1300, Loss: 679840256.0000\n",
            "Epoch 1/10, Batch 1350, Loss: 230780880.0000\n",
            "Epoch 1/10, Batch 1400, Loss: 574841344.0000\n",
            "Epoch 1/10, Batch 1450, Loss: 475175360.0000\n",
            "Epoch 1/10, Batch 1500, Loss: 664064832.0000\n",
            "Epoch 1/10, Batch 1550, Loss: 596018880.0000\n",
            "Epoch 1/10, Batch 1600, Loss: 592414336.0000\n",
            "Epoch 1/10, Batch 1650, Loss: 691020544.0000\n",
            "Epoch 1/10, Batch 1700, Loss: 454792320.0000\n",
            "Epoch 1/10, Batch 1750, Loss: 675939328.0000\n",
            "Epoch 1/10, Batch 1800, Loss: 848904576.0000\n",
            "Epoch 1/10, Batch 1850, Loss: 456225248.0000\n",
            "Epoch 1/10, Batch 1900, Loss: 587539456.0000\n",
            "Epoch 1/10, Batch 1950, Loss: 1011518976.0000\n",
            "Epoch 1/10, Batch 2000, Loss: 530613184.0000\n",
            "Epoch 1/10, Batch 2050, Loss: 648498688.0000\n",
            "Epoch 1/10, Batch 2100, Loss: 570451456.0000\n",
            "Epoch 1/10, Batch 2150, Loss: 888721600.0000\n",
            "Epoch 1/10, Batch 2200, Loss: 403640640.0000\n",
            "Epoch 1/10, Batch 2250, Loss: 469299008.0000\n",
            "Epoch 1/10, Batch 2300, Loss: 302952768.0000\n",
            "Epoch 1/10, Batch 2350, Loss: 501098432.0000\n",
            "Epoch 1/10, Batch 2400, Loss: 363426880.0000\n",
            "Epoch 1/10, Batch 2450, Loss: 979333760.0000\n",
            "Epoch 1/10, Batch 2500, Loss: 284096064.0000\n",
            "Epoch 1/10, Batch 2550, Loss: 395737632.0000\n",
            "Epoch 1/10, Batch 2600, Loss: 372697472.0000\n",
            "Epoch 1/10, Batch 2650, Loss: 308739872.0000\n",
            "Epoch 1/10, Batch 2700, Loss: 361219872.0000\n",
            "Epoch 1/10, Batch 2750, Loss: 971468608.0000\n",
            "Epoch 1/10, Batch 2800, Loss: 645608000.0000\n",
            "Epoch 1/10, Batch 2850, Loss: 569246208.0000\n",
            "Epoch 1/10, Batch 2900, Loss: 227016544.0000\n",
            "Epoch 1/10, Batch 2950, Loss: 527958592.0000\n",
            "Epoch 1/10, Batch 3000, Loss: 861014784.0000\n",
            "Epoch 1/10, Batch 3050, Loss: 407402080.0000\n",
            "Epoch 1/10, Batch 3100, Loss: 580523072.0000\n",
            "Epoch 1/10, Batch 3150, Loss: 555347584.0000\n",
            "Epoch 1/10, Batch 3200, Loss: 664640384.0000\n",
            "Epoch 1/10, Batch 3250, Loss: 609311744.0000\n",
            "Epoch 1/10:\n",
            "  Train Loss: 690540380.6225\n",
            "  Val Loss: 571218928.7843\n",
            "  Val MAE: 13612.18\n",
            "  Val RMSE: 23899.52\n",
            "  Val MAPE: 9192.15%\n",
            "  Val R²: -0.0963\n",
            "Epoch 2/10, Batch 0, Loss: 767223680.0000\n",
            "Epoch 2/10, Batch 50, Loss: 832246592.0000\n",
            "Epoch 2/10, Batch 100, Loss: 254679104.0000\n",
            "Epoch 2/10, Batch 150, Loss: 259838720.0000\n",
            "Epoch 2/10, Batch 200, Loss: 945674112.0000\n",
            "Epoch 2/10, Batch 250, Loss: 586155520.0000\n",
            "Epoch 2/10, Batch 300, Loss: 258616608.0000\n",
            "Epoch 2/10, Batch 350, Loss: 960716800.0000\n",
            "Epoch 2/10, Batch 400, Loss: 946069568.0000\n",
            "Epoch 2/10, Batch 450, Loss: 511075168.0000\n",
            "Epoch 2/10, Batch 500, Loss: 685743744.0000\n",
            "Epoch 2/10, Batch 550, Loss: 615623040.0000\n",
            "Epoch 2/10, Batch 600, Loss: 346096352.0000\n",
            "Epoch 2/10, Batch 650, Loss: 330699040.0000\n",
            "Epoch 2/10, Batch 700, Loss: 347707296.0000\n",
            "Epoch 2/10, Batch 750, Loss: 748909440.0000\n",
            "Epoch 2/10, Batch 800, Loss: 349493376.0000\n",
            "Epoch 2/10, Batch 850, Loss: 839016640.0000\n",
            "Epoch 2/10, Batch 900, Loss: 490055456.0000\n",
            "Epoch 2/10, Batch 950, Loss: 519114112.0000\n",
            "Epoch 2/10, Batch 1000, Loss: 1167999744.0000\n",
            "Epoch 2/10, Batch 1050, Loss: 232566416.0000\n",
            "Epoch 2/10, Batch 1100, Loss: 315357728.0000\n",
            "Epoch 2/10, Batch 1150, Loss: 313779776.0000\n",
            "Epoch 2/10, Batch 1200, Loss: 191969760.0000\n",
            "Epoch 2/10, Batch 1250, Loss: 743529856.0000\n",
            "Epoch 2/10, Batch 1300, Loss: 175050912.0000\n",
            "Epoch 2/10, Batch 1350, Loss: 600267776.0000\n",
            "Epoch 2/10, Batch 1400, Loss: 638668096.0000\n",
            "Epoch 2/10, Batch 1450, Loss: 621330112.0000\n",
            "Epoch 2/10, Batch 1500, Loss: 538737536.0000\n",
            "Epoch 2/10, Batch 1550, Loss: 579120640.0000\n",
            "Epoch 2/10, Batch 1600, Loss: 447505056.0000\n",
            "Epoch 2/10, Batch 1650, Loss: 685487552.0000\n",
            "Epoch 2/10, Batch 1700, Loss: 258433776.0000\n",
            "Epoch 2/10, Batch 1750, Loss: 545783296.0000\n",
            "Epoch 2/10, Batch 1800, Loss: 374933792.0000\n",
            "Epoch 2/10, Batch 1850, Loss: 600882304.0000\n",
            "Epoch 2/10, Batch 1900, Loss: 299907904.0000\n",
            "Epoch 2/10, Batch 1950, Loss: 706780928.0000\n",
            "Epoch 2/10, Batch 2000, Loss: 912017728.0000\n",
            "Epoch 2/10, Batch 2050, Loss: 583565248.0000\n",
            "Epoch 2/10, Batch 2100, Loss: 425699264.0000\n",
            "Epoch 2/10, Batch 2150, Loss: 301374016.0000\n",
            "Epoch 2/10, Batch 2200, Loss: 382179648.0000\n",
            "Epoch 2/10, Batch 2250, Loss: 528629728.0000\n",
            "Epoch 2/10, Batch 2300, Loss: 684811648.0000\n",
            "Epoch 2/10, Batch 2350, Loss: 392952896.0000\n",
            "Epoch 2/10, Batch 2400, Loss: 649827328.0000\n",
            "Epoch 2/10, Batch 2450, Loss: 449910688.0000\n",
            "Epoch 2/10, Batch 2500, Loss: 335897536.0000\n",
            "Epoch 2/10, Batch 2550, Loss: 819003200.0000\n",
            "Epoch 2/10, Batch 2600, Loss: 899519488.0000\n",
            "Epoch 2/10, Batch 2650, Loss: 195494304.0000\n",
            "Epoch 2/10, Batch 2700, Loss: 719621760.0000\n",
            "Epoch 2/10, Batch 2750, Loss: 699019392.0000\n",
            "Epoch 2/10, Batch 2800, Loss: 936691392.0000\n",
            "Epoch 2/10, Batch 2850, Loss: 548972160.0000\n",
            "Epoch 2/10, Batch 2900, Loss: 307342464.0000\n",
            "Epoch 2/10, Batch 2950, Loss: 396404320.0000\n",
            "Epoch 2/10, Batch 3000, Loss: 839139072.0000\n",
            "Epoch 2/10, Batch 3050, Loss: 573308160.0000\n",
            "Epoch 2/10, Batch 3100, Loss: 393715072.0000\n",
            "Epoch 2/10, Batch 3150, Loss: 538456064.0000\n",
            "Epoch 2/10, Batch 3200, Loss: 535481216.0000\n",
            "Epoch 2/10, Batch 3250, Loss: 266970368.0000\n",
            "Epoch 2/10:\n",
            "  Train Loss: 528910864.7279\n",
            "  Val Loss: 521039523.5882\n",
            "  Val MAE: 15295.72\n",
            "  Val RMSE: 22825.79\n",
            "  Val MAPE: 16171.38%\n",
            "  Val R²: -0.0000\n",
            "Epoch 3/10, Batch 0, Loss: 297528384.0000\n",
            "Epoch 3/10, Batch 50, Loss: 534467872.0000\n",
            "Epoch 3/10, Batch 100, Loss: 635566848.0000\n",
            "Epoch 3/10, Batch 150, Loss: 488701312.0000\n",
            "Epoch 3/10, Batch 200, Loss: 588398464.0000\n",
            "Epoch 3/10, Batch 250, Loss: 363787936.0000\n",
            "Epoch 3/10, Batch 300, Loss: 494703552.0000\n",
            "Epoch 3/10, Batch 350, Loss: 926273984.0000\n",
            "Epoch 3/10, Batch 400, Loss: 428916544.0000\n",
            "Epoch 3/10, Batch 450, Loss: 518745888.0000\n",
            "Epoch 3/10, Batch 500, Loss: 597637248.0000\n",
            "Epoch 3/10, Batch 550, Loss: 566542912.0000\n",
            "Epoch 3/10, Batch 600, Loss: 521926016.0000\n",
            "Epoch 3/10, Batch 650, Loss: 246020208.0000\n",
            "Epoch 3/10, Batch 700, Loss: 799080768.0000\n",
            "Epoch 3/10, Batch 750, Loss: 625221248.0000\n",
            "Epoch 3/10, Batch 800, Loss: 386379968.0000\n",
            "Epoch 3/10, Batch 850, Loss: 246226336.0000\n",
            "Epoch 3/10, Batch 900, Loss: 200319968.0000\n",
            "Epoch 3/10, Batch 950, Loss: 237949008.0000\n",
            "Epoch 3/10, Batch 1000, Loss: 368577088.0000\n",
            "Epoch 3/10, Batch 1050, Loss: 280016128.0000\n",
            "Epoch 3/10, Batch 1100, Loss: 603030464.0000\n",
            "Epoch 3/10, Batch 1150, Loss: 932954048.0000\n",
            "Epoch 3/10, Batch 1200, Loss: 481158176.0000\n",
            "Epoch 3/10, Batch 1250, Loss: 213921184.0000\n",
            "Epoch 3/10, Batch 1300, Loss: 374365824.0000\n",
            "Epoch 3/10, Batch 1350, Loss: 753930880.0000\n",
            "Epoch 3/10, Batch 1400, Loss: 266031232.0000\n",
            "Epoch 3/10, Batch 1450, Loss: 200712768.0000\n",
            "Epoch 3/10, Batch 1500, Loss: 358282048.0000\n",
            "Epoch 3/10, Batch 1550, Loss: 698640384.0000\n",
            "Epoch 3/10, Batch 1600, Loss: 326050080.0000\n",
            "Epoch 3/10, Batch 1650, Loss: 721666688.0000\n",
            "Epoch 3/10, Batch 1700, Loss: 504481408.0000\n",
            "Epoch 3/10, Batch 1750, Loss: 480357632.0000\n",
            "Epoch 3/10, Batch 1800, Loss: 571468928.0000\n",
            "Epoch 3/10, Batch 1850, Loss: 341284576.0000\n",
            "Epoch 3/10, Batch 1900, Loss: 937744960.0000\n",
            "Epoch 3/10, Batch 1950, Loss: 430100032.0000\n",
            "Epoch 3/10, Batch 2000, Loss: 782309376.0000\n",
            "Epoch 3/10, Batch 2050, Loss: 819125120.0000\n",
            "Epoch 3/10, Batch 2100, Loss: 643141568.0000\n",
            "Epoch 3/10, Batch 2150, Loss: 660601472.0000\n",
            "Epoch 3/10, Batch 2200, Loss: 363081024.0000\n",
            "Epoch 3/10, Batch 2250, Loss: 930425152.0000\n",
            "Epoch 3/10, Batch 2300, Loss: 233042272.0000\n",
            "Epoch 3/10, Batch 2350, Loss: 881133824.0000\n",
            "Epoch 3/10, Batch 2400, Loss: 368325440.0000\n",
            "Epoch 3/10, Batch 2450, Loss: 231892832.0000\n",
            "Epoch 3/10, Batch 2500, Loss: 269153888.0000\n",
            "Epoch 3/10, Batch 2550, Loss: 513232832.0000\n",
            "Epoch 3/10, Batch 2600, Loss: 303853888.0000\n",
            "Epoch 3/10, Batch 2650, Loss: 858348032.0000\n",
            "Epoch 3/10, Batch 2700, Loss: 387867904.0000\n",
            "Epoch 3/10, Batch 2750, Loss: 378196256.0000\n",
            "Epoch 3/10, Batch 2800, Loss: 259276640.0000\n",
            "Epoch 3/10, Batch 2850, Loss: 745743104.0000\n",
            "Epoch 3/10, Batch 2900, Loss: 964020352.0000\n",
            "Epoch 3/10, Batch 2950, Loss: 760458304.0000\n",
            "Epoch 3/10, Batch 3000, Loss: 615018368.0000\n",
            "Epoch 3/10, Batch 3050, Loss: 449197632.0000\n",
            "Epoch 3/10, Batch 3100, Loss: 356421888.0000\n",
            "Epoch 3/10, Batch 3150, Loss: 977863808.0000\n",
            "Epoch 3/10, Batch 3200, Loss: 534160352.0000\n",
            "Epoch 3/10, Batch 3250, Loss: 393996160.0000\n",
            "Epoch 3/10:\n",
            "  Train Loss: 518201779.6275\n",
            "  Val Loss: 521048726.3922\n",
            "  Val MAE: 15368.53\n",
            "  Val RMSE: 22825.99\n",
            "  Val MAPE: 16365.31%\n",
            "  Val R²: -0.0000\n",
            "Epoch 4/10, Batch 0, Loss: 575965056.0000\n",
            "Epoch 4/10, Batch 50, Loss: 236435104.0000\n",
            "Epoch 4/10, Batch 100, Loss: 332653024.0000\n",
            "Epoch 4/10, Batch 150, Loss: 964080640.0000\n",
            "Epoch 4/10, Batch 200, Loss: 245989312.0000\n",
            "Epoch 4/10, Batch 250, Loss: 194288496.0000\n",
            "Epoch 4/10, Batch 300, Loss: 732414080.0000\n",
            "Epoch 4/10, Batch 350, Loss: 766333440.0000\n",
            "Epoch 4/10, Batch 400, Loss: 1162821248.0000\n",
            "Epoch 4/10, Batch 450, Loss: 455509664.0000\n",
            "Epoch 4/10, Batch 500, Loss: 343254208.0000\n",
            "Epoch 4/10, Batch 550, Loss: 314705760.0000\n",
            "Epoch 4/10, Batch 600, Loss: 1664741376.0000\n",
            "Epoch 4/10, Batch 650, Loss: 380773376.0000\n",
            "Epoch 4/10, Batch 700, Loss: 359905664.0000\n",
            "Epoch 4/10, Batch 750, Loss: 504017600.0000\n",
            "Epoch 4/10, Batch 800, Loss: 498597568.0000\n",
            "Epoch 4/10, Batch 850, Loss: 240144000.0000\n",
            "Epoch 4/10, Batch 900, Loss: 383834304.0000\n",
            "Epoch 4/10, Batch 950, Loss: 411333056.0000\n",
            "Epoch 4/10, Batch 1000, Loss: 692078976.0000\n",
            "Epoch 4/10, Batch 1050, Loss: 432872960.0000\n",
            "Epoch 4/10, Batch 1100, Loss: 675578816.0000\n",
            "Epoch 4/10, Batch 1150, Loss: 383759232.0000\n",
            "Epoch 4/10, Batch 1200, Loss: 337908992.0000\n",
            "Epoch 4/10, Batch 1250, Loss: 235209312.0000\n",
            "Epoch 4/10, Batch 1300, Loss: 593327296.0000\n",
            "Epoch 4/10, Batch 1350, Loss: 234785920.0000\n",
            "Epoch 4/10, Batch 1400, Loss: 701002752.0000\n",
            "Epoch 4/10, Batch 1450, Loss: 467544832.0000\n",
            "Epoch 4/10, Batch 1500, Loss: 666907520.0000\n",
            "Epoch 4/10, Batch 1550, Loss: 229480480.0000\n",
            "Epoch 4/10, Batch 1600, Loss: 535030400.0000\n",
            "Epoch 4/10, Batch 1650, Loss: 352218880.0000\n",
            "Epoch 4/10, Batch 1700, Loss: 418425152.0000\n",
            "Epoch 4/10, Batch 1750, Loss: 1587211776.0000\n",
            "Epoch 4/10, Batch 1800, Loss: 330740416.0000\n",
            "Epoch 4/10, Batch 1850, Loss: 635804032.0000\n",
            "Epoch 4/10, Batch 1900, Loss: 439353152.0000\n",
            "Epoch 4/10, Batch 1950, Loss: 365997824.0000\n",
            "Epoch 4/10, Batch 2000, Loss: 368043136.0000\n",
            "Epoch 4/10, Batch 2050, Loss: 348271040.0000\n",
            "Epoch 4/10, Batch 2100, Loss: 243282784.0000\n",
            "Epoch 4/10, Batch 2150, Loss: 548764864.0000\n",
            "Epoch 4/10, Batch 2200, Loss: 634789568.0000\n",
            "Epoch 4/10, Batch 2250, Loss: 398084704.0000\n",
            "Epoch 4/10, Batch 2300, Loss: 260089760.0000\n",
            "Epoch 4/10, Batch 2350, Loss: 409943872.0000\n",
            "Epoch 4/10, Batch 2400, Loss: 518456704.0000\n",
            "Epoch 4/10, Batch 2450, Loss: 392480000.0000\n",
            "Epoch 4/10, Batch 2500, Loss: 2443328000.0000\n",
            "Epoch 4/10, Batch 2550, Loss: 320204096.0000\n",
            "Epoch 4/10, Batch 2600, Loss: 225784640.0000\n",
            "Epoch 4/10, Batch 2650, Loss: 498144192.0000\n",
            "Epoch 4/10, Batch 2700, Loss: 580245248.0000\n",
            "Epoch 4/10, Batch 2750, Loss: 586937088.0000\n",
            "Epoch 4/10, Batch 2800, Loss: 955754880.0000\n",
            "Epoch 4/10, Batch 2850, Loss: 1011127552.0000\n",
            "Epoch 4/10, Batch 2900, Loss: 444647616.0000\n",
            "Epoch 4/10, Batch 2950, Loss: 818113920.0000\n",
            "Epoch 4/10, Batch 3000, Loss: 269163104.0000\n",
            "Epoch 4/10, Batch 3050, Loss: 549913856.0000\n",
            "Epoch 4/10, Batch 3100, Loss: 280550528.0000\n",
            "Epoch 4/10, Batch 3150, Loss: 293051200.0000\n",
            "Epoch 4/10, Batch 3200, Loss: 330520832.0000\n",
            "Epoch 4/10, Batch 3250, Loss: 422160032.0000\n",
            "Epoch 4/10:\n",
            "  Train Loss: 518221480.9853\n",
            "  Val Loss: 521133396.2941\n",
            "  Val MAE: 15442.38\n",
            "  Val RMSE: 22827.86\n",
            "  Val MAPE: 16558.83%\n",
            "  Val R²: -0.0002\n",
            "Epoch 5/10, Batch 0, Loss: 466666080.0000\n",
            "Epoch 5/10, Batch 50, Loss: 588081664.0000\n",
            "Epoch 5/10, Batch 100, Loss: 593278080.0000\n",
            "Epoch 5/10, Batch 150, Loss: 205621600.0000\n",
            "Epoch 5/10, Batch 200, Loss: 458676288.0000\n",
            "Epoch 5/10, Batch 250, Loss: 296937952.0000\n",
            "Epoch 5/10, Batch 300, Loss: 496698816.0000\n",
            "Epoch 5/10, Batch 350, Loss: 268861056.0000\n",
            "Epoch 5/10, Batch 400, Loss: 1071775104.0000\n",
            "Epoch 5/10, Batch 450, Loss: 225804544.0000\n",
            "Epoch 5/10, Batch 500, Loss: 560784960.0000\n",
            "Epoch 5/10, Batch 550, Loss: 576029312.0000\n",
            "Epoch 5/10, Batch 600, Loss: 434937824.0000\n",
            "Epoch 5/10, Batch 650, Loss: 345419008.0000\n",
            "Epoch 5/10, Batch 700, Loss: 597482048.0000\n",
            "Epoch 5/10, Batch 750, Loss: 719594752.0000\n",
            "Epoch 5/10, Batch 800, Loss: 927009024.0000\n",
            "Epoch 5/10, Batch 850, Loss: 662122560.0000\n",
            "Epoch 5/10, Batch 900, Loss: 569377344.0000\n",
            "Epoch 5/10, Batch 950, Loss: 627110144.0000\n",
            "Epoch 5/10, Batch 1000, Loss: 310160224.0000\n",
            "Epoch 5/10, Batch 1050, Loss: 382230016.0000\n",
            "Epoch 5/10, Batch 1100, Loss: 313036896.0000\n",
            "Epoch 5/10, Batch 1150, Loss: 373678720.0000\n",
            "Epoch 5/10, Batch 1200, Loss: 704116544.0000\n",
            "Epoch 5/10, Batch 1250, Loss: 517168704.0000\n",
            "Epoch 5/10, Batch 1300, Loss: 348452672.0000\n",
            "Epoch 5/10, Batch 1350, Loss: 499264832.0000\n",
            "Epoch 5/10, Batch 1400, Loss: 278579200.0000\n",
            "Epoch 5/10, Batch 1450, Loss: 479972736.0000\n",
            "Epoch 5/10, Batch 1500, Loss: 408462336.0000\n",
            "Epoch 5/10, Batch 1550, Loss: 313839008.0000\n",
            "Epoch 5/10, Batch 1600, Loss: 978949184.0000\n",
            "Epoch 5/10, Batch 1650, Loss: 649329408.0000\n",
            "Epoch 5/10, Batch 1700, Loss: 439583584.0000\n",
            "Epoch 5/10, Batch 1750, Loss: 412142912.0000\n",
            "Epoch 5/10, Batch 1800, Loss: 618011008.0000\n",
            "Epoch 5/10, Batch 1850, Loss: 821216640.0000\n",
            "Epoch 5/10, Batch 1900, Loss: 603837248.0000\n",
            "Epoch 5/10, Batch 1950, Loss: 622312576.0000\n",
            "Epoch 5/10, Batch 2000, Loss: 529774016.0000\n",
            "Epoch 5/10, Batch 2050, Loss: 915550720.0000\n",
            "Epoch 5/10, Batch 2100, Loss: 422232032.0000\n",
            "Epoch 5/10, Batch 2150, Loss: 180899552.0000\n",
            "Epoch 5/10, Batch 2200, Loss: 346607296.0000\n",
            "Epoch 5/10, Batch 2250, Loss: 333724096.0000\n",
            "Epoch 5/10, Batch 2300, Loss: 494699104.0000\n",
            "Epoch 5/10, Batch 2350, Loss: 403651840.0000\n",
            "Epoch 5/10, Batch 2400, Loss: 892075968.0000\n",
            "Epoch 5/10, Batch 2450, Loss: 507303008.0000\n",
            "Epoch 5/10, Batch 2500, Loss: 395798720.0000\n",
            "Epoch 5/10, Batch 2550, Loss: 582267392.0000\n",
            "Epoch 5/10, Batch 2600, Loss: 624370048.0000\n",
            "Epoch 5/10, Batch 2650, Loss: 810152256.0000\n",
            "Epoch 5/10, Batch 2700, Loss: 910961152.0000\n",
            "Epoch 5/10, Batch 2750, Loss: 905575680.0000\n",
            "Epoch 5/10, Batch 2800, Loss: 264088832.0000\n",
            "Epoch 5/10, Batch 2850, Loss: 386616672.0000\n",
            "Epoch 5/10, Batch 2900, Loss: 222088448.0000\n",
            "Epoch 5/10, Batch 2950, Loss: 577208960.0000\n",
            "Epoch 5/10, Batch 3000, Loss: 322688384.0000\n",
            "Epoch 5/10, Batch 3050, Loss: 543101824.0000\n",
            "Epoch 5/10, Batch 3100, Loss: 233577056.0000\n",
            "Epoch 5/10, Batch 3150, Loss: 415469760.0000\n",
            "Epoch 5/10, Batch 3200, Loss: 143340032.0000\n",
            "Epoch 5/10, Batch 3250, Loss: 767779648.0000\n",
            "Epoch 5/10:\n",
            "  Train Loss: 518220733.4412\n",
            "  Val Loss: 521283650.7451\n",
            "  Val MAE: 15513.59\n",
            "  Val RMSE: 22831.15\n",
            "  Val MAPE: 16742.65%\n",
            "  Val R²: -0.0005\n",
            "Epoch 6/10, Batch 0, Loss: 416301760.0000\n",
            "Epoch 6/10, Batch 50, Loss: 324934400.0000\n",
            "Epoch 6/10, Batch 100, Loss: 468664832.0000\n",
            "Epoch 6/10, Batch 150, Loss: 344276672.0000\n",
            "Epoch 6/10, Batch 200, Loss: 665156096.0000\n",
            "Epoch 6/10, Batch 250, Loss: 876285312.0000\n",
            "Epoch 6/10, Batch 300, Loss: 253313568.0000\n",
            "Epoch 6/10, Batch 350, Loss: 791929984.0000\n",
            "Epoch 6/10, Batch 400, Loss: 326740000.0000\n",
            "Epoch 6/10, Batch 450, Loss: 525929792.0000\n",
            "Epoch 6/10, Batch 500, Loss: 348962688.0000\n",
            "Epoch 6/10, Batch 550, Loss: 582484352.0000\n",
            "Epoch 6/10, Batch 600, Loss: 263800288.0000\n",
            "Epoch 6/10, Batch 650, Loss: 259425520.0000\n",
            "Epoch 6/10, Batch 700, Loss: 348970624.0000\n",
            "Epoch 6/10, Batch 750, Loss: 397518720.0000\n",
            "Epoch 6/10, Batch 800, Loss: 133477144.0000\n",
            "Epoch 6/10, Batch 850, Loss: 820961088.0000\n",
            "Epoch 6/10, Batch 900, Loss: 770653632.0000\n",
            "Epoch 6/10, Batch 950, Loss: 500289504.0000\n",
            "Epoch 6/10, Batch 1000, Loss: 312348736.0000\n",
            "Epoch 6/10, Batch 1050, Loss: 261416464.0000\n",
            "Epoch 6/10, Batch 1100, Loss: 695354880.0000\n",
            "Epoch 6/10, Batch 1150, Loss: 243755552.0000\n",
            "Epoch 6/10, Batch 1200, Loss: 273334144.0000\n",
            "Epoch 6/10, Batch 1250, Loss: 256812704.0000\n",
            "Epoch 6/10, Batch 1300, Loss: 334858112.0000\n",
            "Epoch 6/10, Batch 1350, Loss: 584204288.0000\n",
            "Epoch 6/10, Batch 1400, Loss: 418845120.0000\n",
            "Epoch 6/10, Batch 1450, Loss: 300258432.0000\n",
            "Epoch 6/10, Batch 1500, Loss: 432388288.0000\n",
            "Epoch 6/10, Batch 1550, Loss: 329315232.0000\n",
            "Epoch 6/10, Batch 1600, Loss: 296864896.0000\n",
            "Epoch 6/10, Batch 1650, Loss: 763224192.0000\n",
            "Epoch 6/10, Batch 1700, Loss: 405808832.0000\n",
            "Epoch 6/10, Batch 1750, Loss: 896306176.0000\n",
            "Epoch 6/10, Batch 1800, Loss: 1430582016.0000\n",
            "Epoch 6/10, Batch 1850, Loss: 475314912.0000\n",
            "Epoch 6/10, Batch 1900, Loss: 550239232.0000\n",
            "Epoch 6/10, Batch 1950, Loss: 1036001600.0000\n",
            "Epoch 6/10, Batch 2000, Loss: 372104576.0000\n",
            "Epoch 6/10, Batch 2050, Loss: 396323904.0000\n",
            "Epoch 6/10, Batch 2100, Loss: 398314240.0000\n",
            "Epoch 6/10, Batch 2150, Loss: 662036672.0000\n",
            "Epoch 6/10, Batch 2200, Loss: 648508544.0000\n",
            "Epoch 6/10, Batch 2250, Loss: 657132800.0000\n",
            "Epoch 6/10, Batch 2300, Loss: 551355328.0000\n",
            "Epoch 6/10, Batch 2350, Loss: 399030624.0000\n",
            "Epoch 6/10, Batch 2400, Loss: 556740800.0000\n",
            "Epoch 6/10, Batch 2450, Loss: 560687744.0000\n",
            "Epoch 6/10, Batch 2500, Loss: 269891520.0000\n",
            "Epoch 6/10, Batch 2550, Loss: 270522016.0000\n",
            "Epoch 6/10, Batch 2600, Loss: 864008832.0000\n",
            "Epoch 6/10, Batch 2650, Loss: 286350272.0000\n",
            "Epoch 6/10, Batch 2700, Loss: 1361502848.0000\n",
            "Epoch 6/10, Batch 2750, Loss: 281151840.0000\n",
            "Epoch 6/10, Batch 2800, Loss: 446915136.0000\n",
            "Epoch 6/10, Batch 2850, Loss: 672889984.0000\n",
            "Epoch 6/10, Batch 2900, Loss: 662810880.0000\n",
            "Epoch 6/10, Batch 2950, Loss: 389204128.0000\n",
            "Epoch 6/10, Batch 3000, Loss: 697078080.0000\n",
            "Epoch 6/10, Batch 3050, Loss: 471106112.0000\n",
            "Epoch 6/10, Batch 3100, Loss: 566030720.0000\n",
            "Epoch 6/10, Batch 3150, Loss: 539266688.0000\n",
            "Epoch 6/10, Batch 3200, Loss: 458400224.0000\n",
            "Epoch 6/10, Batch 3250, Loss: 431755328.0000\n",
            "Epoch 6/10:\n",
            "  Train Loss: 518189805.6691\n",
            "  Val Loss: 521366556.6667\n",
            "  Val MAE: 15543.67\n",
            "  Val RMSE: 22832.97\n",
            "  Val MAPE: 16819.44%\n",
            "  Val R²: -0.0006\n",
            "Epoch 7/10, Batch 0, Loss: 218023728.0000\n",
            "Epoch 7/10, Batch 50, Loss: 247913024.0000\n",
            "Epoch 7/10, Batch 100, Loss: 482619552.0000\n",
            "Epoch 7/10, Batch 150, Loss: 345239808.0000\n",
            "Epoch 7/10, Batch 200, Loss: 574097856.0000\n",
            "Epoch 7/10, Batch 250, Loss: 332898688.0000\n",
            "Epoch 7/10, Batch 300, Loss: 600364544.0000\n",
            "Epoch 7/10, Batch 350, Loss: 565283392.0000\n",
            "Epoch 7/10, Batch 400, Loss: 797756800.0000\n",
            "Epoch 7/10, Batch 450, Loss: 397716352.0000\n",
            "Epoch 7/10, Batch 500, Loss: 346093824.0000\n",
            "Epoch 7/10, Batch 550, Loss: 644728192.0000\n",
            "Epoch 7/10, Batch 600, Loss: 407538240.0000\n",
            "Epoch 7/10, Batch 650, Loss: 670843136.0000\n",
            "Epoch 7/10, Batch 700, Loss: 280467648.0000\n",
            "Epoch 7/10, Batch 750, Loss: 614598592.0000\n",
            "Epoch 7/10, Batch 800, Loss: 548434368.0000\n",
            "Epoch 7/10, Batch 850, Loss: 484597696.0000\n",
            "Epoch 7/10, Batch 900, Loss: 708552640.0000\n",
            "Epoch 7/10, Batch 950, Loss: 793886080.0000\n",
            "Epoch 7/10, Batch 1000, Loss: 529419968.0000\n",
            "Epoch 7/10, Batch 1050, Loss: 362599360.0000\n",
            "Epoch 7/10, Batch 1100, Loss: 321845760.0000\n",
            "Epoch 7/10, Batch 1150, Loss: 1022092672.0000\n",
            "Epoch 7/10, Batch 1200, Loss: 343468768.0000\n",
            "Epoch 7/10, Batch 1250, Loss: 395701344.0000\n",
            "Epoch 7/10, Batch 1300, Loss: 482594048.0000\n",
            "Epoch 7/10, Batch 1350, Loss: 366585056.0000\n",
            "Epoch 7/10, Batch 1400, Loss: 1225038848.0000\n",
            "Epoch 7/10, Batch 1450, Loss: 330354688.0000\n",
            "Epoch 7/10, Batch 1500, Loss: 428494336.0000\n",
            "Epoch 7/10, Batch 1550, Loss: 483596448.0000\n",
            "Epoch 7/10, Batch 1600, Loss: 175376176.0000\n",
            "Epoch 7/10, Batch 1650, Loss: 342978304.0000\n",
            "Epoch 7/10, Batch 1700, Loss: 547024640.0000\n",
            "Epoch 7/10, Batch 1750, Loss: 260055504.0000\n",
            "Epoch 7/10, Batch 1800, Loss: 319068704.0000\n",
            "Epoch 7/10, Batch 1850, Loss: 701805376.0000\n",
            "Epoch 7/10, Batch 1900, Loss: 399412128.0000\n",
            "Epoch 7/10, Batch 1950, Loss: 777973376.0000\n",
            "Epoch 7/10, Batch 2000, Loss: 836459136.0000\n",
            "Epoch 7/10, Batch 2050, Loss: 279232512.0000\n",
            "Epoch 7/10, Batch 2100, Loss: 505904000.0000\n",
            "Epoch 7/10, Batch 2150, Loss: 535200096.0000\n",
            "Epoch 7/10, Batch 2200, Loss: 357039296.0000\n",
            "Epoch 7/10, Batch 2250, Loss: 532190752.0000\n",
            "Epoch 7/10, Batch 2300, Loss: 641771520.0000\n",
            "Epoch 7/10, Batch 2350, Loss: 925417536.0000\n",
            "Epoch 7/10, Batch 2400, Loss: 432932160.0000\n",
            "Epoch 7/10, Batch 2450, Loss: 1050293120.0000\n",
            "Epoch 7/10, Batch 2500, Loss: 270317632.0000\n",
            "Epoch 7/10, Batch 2550, Loss: 591704192.0000\n",
            "Epoch 7/10, Batch 2600, Loss: 590622528.0000\n",
            "Epoch 7/10, Batch 2650, Loss: 462670816.0000\n",
            "Epoch 7/10, Batch 2700, Loss: 442158848.0000\n",
            "Epoch 7/10, Batch 2750, Loss: 287194560.0000\n",
            "Epoch 7/10, Batch 2800, Loss: 719844416.0000\n",
            "Epoch 7/10, Batch 2850, Loss: 333635552.0000\n",
            "Epoch 7/10, Batch 2900, Loss: 606986240.0000\n",
            "Epoch 7/10, Batch 2950, Loss: 253230368.0000\n",
            "Epoch 7/10, Batch 3000, Loss: 381279936.0000\n",
            "Epoch 7/10, Batch 3050, Loss: 372645408.0000\n",
            "Epoch 7/10, Batch 3100, Loss: 534347200.0000\n",
            "Epoch 7/10, Batch 3150, Loss: 361704960.0000\n",
            "Epoch 7/10, Batch 3200, Loss: 425133280.0000\n",
            "Epoch 7/10, Batch 3250, Loss: 599673216.0000\n",
            "Epoch 7/10:\n",
            "  Train Loss: 518220481.2059\n",
            "  Val Loss: 521269792.0588\n",
            "  Val MAE: 15508.13\n",
            "  Val RMSE: 22830.85\n",
            "  Val MAPE: 16728.64%\n",
            "  Val R²: -0.0005\n",
            "Epoch 8/10, Batch 0, Loss: 757344576.0000\n",
            "Epoch 8/10, Batch 50, Loss: 305793536.0000\n",
            "Epoch 8/10, Batch 100, Loss: 648823936.0000\n",
            "Epoch 8/10, Batch 150, Loss: 498913696.0000\n",
            "Epoch 8/10, Batch 200, Loss: 473356704.0000\n",
            "Epoch 8/10, Batch 250, Loss: 414321984.0000\n",
            "Epoch 8/10, Batch 300, Loss: 700655104.0000\n",
            "Epoch 8/10, Batch 350, Loss: 320787392.0000\n",
            "Epoch 8/10, Batch 400, Loss: 711052416.0000\n",
            "Epoch 8/10, Batch 450, Loss: 438391552.0000\n",
            "Epoch 8/10, Batch 500, Loss: 394232000.0000\n",
            "Epoch 8/10, Batch 550, Loss: 827315584.0000\n",
            "Epoch 8/10, Batch 600, Loss: 992833984.0000\n",
            "Epoch 8/10, Batch 650, Loss: 899020928.0000\n",
            "Epoch 8/10, Batch 700, Loss: 434807648.0000\n",
            "Epoch 8/10, Batch 750, Loss: 254172304.0000\n",
            "Epoch 8/10, Batch 800, Loss: 491269440.0000\n",
            "Epoch 8/10, Batch 850, Loss: 909040064.0000\n",
            "Epoch 8/10, Batch 900, Loss: 1531965440.0000\n",
            "Epoch 8/10, Batch 950, Loss: 400466496.0000\n",
            "Epoch 8/10, Batch 1000, Loss: 270677152.0000\n",
            "Epoch 8/10, Batch 1050, Loss: 383254592.0000\n",
            "Epoch 8/10, Batch 1100, Loss: 396263040.0000\n",
            "Epoch 8/10, Batch 1150, Loss: 524920448.0000\n",
            "Epoch 8/10, Batch 1200, Loss: 273457600.0000\n",
            "Epoch 8/10, Batch 1250, Loss: 409959296.0000\n",
            "Epoch 8/10, Batch 1300, Loss: 604575040.0000\n",
            "Epoch 8/10, Batch 1350, Loss: 239725632.0000\n",
            "Epoch 8/10, Batch 1400, Loss: 226885248.0000\n",
            "Epoch 8/10, Batch 1450, Loss: 560903488.0000\n",
            "Epoch 8/10, Batch 1500, Loss: 328710208.0000\n",
            "Epoch 8/10, Batch 1550, Loss: 349689664.0000\n",
            "Epoch 8/10, Batch 1600, Loss: 417663744.0000\n",
            "Epoch 8/10, Batch 1650, Loss: 381443008.0000\n",
            "Epoch 8/10, Batch 1700, Loss: 456186848.0000\n",
            "Epoch 8/10, Batch 1750, Loss: 259478656.0000\n",
            "Epoch 8/10, Batch 1800, Loss: 316747840.0000\n",
            "Epoch 8/10, Batch 1850, Loss: 347397984.0000\n",
            "Epoch 8/10, Batch 1900, Loss: 333539456.0000\n",
            "Epoch 8/10, Batch 1950, Loss: 270563904.0000\n",
            "Epoch 8/10, Batch 2000, Loss: 326902400.0000\n",
            "Epoch 8/10, Batch 2050, Loss: 529433120.0000\n",
            "Epoch 8/10, Batch 2100, Loss: 214432848.0000\n",
            "Epoch 8/10, Batch 2150, Loss: 227572992.0000\n",
            "Epoch 8/10, Batch 2200, Loss: 869618112.0000\n",
            "Epoch 8/10, Batch 2250, Loss: 215570592.0000\n",
            "Epoch 8/10, Batch 2300, Loss: 416022912.0000\n",
            "Epoch 8/10, Batch 2350, Loss: 790740480.0000\n",
            "Epoch 8/10, Batch 2400, Loss: 317141888.0000\n",
            "Epoch 8/10, Batch 2450, Loss: 786822528.0000\n",
            "Epoch 8/10, Batch 2500, Loss: 1110711168.0000\n",
            "Epoch 8/10, Batch 2550, Loss: 234632032.0000\n",
            "Epoch 8/10, Batch 2600, Loss: 696396608.0000\n",
            "Epoch 8/10, Batch 2650, Loss: 954142784.0000\n",
            "Epoch 8/10, Batch 2700, Loss: 385101760.0000\n",
            "Epoch 8/10, Batch 2750, Loss: 450455200.0000\n",
            "Epoch 8/10, Batch 2800, Loss: 500625280.0000\n",
            "Epoch 8/10, Batch 2850, Loss: 229057760.0000\n",
            "Epoch 8/10, Batch 2900, Loss: 881231360.0000\n",
            "Epoch 8/10, Batch 2950, Loss: 694635072.0000\n",
            "Epoch 8/10, Batch 3000, Loss: 366156480.0000\n",
            "Epoch 8/10, Batch 3050, Loss: 815955072.0000\n",
            "Epoch 8/10, Batch 3100, Loss: 476465600.0000\n",
            "Epoch 8/10, Batch 3150, Loss: 862127872.0000\n",
            "Epoch 8/10, Batch 3200, Loss: 534074304.0000\n",
            "Epoch 8/10, Batch 3250, Loss: 343844672.0000\n",
            "Epoch 8/10:\n",
            "  Train Loss: 518244156.7770\n",
            "  Val Loss: 521134960.9020\n",
            "  Val MAE: 15443.33\n",
            "  Val RMSE: 22827.89\n",
            "  Val MAPE: 16561.31%\n",
            "  Val R²: -0.0002\n",
            "Epoch 9/10, Batch 0, Loss: 342239296.0000\n",
            "Epoch 9/10, Batch 50, Loss: 643279744.0000\n",
            "Epoch 9/10, Batch 100, Loss: 229547600.0000\n",
            "Epoch 9/10, Batch 150, Loss: 565080832.0000\n",
            "Epoch 9/10, Batch 200, Loss: 938728192.0000\n",
            "Epoch 9/10, Batch 250, Loss: 228116320.0000\n",
            "Epoch 9/10, Batch 300, Loss: 1038223296.0000\n",
            "Epoch 9/10, Batch 350, Loss: 190573536.0000\n",
            "Epoch 9/10, Batch 400, Loss: 276430880.0000\n",
            "Epoch 9/10, Batch 450, Loss: 548155520.0000\n",
            "Epoch 9/10, Batch 500, Loss: 390712960.0000\n",
            "Epoch 9/10, Batch 550, Loss: 640179136.0000\n",
            "Epoch 9/10, Batch 600, Loss: 397722400.0000\n",
            "Epoch 9/10, Batch 650, Loss: 638423744.0000\n",
            "Epoch 9/10, Batch 700, Loss: 285432960.0000\n",
            "Epoch 9/10, Batch 750, Loss: 326406720.0000\n",
            "Epoch 9/10, Batch 800, Loss: 529569376.0000\n",
            "Epoch 9/10, Batch 850, Loss: 423753280.0000\n",
            "Epoch 9/10, Batch 900, Loss: 555806912.0000\n",
            "Epoch 9/10, Batch 950, Loss: 837476672.0000\n",
            "Epoch 9/10, Batch 1000, Loss: 345043264.0000\n",
            "Epoch 9/10, Batch 1050, Loss: 500962112.0000\n",
            "Epoch 9/10, Batch 1100, Loss: 348226816.0000\n",
            "Epoch 9/10, Batch 1150, Loss: 802932544.0000\n",
            "Epoch 9/10, Batch 1200, Loss: 237259296.0000\n",
            "Epoch 9/10, Batch 1250, Loss: 777963776.0000\n",
            "Epoch 9/10, Batch 1300, Loss: 301932736.0000\n",
            "Epoch 9/10, Batch 1350, Loss: 391749568.0000\n",
            "Epoch 9/10, Batch 1400, Loss: 1157027328.0000\n",
            "Epoch 9/10, Batch 1450, Loss: 399044128.0000\n",
            "Epoch 9/10, Batch 1500, Loss: 594548672.0000\n",
            "Epoch 9/10, Batch 1550, Loss: 551322752.0000\n",
            "Epoch 9/10, Batch 1600, Loss: 664205824.0000\n",
            "Epoch 9/10, Batch 1650, Loss: 282859040.0000\n",
            "Epoch 9/10, Batch 1700, Loss: 272590976.0000\n",
            "Epoch 9/10, Batch 1750, Loss: 371840640.0000\n",
            "Epoch 9/10, Batch 1800, Loss: 581150208.0000\n",
            "Epoch 9/10, Batch 1850, Loss: 433883168.0000\n",
            "Epoch 9/10, Batch 1900, Loss: 624188736.0000\n",
            "Epoch 9/10, Batch 1950, Loss: 417354368.0000\n",
            "Epoch 9/10, Batch 2000, Loss: 335024960.0000\n",
            "Epoch 9/10, Batch 2050, Loss: 738472256.0000\n",
            "Epoch 9/10, Batch 2100, Loss: 470334272.0000\n",
            "Epoch 9/10, Batch 2150, Loss: 211390144.0000\n",
            "Epoch 9/10, Batch 2200, Loss: 882742272.0000\n",
            "Epoch 9/10, Batch 2250, Loss: 309752256.0000\n",
            "Epoch 9/10, Batch 2300, Loss: 304968544.0000\n",
            "Epoch 9/10, Batch 2350, Loss: 335396672.0000\n",
            "Epoch 9/10, Batch 2400, Loss: 385843744.0000\n",
            "Epoch 9/10, Batch 2450, Loss: 326855616.0000\n",
            "Epoch 9/10, Batch 2500, Loss: 445217472.0000\n",
            "Epoch 9/10, Batch 2550, Loss: 363788224.0000\n",
            "Epoch 9/10, Batch 2600, Loss: 521246048.0000\n",
            "Epoch 9/10, Batch 2650, Loss: 788104128.0000\n",
            "Epoch 9/10, Batch 2700, Loss: 1524330496.0000\n",
            "Epoch 9/10, Batch 2750, Loss: 408200256.0000\n",
            "Epoch 9/10, Batch 2800, Loss: 579817344.0000\n",
            "Epoch 9/10, Batch 2850, Loss: 318739456.0000\n",
            "Epoch 9/10, Batch 2900, Loss: 963700672.0000\n",
            "Epoch 9/10, Batch 2950, Loss: 556774784.0000\n",
            "Epoch 9/10, Batch 3000, Loss: 812134848.0000\n",
            "Epoch 9/10, Batch 3050, Loss: 466287104.0000\n",
            "Epoch 9/10, Batch 3100, Loss: 601763904.0000\n",
            "Epoch 9/10, Batch 3150, Loss: 868649600.0000\n",
            "Epoch 9/10, Batch 3200, Loss: 679393664.0000\n",
            "Epoch 9/10, Batch 3250, Loss: 744258624.0000\n",
            "Epoch 9/10:\n",
            "  Train Loss: 518196951.5490\n",
            "  Val Loss: 521420873.5490\n",
            "  Val MAE: 15561.46\n",
            "  Val RMSE: 22834.16\n",
            "  Val MAPE: 16864.60%\n",
            "  Val R²: -0.0007\n",
            "Epoch 10/10, Batch 0, Loss: 526031552.0000\n",
            "Epoch 10/10, Batch 50, Loss: 1058990272.0000\n",
            "Epoch 10/10, Batch 100, Loss: 311517024.0000\n",
            "Epoch 10/10, Batch 150, Loss: 394958496.0000\n",
            "Epoch 10/10, Batch 200, Loss: 642332416.0000\n",
            "Epoch 10/10, Batch 250, Loss: 437965888.0000\n",
            "Epoch 10/10, Batch 300, Loss: 735559808.0000\n",
            "Epoch 10/10, Batch 350, Loss: 886071808.0000\n",
            "Epoch 10/10, Batch 400, Loss: 226837408.0000\n",
            "Epoch 10/10, Batch 450, Loss: 470555072.0000\n",
            "Epoch 10/10, Batch 500, Loss: 1096003072.0000\n",
            "Epoch 10/10, Batch 550, Loss: 344932608.0000\n",
            "Epoch 10/10, Batch 600, Loss: 771793856.0000\n",
            "Epoch 10/10, Batch 650, Loss: 354918176.0000\n",
            "Epoch 10/10, Batch 700, Loss: 469737344.0000\n",
            "Epoch 10/10, Batch 750, Loss: 719718720.0000\n",
            "Epoch 10/10, Batch 800, Loss: 1004010496.0000\n",
            "Epoch 10/10, Batch 850, Loss: 278936768.0000\n",
            "Epoch 10/10, Batch 900, Loss: 414587776.0000\n",
            "Epoch 10/10, Batch 950, Loss: 413064736.0000\n",
            "Epoch 10/10, Batch 1000, Loss: 348313792.0000\n",
            "Epoch 10/10, Batch 1050, Loss: 517579520.0000\n",
            "Epoch 10/10, Batch 1100, Loss: 803519552.0000\n",
            "Epoch 10/10, Batch 1150, Loss: 448034944.0000\n",
            "Epoch 10/10, Batch 1200, Loss: 330570816.0000\n",
            "Epoch 10/10, Batch 1250, Loss: 411529024.0000\n",
            "Epoch 10/10, Batch 1300, Loss: 813811392.0000\n",
            "Epoch 10/10, Batch 1350, Loss: 759873024.0000\n",
            "Epoch 10/10, Batch 1400, Loss: 581151232.0000\n",
            "Epoch 10/10, Batch 1450, Loss: 603397504.0000\n",
            "Epoch 10/10, Batch 1500, Loss: 414292416.0000\n",
            "Epoch 10/10, Batch 1550, Loss: 694388352.0000\n",
            "Epoch 10/10, Batch 1600, Loss: 1711660544.0000\n",
            "Epoch 10/10, Batch 1650, Loss: 230481552.0000\n",
            "Epoch 10/10, Batch 1700, Loss: 276432000.0000\n",
            "Epoch 10/10, Batch 1750, Loss: 535673952.0000\n",
            "Epoch 10/10, Batch 1800, Loss: 987195008.0000\n",
            "Epoch 10/10, Batch 1850, Loss: 621287168.0000\n",
            "Epoch 10/10, Batch 1900, Loss: 350668800.0000\n",
            "Epoch 10/10, Batch 1950, Loss: 844642048.0000\n",
            "Epoch 10/10, Batch 2000, Loss: 378354944.0000\n",
            "Epoch 10/10, Batch 2050, Loss: 382597024.0000\n",
            "Epoch 10/10, Batch 2100, Loss: 824372224.0000\n",
            "Epoch 10/10, Batch 2150, Loss: 528728576.0000\n",
            "Epoch 10/10, Batch 2200, Loss: 253415792.0000\n",
            "Epoch 10/10, Batch 2250, Loss: 770836224.0000\n",
            "Epoch 10/10, Batch 2300, Loss: 220062112.0000\n",
            "Epoch 10/10, Batch 2350, Loss: 570638656.0000\n",
            "Epoch 10/10, Batch 2400, Loss: 274852960.0000\n",
            "Epoch 10/10, Batch 2450, Loss: 306560992.0000\n",
            "Epoch 10/10, Batch 2500, Loss: 340506720.0000\n",
            "Epoch 10/10, Batch 2550, Loss: 954677376.0000\n",
            "Epoch 10/10, Batch 2600, Loss: 424088832.0000\n",
            "Epoch 10/10, Batch 2650, Loss: 1431192064.0000\n",
            "Epoch 10/10, Batch 2700, Loss: 1189200384.0000\n",
            "Epoch 10/10, Batch 2750, Loss: 771487232.0000\n",
            "Epoch 10/10, Batch 2800, Loss: 528553024.0000\n",
            "Epoch 10/10, Batch 2850, Loss: 1065746816.0000\n",
            "Epoch 10/10, Batch 2900, Loss: 251427936.0000\n",
            "Epoch 10/10, Batch 2950, Loss: 297811232.0000\n",
            "Epoch 10/10, Batch 3000, Loss: 447499104.0000\n",
            "Epoch 10/10, Batch 3050, Loss: 359053664.0000\n",
            "Epoch 10/10, Batch 3100, Loss: 494371008.0000\n",
            "Epoch 10/10, Batch 3150, Loss: 603636928.0000\n",
            "Epoch 10/10, Batch 3200, Loss: 569555840.0000\n",
            "Epoch 10/10, Batch 3250, Loss: 446845120.0000\n",
            "Epoch 10/10:\n",
            "  Train Loss: 518270558.3113\n",
            "  Val Loss: 521274203.4902\n",
            "  Val MAE: 15509.88\n",
            "  Val RMSE: 22830.94\n",
            "  Val MAPE: 16733.14%\n",
            "  Val R²: -0.0005\n",
            "\n",
            "=== FINAL EVALUATION ===\n",
            "\n",
            "Final Validation Metrics:\n",
            "MAE: 15509.88\n",
            "RMSE: 22830.94\n",
            "MAPE: 16733.14%\n",
            "R²: -0.0005\n",
            "\n",
            "=== SAVING MODEL ===\n",
            "Pipeline saved as: patchtst_pipeline_20250706_183256.pkl\n",
            "✓ Model logged to wandb\n",
            "\n",
            "=== EXPERIMENT COMPLETED ===\n",
            "\n",
            "🎉 PatchTST experiment completed successfully!\n",
            "Final metrics: {'mae': 15509.884402274905, 'rmse': np.float64(22830.944097328524), 'mape': np.float32(16733.145), 'r2': -0.0004620768467100689}\n",
            "Model saved as: patchtst_pipeline_20250706_183256.pkl\n"
          ]
        }
      ],
      "source": [
        "# Run the complete PatchTST experiment\n",
        "results = run_patchtst_experiment()\n",
        "\n",
        "if results:\n",
        "    print(f\"\\n🎉 PatchTST experiment completed successfully!\")\n",
        "    print(f\"Final metrics: {results['metrics']}\")\n",
        "    print(f\"Model saved as: {results['filename']}\")\n",
        "else:\n",
        "    print(\"❌ Experiment failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_pipeline"
      },
      "source": [
        "# Test Pipeline on Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_pipeline_code",
        "outputId": "e7896d5e-7a46-4fa7-f7a9-bf899aaeb93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TESTING PIPELINE ===\n",
            "Creating patch-based sequences for 3331 store-dept combinations...\n",
            "Created 0 patch-based sequences\n",
            "✓ Pipeline test successful!\n",
            "Test sample size: 1000\n",
            "Predictions generated: 0\n"
          ]
        }
      ],
      "source": [
        "# Test the pipeline on sample test data\n",
        "if results and 'pipeline' in results:\n",
        "    print(\"\\n=== TESTING PIPELINE ===\")\n",
        "\n",
        "    # Create a small sample from test data for testing\n",
        "    test_sample = test_df.head(1000).copy()\n",
        "\n",
        "    try:\n",
        "        # Test pipeline prediction\n",
        "        predictions = results['pipeline'].predict(\n",
        "            test_sample,\n",
        "            stores_df=stores_df,\n",
        "            features_df=features_df\n",
        "        )\n",
        "\n",
        "        print(f\"✓ Pipeline test successful!\")\n",
        "        print(f\"Test sample size: {len(test_sample)}\")\n",
        "        print(f\"Predictions generated: {len(predictions)}\")\n",
        "\n",
        "        if len(predictions) > 0:\n",
        "            print(f\"Sample predictions: {predictions[:5]}\")\n",
        "            print(f\"Prediction statistics:\")\n",
        "            print(f\"  Mean: {np.mean(predictions):.2f}\")\n",
        "            print(f\"  Std: {np.std(predictions):.2f}\")\n",
        "            print(f\"  Min: {np.min(predictions):.2f}\")\n",
        "            print(f\"  Max: {np.max(predictions):.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Pipeline test failed: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ No pipeline available for testing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "# Experiment Conclusion and Model Registry\n",
        "\n",
        "This notebook implements PatchTST for Walmart sales forecasting with:\n",
        "\n",
        "## Architecture Features:\n",
        "- **Patch-based processing**: 13-week patches (quarterly) from 52-week lookback\n",
        "- **Channel independence**: Separate processing for each feature\n",
        "- **Transformer encoder**: Multi-head attention with positional encoding\n",
        "- **Optimized for time series**: Designed for sequential forecasting\n",
        "\n",
        "## Pipeline Components:\n",
        "1. **WalmartFeatureMerger**: Merges train/test with stores and features\n",
        "2. **WalmartMissingValueHandler**: Forward-fill and median imputation\n",
        "3. **PatchTimeSeriesDataProcessor**: Creates patch-based sequences\n",
        "4. **PatchTSTModel**: Full transformer architecture\n",
        "\n",
        "## Experiments Conducted:\n",
        "- **PatchTST_Initial_Setup**: Data exploration and setup\n",
        "- **PatchTST_Training**: Main model training\n",
        "- **PatchTST_CrossValidation**: Time series cross-validation\n",
        "\n",
        "## Model Registry:\n",
        "The best performing model is saved as a complete pipeline that can be applied directly to raw test data without preprocessing. Use the saved pipeline for production inference."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}