{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrymp/Final-Project-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_TFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install pytorch-forecasting pytorch-lightning -q\n",
        "!pip install optuna scikit-learn -q\n",
        "!pip install kaggle wandb onnx dill -Uq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyNRP9zXBKpN",
        "outputId": "118f0e82-1504-45be-cbf0-499d30a21433"
      },
      "id": "ZyNRP9zXBKpN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.7.1%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cpu/torch-2.7.1%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl (176.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.13.3 torch-2.7.1+cpu torchaudio-2.7.1+cpu torchvision-0.22.1+cpu\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoK4vXu5BMgC",
        "outputId": "6c2ae4b4-8808-43d4-c1ba-18989e125f96"
      },
      "id": "qoK4vXu5BMgC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "! unzip /content/train.csv.zip\n",
        "! unzip /content/test.csv.zip\n",
        "! unzip /content/features.csv.zip\n",
        "! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOaS3NYPBQ0X",
        "outputId": "de38e5d0-810c-4eea-eccc-72b76024906e"
      },
      "id": "ZOaS3NYPBQ0X",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 709MB/s]\n",
            "Archive:  /content/walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n",
            "Archive:  /content/train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  /content/test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  /content/features.csv.zip\n",
            "  inflating: features.csv            \n",
            "Archive:  /content/sampleSubmission.csv.zip\n",
            "  inflating: sampleSubmission.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import dill\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "\n",
        "# Test PyTorch installation\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# TFT specific imports\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# WandB setup\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"TFT_TimeSeries_CPU\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "iOhF4OdxBTB8",
        "outputId": "97e586f4-b265-4441-c96a-fdc4fa827e75"
      },
      "id": "iOhF4OdxBTB8",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.7.1+cpu\n",
            "CUDA available: False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdshan21\u001b[0m (\u001b[33mdshan21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250802_214014-1q9cvs9r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/1q9cvs9r' target=\"_blank\">TFT_TimeSeries_CPU</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/1q9cvs9r' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/1q9cvs9r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 1: Data Loading and Initial Setup\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "features_df = pd.read_csv(\"/content/features.csv\")\n",
        "stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "\n",
        "# Convert dates\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "print(f\"Data loaded: Train {train_df.shape}, Test {test_df.shape}\")\n",
        "print(f\"Train columns: {list(train_df.columns)}\")\n",
        "print(f\"Features columns: {list(features_df.columns)}\")\n",
        "print(f\"Date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "\n",
        "# Log basic info\n",
        "wandb.log({\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"n_stores\": train_df['Store'].nunique(),\n",
        "    \"n_departments\": train_df['Dept'].nunique(),\n",
        "    \"date_range_days\": (train_df['Date'].max() - train_df['Date'].min()).days\n",
        "})\n",
        "\n",
        "print(\"Initial data check completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGyZZf2EBTfF",
        "outputId": "59affc45-0ee5-4636-e449-7fc2818b0460"
      },
      "id": "wGyZZf2EBTfF",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Data loaded: Train (421570, 5), Test (115064, 4)\n",
            "Train columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']\n",
            "Features columns: ['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday']\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Initial data check completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FIXED Block 1: ROBUST ENHANCED FEATURE ENGINEERING (Fixed Categorical Types)\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedWalmartFeatureEngineer:\n",
        "    def __init__(self):\n",
        "        self.dept_stats = {}\n",
        "        self.store_stats = {}\n",
        "        self.global_stats = {}\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, train_df):\n",
        "        \"\"\"Learn patterns from training data\"\"\"\n",
        "        print(\"Learning advanced retail patterns...\")\n",
        "\n",
        "        # Department statistics\n",
        "        self.dept_stats = train_df.groupby('Dept').agg({\n",
        "            'Weekly_Sales': ['mean', 'std', 'median', 'min', 'max']\n",
        "        }).round(2)\n",
        "        self.dept_stats.columns = ['_'.join(col).strip() for col in self.dept_stats.columns]\n",
        "\n",
        "        # Store statistics\n",
        "        self.store_stats = train_df.groupby('Store').agg({\n",
        "            'Weekly_Sales': ['mean', 'std', 'median']\n",
        "        }).round(2)\n",
        "        self.store_stats.columns = ['_'.join(col).strip() for col in self.store_stats.columns]\n",
        "\n",
        "        # Global statistics\n",
        "        self.global_stats = {\n",
        "            'sales_mean': train_df['Weekly_Sales'].mean(),\n",
        "            'sales_std': train_df['Weekly_Sales'].std(),\n",
        "            'sales_median': train_df['Weekly_Sales'].median()\n",
        "        }\n",
        "\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        \"\"\"Transform data with comprehensive feature engineering\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        print(f\"Starting transform with shape: {df.shape}\")\n",
        "\n",
        "        # CRITICAL FIX: Convert Store and Dept to strings FIRST\n",
        "        df['Store'] = df['Store'].astype(str)\n",
        "        df['Dept'] = df['Dept'].astype(str)\n",
        "\n",
        "        # CRITICAL FIX: Add DayOfWeek and other missing temporal features\n",
        "        if 'Date' in df.columns:\n",
        "            df['DayOfWeek'] = df['Date'].dt.dayofweek.astype(str)\n",
        "            df['DayOfMonth'] = df['Date'].dt.day\n",
        "            df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "            df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "        # Basic temporal features - convert to strings for categoricals\n",
        "        df['Year'] = df['Date'].dt.year.astype(str)\n",
        "        df['Month'] = df['Date'].dt.month.astype(str)\n",
        "        df['Quarter'] = df['Date'].dt.quarter.astype(str)\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week.astype(str)\n",
        "\n",
        "        # Create time_idx for TFT (critical!)\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "        df['time_idx'] = df.groupby(['Store', 'Dept']).cumcount()\n",
        "\n",
        "        # Create group_id for TFT\n",
        "        df['group_id'] = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n",
        "\n",
        "        # ROBUST EXTERNAL DATA LOADING\n",
        "        try:\n",
        "            features_df = pd.read_csv(\"/content/features.csv\")\n",
        "            stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "\n",
        "            # Convert dates in features\n",
        "            features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "            # CRITICAL: Convert Store to string in external data too\n",
        "            features_df['Store'] = features_df['Store'].astype(str)\n",
        "            stores_df['Store'] = stores_df['Store'].astype(str)\n",
        "\n",
        "            # Merge with features and stores\n",
        "            df = df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "            df = df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading external data: {e}\")\n",
        "            # Create minimal required columns if files don't exist\n",
        "            df['Temperature'] = 70.0\n",
        "            df['Fuel_Price'] = 3.5\n",
        "            df['CPI'] = 200.0\n",
        "            df['Unemployment'] = 7.0\n",
        "            df['IsHoliday'] = 0\n",
        "            df['Type'] = 'A'\n",
        "            df['Size'] = 150000\n",
        "            for i in range(1, 6):\n",
        "                df[f'MarkDown{i}'] = 0.0\n",
        "\n",
        "        # Fill missing values intelligently\n",
        "        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "            else:\n",
        "                if col == 'Temperature':\n",
        "                    df[col] = 70.0\n",
        "                elif col == 'Fuel_Price':\n",
        "                    df[col] = 3.5\n",
        "                elif col == 'CPI':\n",
        "                    df[col] = 200.0\n",
        "                elif col == 'Unemployment':\n",
        "                    df[col] = 7.0\n",
        "\n",
        "        # ROBUST Holiday handling\n",
        "        if 'IsHoliday' not in df.columns:\n",
        "            df['IsHoliday'] = 0\n",
        "        else:\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(0).astype(int)\n",
        "\n",
        "        # ROBUST Markdown features\n",
        "        markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        for col in markdown_cols:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 0.0\n",
        "            else:\n",
        "                df[col] = df[col].fillna(0)\n",
        "            df[f'Has{col}'] = (df[col] > 0).astype(int)\n",
        "\n",
        "        # Promotional features\n",
        "        df['TotalMarkDown'] = df[markdown_cols].sum(axis=1)\n",
        "        df['HasAnyPromo'] = (df['TotalMarkDown'] > 0).astype(int)\n",
        "        df['PromoIntensity'] = df['TotalMarkDown'] / (df['TotalMarkDown'].quantile(0.95) + 1)\n",
        "        df['PromoIntensity'] = df['PromoIntensity'].clip(0, 1)\n",
        "\n",
        "        # ROBUST Store and Type handling\n",
        "        if 'Type' not in df.columns:\n",
        "            df['Type'] = 'A'\n",
        "        if 'Size' not in df.columns:\n",
        "            df['Size'] = 150000\n",
        "\n",
        "        df['Type'] = df['Type'].fillna('A').astype(str)\n",
        "        df['Size'] = df['Size'].fillna(df['Size'].median())\n",
        "\n",
        "        # Seasonal features\n",
        "        df['IsQ4'] = (df['Quarter'].astype(int) == 4).astype(int)\n",
        "        df['IsQ1'] = (df['Quarter'].astype(int) == 1).astype(int)\n",
        "        df['IsBackToSchool'] = ((df['Month'].astype(int) == 8) | (df['Month'].astype(int) == 9)).astype(int)\n",
        "\n",
        "        # Weather categories\n",
        "        if 'Temperature' in df.columns:\n",
        "            df['TempCategory'] = pd.cut(df['Temperature'],\n",
        "                                      bins=[-np.inf, 32, 50, 70, 85, np.inf],\n",
        "                                      labels=['Freezing', 'Cold', 'Cool', 'Warm', 'Hot']).astype(str)\n",
        "        else:\n",
        "            df['TempCategory'] = 'Cool'\n",
        "\n",
        "        # Store categorization\n",
        "        if 'Size' in df.columns:\n",
        "            df['StoreSize_Cat'] = pd.cut(df['Size'],\n",
        "                                       bins=[0, 50000, 100000, 150000, 200000, np.inf],\n",
        "                                       labels=['XS', 'S', 'M', 'L', 'XL']).astype(str)\n",
        "        else:\n",
        "            df['StoreSize_Cat'] = 'M'\n",
        "\n",
        "        # Department categorization\n",
        "        high_volume_depts = [1, 2, 3, 7, 8, 13, 16, 20, 24, 27, 40, 46, 50, 57, 79, 81]\n",
        "        volatile_depts = [5, 6, 9, 12, 14, 18, 21, 25, 28, 34, 39, 47, 48, 54, 56, 60, 67, 77, 80, 86, 87, 91, 92, 95]\n",
        "        seasonal_depts = [11, 15, 23, 29, 33, 35, 41, 45, 65, 68, 74, 78, 96, 97, 98, 99]\n",
        "\n",
        "        df['DeptCategory'] = 'Standard'\n",
        "        df.loc[df['Dept'].astype(int).isin(high_volume_depts), 'DeptCategory'] = 'High_Volume'\n",
        "        df.loc[df['Dept'].astype(int).isin(volatile_depts), 'DeptCategory'] = 'Volatile'\n",
        "        df.loc[df['Dept'].astype(int).isin(seasonal_depts), 'DeptCategory'] = 'Seasonal'\n",
        "\n",
        "        # Cyclical encoding for temporal features\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'].astype(int) / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'].astype(int) / 12)\n",
        "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'].astype(int) / 52)\n",
        "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'].astype(int) / 52)\n",
        "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'].astype(int) / 7)\n",
        "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'].astype(int) / 7)\n",
        "\n",
        "        # Store-Dept interaction features\n",
        "        if self.is_fitted:\n",
        "            dept_means = self.dept_stats.get('Weekly_Sales_mean', {})\n",
        "            store_means = self.store_stats.get('Weekly_Sales_mean', {})\n",
        "\n",
        "            df['Dept_HistoricalMean'] = df['Dept'].map(dept_means).fillna(self.global_stats['sales_mean'])\n",
        "            df['Store_HistoricalMean'] = df['Store'].map(store_means).fillna(self.global_stats['sales_mean'])\n",
        "\n",
        "        # CRITICAL: Ensure all categorical columns are STRING type for TFT\n",
        "        categorical_string_cols = ['Store', 'Dept', 'Type', 'StoreSize_Cat', 'DeptCategory', 'TempCategory',\n",
        "                                 'Month', 'Quarter', 'Week', 'DayOfWeek']\n",
        "        for col in categorical_string_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "        print(f\"Enhanced features created. Final shape: {df.shape}\")\n",
        "        return df"
      ],
      "metadata": {
        "id": "UvJcvfjPBUsY"
      },
      "id": "UvJcvfjPBUsY",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 2: FAST OPTIMIZED LAG FEATURES - 10x Faster\n",
        "# =============================================================================\n",
        "\n",
        "def create_fast_lag_features(df, include_target_lags=True):\n",
        "    \"\"\"Super fast vectorized lag feature creation\"\"\"\n",
        "    print(\"Creating FAST lag features...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['group_id', 'Date']).reset_index(drop=True)\n",
        "\n",
        "    # Only create lag features if we have the target variable\n",
        "    if include_target_lags and 'Weekly_Sales' in df.columns:\n",
        "        print(\"Creating target-based lag features...\")\n",
        "\n",
        "        # VECTORIZED LAG CREATION - Much faster than loops\n",
        "        lag_windows = [1, 2, 4, 8, 12]  # Reduced for speed\n",
        "        rolling_windows = [4, 8, 12]    # Reduced for speed\n",
        "\n",
        "        # Create lag features using groupby.shift (vectorized)\n",
        "        for lag in lag_windows:\n",
        "            df[f'sales_lag_{lag}'] = df.groupby('group_id')['Weekly_Sales'].shift(lag)\n",
        "\n",
        "        # Create rolling features using groupby.rolling (vectorized)\n",
        "        for window in rolling_windows:\n",
        "            df[f'sales_rolling_mean_{window}'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window, min_periods=1).mean()\n",
        "            )\n",
        "            df[f'sales_rolling_std_{window}'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window, min_periods=1).std()\n",
        "            )\n",
        "\n",
        "        # Simple trend features (vectorized)\n",
        "        df['sales_trend_4w'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "            lambda x: x.pct_change(periods=4).fillna(0).clip(-1, 1)\n",
        "        )\n",
        "\n",
        "        # Fill NaN values with intelligent defaults\n",
        "        dept_medians = df.groupby('DeptCategory')['Weekly_Sales'].median().to_dict()\n",
        "        global_median = df['Weekly_Sales'].median()\n",
        "\n",
        "        # Fast NaN filling\n",
        "        for lag in lag_windows:\n",
        "            col = f'sales_lag_{lag}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, 'DeptCategory'].map(dept_medians).fillna(global_median)\n",
        "\n",
        "        for window in rolling_windows:\n",
        "            # Fill rolling mean\n",
        "            col = f'sales_rolling_mean_{window}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, 'DeptCategory'].map(dept_medians).fillna(global_median)\n",
        "\n",
        "            # Fill rolling std\n",
        "            col = f'sales_rolling_std_{window}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, col.replace('_std_', '_mean_')] * 0.3\n",
        "\n",
        "        df['sales_trend_4w'] = df['sales_trend_4w'].fillna(0)\n",
        "\n",
        "    else:\n",
        "        print(\"Creating lag features from historical patterns (no target)...\")\n",
        "        # For test data, use historical patterns from dept/store means\n",
        "\n",
        "        lag_cols = ['sales_lag_1', 'sales_lag_2', 'sales_lag_4', 'sales_lag_8', 'sales_lag_12']\n",
        "        rolling_cols = ['sales_rolling_mean_4', 'sales_rolling_mean_8', 'sales_rolling_mean_12',\n",
        "                       'sales_rolling_std_4', 'sales_rolling_std_8', 'sales_rolling_std_12']\n",
        "        trend_cols = ['sales_trend_4w']\n",
        "\n",
        "        # Use department historical means as base\n",
        "        base_values = {\n",
        "            'High_Volume': 20000,\n",
        "            'Volatile': 12000,\n",
        "            'Seasonal': 8000,\n",
        "            'Standard': 15000\n",
        "        }\n",
        "\n",
        "        for col in lag_cols + rolling_cols:\n",
        "            if 'std' in col:\n",
        "                df[col] = df['DeptCategory'].map(base_values).fillna(15000) * 0.3\n",
        "            else:\n",
        "                df[col] = df['DeptCategory'].map(base_values).fillna(15000)\n",
        "\n",
        "        for col in trend_cols:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Fast lag features created in {elapsed:.1f} seconds. Shape: {df.shape}\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "JDW6TwbbBWEs"
      },
      "id": "JDW6TwbbBWEs",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 2: FAST OPTIMIZED LAG FEATURES - 10x Faster\n",
        "# =============================================================================\n",
        "\n",
        "def create_fast_lag_features(df, include_target_lags=True):\n",
        "    \"\"\"Super fast vectorized lag feature creation\"\"\"\n",
        "    print(\"Creating FAST lag features...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['group_id', 'Date']).reset_index(drop=True)\n",
        "\n",
        "    # Only create lag features if we have the target variable\n",
        "    if include_target_lags and 'Weekly_Sales' in df.columns:\n",
        "        print(\"Creating target-based lag features...\")\n",
        "\n",
        "        # VECTORIZED LAG CREATION - Much faster than loops\n",
        "        lag_windows = [1, 2, 4, 8, 12]  # Reduced for speed\n",
        "        rolling_windows = [4, 8, 12]    # Reduced for speed\n",
        "\n",
        "        # Create lag features using groupby.shift (vectorized)\n",
        "        for lag in lag_windows:\n",
        "            df[f'sales_lag_{lag}'] = df.groupby('group_id')['Weekly_Sales'].shift(lag)\n",
        "\n",
        "        # Create rolling features using groupby.rolling (vectorized)\n",
        "        for window in rolling_windows:\n",
        "            df[f'sales_rolling_mean_{window}'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window, min_periods=1).mean()\n",
        "            )\n",
        "            df[f'sales_rolling_std_{window}'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window, min_periods=1).std()\n",
        "            )\n",
        "\n",
        "        # Simple trend features (vectorized)\n",
        "        df['sales_trend_4w'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "            lambda x: x.pct_change(periods=4).fillna(0).clip(-1, 1)\n",
        "        )\n",
        "\n",
        "        # Fill NaN values with intelligent defaults\n",
        "        dept_medians = df.groupby('DeptCategory')['Weekly_Sales'].median().to_dict()\n",
        "        global_median = df['Weekly_Sales'].median()\n",
        "\n",
        "        # Fast NaN filling\n",
        "        for lag in lag_windows:\n",
        "            col = f'sales_lag_{lag}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, 'DeptCategory'].map(dept_medians).fillna(global_median)\n",
        "\n",
        "        for window in rolling_windows:\n",
        "            # Fill rolling mean\n",
        "            col = f'sales_rolling_mean_{window}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, 'DeptCategory'].map(dept_medians).fillna(global_median)\n",
        "\n",
        "            # Fill rolling std\n",
        "            col = f'sales_rolling_std_{window}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, col.replace('_std_', '_mean_')] * 0.3\n",
        "\n",
        "        df['sales_trend_4w'] = df['sales_trend_4w'].fillna(0)\n",
        "\n",
        "    else:\n",
        "        print(\"Creating lag features from historical patterns (no target)...\")\n",
        "        # For test data, use historical patterns from dept/store means\n",
        "\n",
        "        lag_cols = ['sales_lag_1', 'sales_lag_2', 'sales_lag_4', 'sales_lag_8', 'sales_lag_12']\n",
        "        rolling_cols = ['sales_rolling_mean_4', 'sales_rolling_mean_8', 'sales_rolling_mean_12',\n",
        "                       'sales_rolling_std_4', 'sales_rolling_std_8', 'sales_rolling_std_12']\n",
        "        trend_cols = ['sales_trend_4w']\n",
        "\n",
        "        # Use department historical means as base\n",
        "        base_values = {\n",
        "            'High_Volume': 20000,\n",
        "            'Volatile': 12000,\n",
        "            'Seasonal': 8000,\n",
        "            'Standard': 15000\n",
        "        }\n",
        "\n",
        "        for col in lag_cols + rolling_cols:\n",
        "            if 'std' in col:\n",
        "                df[col] = df['DeptCategory'].map(base_values).fillna(15000) * 0.3\n",
        "            else:\n",
        "                df[col] = df['DeptCategory'].map(base_values).fillna(15000)\n",
        "\n",
        "        for col in trend_cols:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Fast lag features created in {elapsed:.1f} seconds. Shape: {df.shape}\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "X7fn01kyBb1E"
      },
      "id": "X7fn01kyBb1E",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 3: FIXED TFT MODEL - Removed categorical_encoders issue\n",
        "# =============================================================================\n",
        "\n",
        "class FixedWalmartTFTModel(BaseEstimator):\n",
        "    \"\"\"TFT model with fixed categorical encoder issue\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 max_prediction_length=8,\n",
        "                 max_encoder_length=32,\n",
        "                 hidden_size=64,\n",
        "                 attention_head_size=2,\n",
        "                 dropout=0.2,\n",
        "                 learning_rate=0.003,\n",
        "                 max_epochs=50,\n",
        "                 patience=10,\n",
        "                 batch_size=256):\n",
        "\n",
        "        self.max_prediction_length = max_prediction_length\n",
        "        self.max_encoder_length = max_encoder_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention_head_size = attention_head_size\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_epochs = max_epochs\n",
        "        self.patience = patience\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = None\n",
        "        self.training_dataset = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def create_tft_dataset(self, df, is_train=True):\n",
        "        \"\"\"Create TFT dataset with FIXED categorical encoders\"\"\"\n",
        "\n",
        "        # ESSENTIAL FEATURES ONLY - for speed\n",
        "        static_categoricals = ['Store', 'Dept', 'Type', 'DeptCategory']\n",
        "\n",
        "        time_varying_known_categoricals = ['Month', 'Quarter', 'TempCategory', 'DayOfWeek']\n",
        "\n",
        "        time_varying_known_reals = [\n",
        "            'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size',\n",
        "            'TotalMarkDown', 'PromoIntensity', 'IsQ4', 'IsBackToSchool',\n",
        "            'Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos',\n",
        "            'sales_lag_1', 'sales_lag_2', 'sales_lag_4',\n",
        "            'sales_rolling_mean_4', 'sales_rolling_mean_8',\n",
        "            'sales_trend_4w'\n",
        "        ]\n",
        "\n",
        "        # Filter to existing columns\n",
        "        static_categoricals = [col for col in static_categoricals if col in df.columns]\n",
        "        time_varying_known_categoricals = [col for col in time_varying_known_categoricals if col in df.columns]\n",
        "        time_varying_known_reals = [col for col in time_varying_known_reals if col in df.columns]\n",
        "\n",
        "        print(f\"TFT Dataset - Static: {len(static_categoricals)}, Time cats: {len(time_varying_known_categoricals)}, Time reals: {len(time_varying_known_reals)}\")\n",
        "\n",
        "        if is_train:\n",
        "            training = TimeSeriesDataSet(\n",
        "                df,\n",
        "                time_idx=\"time_idx\",\n",
        "                target=\"Weekly_Sales\",\n",
        "                group_ids=[\"group_id\"],\n",
        "                min_encoder_length=self.max_encoder_length // 2,\n",
        "                max_encoder_length=self.max_encoder_length,\n",
        "                min_prediction_length=1,\n",
        "                max_prediction_length=self.max_prediction_length,\n",
        "                static_categoricals=static_categoricals,\n",
        "                time_varying_known_categoricals=time_varying_known_categoricals,\n",
        "                time_varying_known_reals=time_varying_known_reals,\n",
        "                target_normalizer=GroupNormalizer(\n",
        "                    groups=[\"group_id\"],\n",
        "                    transformation=\"softplus\",\n",
        "                    center=True\n",
        "                ),\n",
        "                add_relative_time_idx=True,\n",
        "                add_target_scales=True,\n",
        "                add_encoder_length=True,\n",
        "                allow_missing_timesteps=True\n",
        "                # REMOVED: categorical_encoders={'group_id': 'auto'}  # This was causing the error\n",
        "            )\n",
        "            return training\n",
        "        return None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"Training FIXED TFT model...\")\n",
        "\n",
        "        # SMART GROUP FILTERING - Keep groups with enough history\n",
        "        min_required = self.max_encoder_length + self.max_prediction_length\n",
        "        group_counts = X['group_id'].value_counts()\n",
        "\n",
        "        # Progressive relaxation of requirements\n",
        "        valid_groups = group_counts[group_counts >= min_required].index\n",
        "        print(f\"Groups with {min_required}+ samples: {len(valid_groups)}\")\n",
        "\n",
        "        if len(valid_groups) < 100:  # Too few groups\n",
        "            min_required = max(20, min_required // 2)\n",
        "            valid_groups = group_counts[group_counts >= min_required].index\n",
        "            print(f\"Relaxed to {min_required}+ samples: {len(valid_groups)}\")\n",
        "\n",
        "        if len(valid_groups) < 50:  # Still too few\n",
        "            min_required = 15\n",
        "            valid_groups = group_counts[group_counts >= min_required].index\n",
        "            print(f\"Final relaxation to {min_required}+ samples: {len(valid_groups)}\")\n",
        "\n",
        "        filtered_data = X[X['group_id'].isin(valid_groups)].copy()\n",
        "        print(f\"Training on {len(valid_groups)} groups with {len(filtered_data)} samples\")\n",
        "\n",
        "        # Create dataset\n",
        "        try:\n",
        "            self.training_dataset = self.create_tft_dataset(filtered_data, is_train=True)\n",
        "            print(\"✅ Training dataset created successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creating training dataset: {e}\")\n",
        "            return self\n",
        "\n",
        "        # Create validation from same data (temporal split within groups)\n",
        "        try:\n",
        "            validation = TimeSeriesDataSet.from_dataset(\n",
        "                self.training_dataset,\n",
        "                filtered_data,\n",
        "                predict=True,\n",
        "                stop_randomization=True\n",
        "            )\n",
        "            print(\"✅ Validation dataset created successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creating validation dataset: {e}\")\n",
        "            return self\n",
        "\n",
        "        # Data loaders\n",
        "        try:\n",
        "            train_dataloader = self.training_dataset.to_dataloader(\n",
        "                train=True,\n",
        "                batch_size=self.batch_size,\n",
        "                num_workers=0,\n",
        "                shuffle=True\n",
        "            )\n",
        "\n",
        "            val_dataloader = validation.to_dataloader(\n",
        "                train=False,\n",
        "                batch_size=self.batch_size * 2,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            print(f\"✅ Data loaders created - Train: {len(train_dataloader)}, Val: {len(val_dataloader)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creating data loaders: {e}\")\n",
        "            return self\n",
        "\n",
        "        # TFT model\n",
        "        try:\n",
        "            self.model = TemporalFusionTransformer.from_dataset(\n",
        "                self.training_dataset,\n",
        "                learning_rate=self.learning_rate,\n",
        "                hidden_size=self.hidden_size,\n",
        "                attention_head_size=self.attention_head_size,\n",
        "                dropout=self.dropout,\n",
        "                hidden_continuous_size=self.hidden_size//2,\n",
        "                output_size=7,\n",
        "                loss=QuantileLoss([0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]),\n",
        "                log_interval=25,\n",
        "                reduce_on_plateau_patience=3,\n",
        "                optimizer=\"AdamW\"\n",
        "            )\n",
        "            print(\"✅ TFT model created successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creating TFT model: {e}\")\n",
        "            return self\n",
        "\n",
        "        # Training callbacks\n",
        "        early_stop_callback = EarlyStopping(\n",
        "            monitor=\"val_loss\",\n",
        "            min_delta=1e-5,\n",
        "            patience=self.patience,\n",
        "            verbose=True,\n",
        "            mode=\"min\"\n",
        "        )\n",
        "\n",
        "        # Trainer\n",
        "        self.trainer = pl.Trainer(\n",
        "            max_epochs=self.max_epochs,\n",
        "            accelerator=\"cpu\",\n",
        "            devices=1,\n",
        "            callbacks=[early_stop_callback],\n",
        "            logger=False,\n",
        "            enable_progress_bar=True,\n",
        "            enable_checkpointing=False,\n",
        "            gradient_clip_val=1.0\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        print(\"Starting training...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            self.trainer.fit(\n",
        "                self.model,\n",
        "                train_dataloaders=train_dataloader,\n",
        "                val_dataloaders=val_dataloader,\n",
        "            )\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "            print(f\"✅ Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Training error: {e}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Generate predictions with fallback\"\"\"\n",
        "        print(f\"Generating predictions for {len(X)} samples...\")\n",
        "\n",
        "        if self.model is None or self.training_dataset is None:\n",
        "            print(\"No trained model available, using fallback\")\n",
        "            return self._intelligent_fallback(X)\n",
        "\n",
        "        try:\n",
        "            # Create prediction dataset\n",
        "            prediction_data = TimeSeriesDataSet.from_dataset(\n",
        "                self.training_dataset,\n",
        "                X,\n",
        "                predict=True,\n",
        "                stop_randomization=True\n",
        "            )\n",
        "\n",
        "            predict_dataloader = prediction_data.to_dataloader(\n",
        "                train=False,\n",
        "                batch_size=self.batch_size * 2,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            if len(predict_dataloader) > 0:\n",
        "                raw_predictions = self.trainer.predict(\n",
        "                    self.model,\n",
        "                    dataloaders=predict_dataloader\n",
        "                )\n",
        "\n",
        "                if raw_predictions and len(raw_predictions) > 0:\n",
        "                    all_preds = torch.cat(raw_predictions, dim=0)\n",
        "                    median_preds = all_preds[:, 3].cpu().numpy()\n",
        "                    median_preds = np.clip(median_preds, 10, 100000)\n",
        "\n",
        "                    if len(median_preds) == len(X):\n",
        "                        print(f\"✅ TFT predictions generated successfully\")\n",
        "                        return median_preds\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Prediction error: {e}\")\n",
        "\n",
        "        print(\"Using fallback predictions\")\n",
        "        return self._intelligent_fallback(X)\n",
        "\n",
        "    def _intelligent_fallback(self, X):\n",
        "        \"\"\"Smart fallback predictions\"\"\"\n",
        "\n",
        "        base_values = {\n",
        "            'High_Volume': 20000,\n",
        "            'Volatile': 12000,\n",
        "            'Seasonal': 8000,\n",
        "            'Standard': 15000\n",
        "        }\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for _, row in X.iterrows():\n",
        "            # Base prediction from department category\n",
        "            base_pred = base_values.get(row.get('DeptCategory', 'Standard'), 15000)\n",
        "\n",
        "            # Use lag features if available\n",
        "            if 'sales_lag_1' in row and pd.notna(row['sales_lag_1']) and row['sales_lag_1'] > 0:\n",
        "                base_pred = 0.6 * base_pred + 0.4 * row['sales_lag_1']\n",
        "\n",
        "            if 'sales_rolling_mean_4' in row and pd.notna(row['sales_rolling_mean_4']):\n",
        "                base_pred = 0.5 * base_pred + 0.5 * row['sales_rolling_mean_4']\n",
        "\n",
        "            # Apply seasonal adjustments\n",
        "            seasonal_mult = 1.0\n",
        "\n",
        "            if row.get('IsQ4', 0) == 1:\n",
        "                seasonal_mult *= 1.2\n",
        "\n",
        "            if row.get('IsBackToSchool', 0) == 1:\n",
        "                seasonal_mult *= 1.1\n",
        "\n",
        "            if row.get('HasAnyPromo', 0) == 1:\n",
        "                seasonal_mult *= 1.05\n",
        "\n",
        "            final_pred = base_pred * seasonal_mult\n",
        "            final_pred = np.clip(final_pred, 10, 80000)\n",
        "\n",
        "            predictions.append(final_pred)\n",
        "\n",
        "        return np.array(predictions)"
      ],
      "metadata": {
        "id": "rSE4E32hUiRS"
      },
      "id": "rSE4E32hUiRS",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STREAMLINED TRAINING FUNCTION - No Complex Validation\n",
        "# =============================================================================\n",
        "\n",
        "def train_fast_tft_model():\n",
        "    \"\"\"Fast training pipeline without complex validation\"\"\"\n",
        "\n",
        "    wandb.init(project=\"walmart-fast-tft\", name=\"fixed_training\")\n",
        "\n",
        "    print(\"=== FAST WALMART TFT TRAINING ===\")\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    train_df = pd.read_csv(\"/content/train.csv\")\n",
        "    test_df = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "    # Convert dates\n",
        "    for df in [train_df, test_df]:\n",
        "        if 'Date' in df.columns:\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    print(f\"Data shapes - Train: {train_df.shape}, Test: {test_df.shape}\")\n",
        "\n",
        "    # Feature engineering\n",
        "    print(\"\\n=== FEATURE ENGINEERING ===\")\n",
        "    feature_engineer = EnhancedWalmartFeatureEngineer()\n",
        "    feature_engineer.fit(train_df)\n",
        "\n",
        "    processed_train = feature_engineer.transform(train_df)\n",
        "    processed_test = feature_engineer.transform(test_df)\n",
        "\n",
        "    # Add lag features\n",
        "    print(\"Adding lag features...\")\n",
        "    processed_train = create_fast_lag_features(processed_train, include_target_lags=True)\n",
        "    processed_test = create_fast_lag_features(processed_test, include_target_lags=False)\n",
        "\n",
        "    print(f\"Final shapes - Train: {processed_train.shape}, Test: {processed_test.shape}\")\n",
        "\n",
        "    # Simple train/val split by time\n",
        "    print(\"\\n=== SIMPLE VALIDATION SPLIT ===\")\n",
        "    cutoff_date = processed_train['Date'].quantile(0.85)  # Use 85% for training\n",
        "\n",
        "    train_data = processed_train[processed_train['Date'] <= cutoff_date].copy()\n",
        "    val_data = processed_train[processed_train['Date'] > cutoff_date].copy()\n",
        "\n",
        "    print(f\"Train: {len(train_data):,}, Val: {len(val_data):,}\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\n=== TRAINING TFT MODEL ===\")\n",
        "    model = FixedWalmartTFTModel(\n",
        "        max_prediction_length=8,\n",
        "        max_encoder_length=32,\n",
        "        hidden_size=64,\n",
        "        attention_head_size=2,\n",
        "        learning_rate=0.003,\n",
        "        max_epochs=50,\n",
        "        patience=10,\n",
        "        batch_size=256\n",
        "    )\n",
        "\n",
        "    model.fit(train_data)\n",
        "\n",
        "    # Validate\n",
        "    print(\"\\n=== VALIDATION ===\")\n",
        "    val_pred = model.predict(val_data)\n",
        "    val_actual = val_data['Weekly_Sales'].values\n",
        "\n",
        "    val_mae = np.mean(np.abs(val_pred - val_actual))\n",
        "    val_wmae = np.sum(np.abs(val_pred - val_actual)) / np.sum(val_actual)\n",
        "\n",
        "    print(f\"Validation MAE: {val_mae:,.2f}\")\n",
        "    print(f\"Validation WMAE: {val_wmae:.4f}\")\n",
        "\n",
        "    wandb.log({\"val_mae\": val_mae, \"val_wmae\": val_wmae})\n",
        "\n",
        "    # Final training on all data\n",
        "    print(\"\\n=== FINAL TRAINING ON ALL DATA ===\")\n",
        "    final_model = FixedWalmartTFTModel(\n",
        "        max_prediction_length=8,\n",
        "        max_encoder_length=32,\n",
        "        hidden_size=64,\n",
        "        attention_head_size=2,\n",
        "        learning_rate=0.003,\n",
        "        max_epochs=30,  # Fewer epochs for final training\n",
        "        patience=8,\n",
        "        batch_size=256\n",
        "    )\n",
        "\n",
        "    final_model.fit(processed_train)\n",
        "\n",
        "    # Generate predictions\n",
        "    print(\"\\n=== GENERATING PREDICTIONS ===\")\n",
        "    test_predictions = final_model.predict(processed_test)\n",
        "\n",
        "    # Create submission\n",
        "    submission = pd.DataFrame({\n",
        "        'Id': range(1, len(test_predictions) + 1),\n",
        "        'Weekly_Sales': test_predictions\n",
        "    })\n",
        "\n",
        "    submission_file = \"fixed_tft_submission.csv\"\n",
        "    submission.to_csv(submission_file, index=False)\n",
        "\n",
        "    print(f\"\\n✅ SUBMISSION SAVED: {submission_file}\")\n",
        "    print(f\"Prediction stats:\")\n",
        "    print(f\"  Mean: {test_predictions.mean():,.2f}\")\n",
        "    print(f\"  Std: {test_predictions.std():,.2f}\")\n",
        "    print(f\"  Min: {test_predictions.min():,.2f}\")\n",
        "    print(f\"  Max: {test_predictions.max():,.2f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"test_pred_mean\": test_predictions.mean(),\n",
        "        \"test_pred_std\": test_predictions.std(),\n",
        "        \"test_pred_min\": test_predictions.min(),\n",
        "        \"test_pred_max\": test_predictions.max()\n",
        "    })\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    return final_model, submission_file\n",
        "\n",
        "# Run the training\n",
        "print(\"✅ FIXED TFT SETUP COMPLETE!\")\n",
        "import time\n",
        "model, submission_file = train_fast_tft_model()"
      ],
      "metadata": {
        "id": "yXR8aSn8W0SN"
      },
      "id": "yXR8aSn8W0SN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL FIXED TFT MODEL WITH PROPER PREDICTION HANDLING\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, GroupNormalizer, QuantileLoss\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from sklearn.base import BaseEstimator\n",
        "import wandb\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# FEATURE ENGINEERING - SAME AS BEFORE\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedWalmartFeatureEngineer:\n",
        "    def __init__(self):\n",
        "        self.dept_stats = {}\n",
        "        self.store_stats = {}\n",
        "        self.global_stats = {}\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, train_df):\n",
        "        \"\"\"Learn patterns from training data\"\"\"\n",
        "        print(\"Learning advanced retail patterns...\")\n",
        "\n",
        "        # Department statistics\n",
        "        self.dept_stats = train_df.groupby('Dept').agg({\n",
        "            'Weekly_Sales': ['mean', 'std', 'median', 'min', 'max']\n",
        "        }).round(2)\n",
        "        self.dept_stats.columns = ['_'.join(col).strip() for col in self.dept_stats.columns]\n",
        "\n",
        "        # Store statistics\n",
        "        self.store_stats = train_df.groupby('Store').agg({\n",
        "            'Weekly_Sales': ['mean', 'std', 'median']\n",
        "        }).round(2)\n",
        "        self.store_stats.columns = ['_'.join(col).strip() for col in self.store_stats.columns]\n",
        "\n",
        "        # Global statistics\n",
        "        self.global_stats = {\n",
        "            'sales_mean': train_df['Weekly_Sales'].mean(),\n",
        "            'sales_std': train_df['Weekly_Sales'].std(),\n",
        "            'sales_median': train_df['Weekly_Sales'].median()\n",
        "        }\n",
        "\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        \"\"\"Transform data with comprehensive feature engineering\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        print(f\"Starting transform with shape: {df.shape}\")\n",
        "\n",
        "        # CRITICAL FIX: Convert Store and Dept to strings FIRST\n",
        "        df['Store'] = df['Store'].astype(str)\n",
        "        df['Dept'] = df['Dept'].astype(str)\n",
        "\n",
        "        # CRITICAL FIX: Add DayOfWeek and other missing temporal features\n",
        "        if 'Date' in df.columns:\n",
        "            df['DayOfWeek'] = df['Date'].dt.dayofweek.astype(str)\n",
        "            df['DayOfMonth'] = df['Date'].dt.day\n",
        "            df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "            df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "        # Basic temporal features - convert to strings for categoricals\n",
        "        df['Year'] = df['Date'].dt.year.astype(str)\n",
        "        df['Month'] = df['Date'].dt.month.astype(str)\n",
        "        df['Quarter'] = df['Date'].dt.quarter.astype(str)\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week.astype(str)\n",
        "\n",
        "        # Create time_idx for TFT (critical!)\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "        df['time_idx'] = df.groupby(['Store', 'Dept']).cumcount()\n",
        "\n",
        "        # Create group_id for TFT\n",
        "        df['group_id'] = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n",
        "\n",
        "        # ROBUST EXTERNAL DATA LOADING\n",
        "        try:\n",
        "            features_df = pd.read_csv(\"/content/features.csv\")\n",
        "            stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "\n",
        "            # Convert dates in features\n",
        "            features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "            # CRITICAL: Convert Store to string in external data too\n",
        "            features_df['Store'] = features_df['Store'].astype(str)\n",
        "            stores_df['Store'] = stores_df['Store'].astype(str)\n",
        "\n",
        "            # Merge with features and stores\n",
        "            df = df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "            df = df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading external data: {e}\")\n",
        "            # Create minimal required columns if files don't exist\n",
        "            df['Temperature'] = 70.0\n",
        "            df['Fuel_Price'] = 3.5\n",
        "            df['CPI'] = 200.0\n",
        "            df['Unemployment'] = 7.0\n",
        "            df['IsHoliday'] = 0\n",
        "            df['Type'] = 'A'\n",
        "            df['Size'] = 150000\n",
        "            for i in range(1, 6):\n",
        "                df[f'MarkDown{i}'] = 0.0\n",
        "\n",
        "        # Fill missing values intelligently\n",
        "        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "            else:\n",
        "                if col == 'Temperature':\n",
        "                    df[col] = 70.0\n",
        "                elif col == 'Fuel_Price':\n",
        "                    df[col] = 3.5\n",
        "                elif col == 'CPI':\n",
        "                    df[col] = 200.0\n",
        "                elif col == 'Unemployment':\n",
        "                    df[col] = 7.0\n",
        "\n",
        "        # ROBUST Holiday handling\n",
        "        if 'IsHoliday' not in df.columns:\n",
        "            df['IsHoliday'] = 0\n",
        "        else:\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(0).astype(int)\n",
        "\n",
        "        # ROBUST Markdown features\n",
        "        markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        for col in markdown_cols:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 0.0\n",
        "            else:\n",
        "                df[col] = df[col].fillna(0)\n",
        "            df[f'Has{col}'] = (df[col] > 0).astype(int)\n",
        "\n",
        "        # Promotional features\n",
        "        df['TotalMarkDown'] = df[markdown_cols].sum(axis=1)\n",
        "        df['HasAnyPromo'] = (df['TotalMarkDown'] > 0).astype(int)\n",
        "        df['PromoIntensity'] = df['TotalMarkDown'] / (df['TotalMarkDown'].quantile(0.95) + 1)\n",
        "        df['PromoIntensity'] = df['PromoIntensity'].clip(0, 1)\n",
        "\n",
        "        # ROBUST Store and Type handling\n",
        "        if 'Type' not in df.columns:\n",
        "            df['Type'] = 'A'\n",
        "        if 'Size' not in df.columns:\n",
        "            df['Size'] = 150000\n",
        "\n",
        "        df['Type'] = df['Type'].fillna('A').astype(str)\n",
        "        df['Size'] = df['Size'].fillna(df['Size'].median())\n",
        "\n",
        "        # Seasonal features\n",
        "        df['IsQ4'] = (df['Quarter'].astype(int) == 4).astype(int)\n",
        "        df['IsQ1'] = (df['Quarter'].astype(int) == 1).astype(int)\n",
        "        df['IsBackToSchool'] = ((df['Month'].astype(int) == 8) | (df['Month'].astype(int) == 9)).astype(int)\n",
        "\n",
        "        # Weather categories\n",
        "        if 'Temperature' in df.columns:\n",
        "            df['TempCategory'] = pd.cut(df['Temperature'],\n",
        "                                      bins=[-np.inf, 32, 50, 70, 85, np.inf],\n",
        "                                      labels=['Freezing', 'Cold', 'Cool', 'Warm', 'Hot']).astype(str)\n",
        "        else:\n",
        "            df['TempCategory'] = 'Cool'\n",
        "\n",
        "        # Store categorization\n",
        "        if 'Size' in df.columns:\n",
        "            df['StoreSize_Cat'] = pd.cut(df['Size'],\n",
        "                                       bins=[0, 50000, 100000, 150000, 200000, np.inf],\n",
        "                                       labels=['XS', 'S', 'M', 'L', 'XL']).astype(str)\n",
        "        else:\n",
        "            df['StoreSize_Cat'] = 'M'\n",
        "\n",
        "        # Department categorization\n",
        "        high_volume_depts = [1, 2, 3, 7, 8, 13, 16, 20, 24, 27, 40, 46, 50, 57, 79, 81]\n",
        "        volatile_depts = [5, 6, 9, 12, 14, 18, 21, 25, 28, 34, 39, 47, 48, 54, 56, 60, 67, 77, 80, 86, 87, 91, 92, 95]\n",
        "        seasonal_depts = [11, 15, 23, 29, 33, 35, 41, 45, 65, 68, 74, 78, 96, 97, 98, 99]\n",
        "\n",
        "        df['DeptCategory'] = 'Standard'\n",
        "        df.loc[df['Dept'].astype(int).isin(high_volume_depts), 'DeptCategory'] = 'High_Volume'\n",
        "        df.loc[df['Dept'].astype(int).isin(volatile_depts), 'DeptCategory'] = 'Volatile'\n",
        "        df.loc[df['Dept'].astype(int).isin(seasonal_depts), 'DeptCategory'] = 'Seasonal'\n",
        "\n",
        "        # Cyclical encoding for temporal features\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'].astype(int) / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'].astype(int) / 12)\n",
        "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'].astype(int) / 52)\n",
        "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'].astype(int) / 52)\n",
        "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'].astype(int) / 7)\n",
        "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'].astype(int) / 7)\n",
        "\n",
        "        # Store-Dept interaction features\n",
        "        if self.is_fitted:\n",
        "            dept_means = self.dept_stats.get('Weekly_Sales_mean', {})\n",
        "            store_means = self.store_stats.get('Weekly_Sales_mean', {})\n",
        "\n",
        "            df['Dept_HistoricalMean'] = df['Dept'].map(dept_means).fillna(self.global_stats['sales_mean'])\n",
        "            df['Store_HistoricalMean'] = df['Store'].map(store_means).fillna(self.global_stats['sales_mean'])\n",
        "\n",
        "        # CRITICAL: Ensure all categorical columns are STRING type for TFT\n",
        "        categorical_string_cols = ['Store', 'Dept', 'Type', 'StoreSize_Cat', 'DeptCategory', 'TempCategory',\n",
        "                                 'Month', 'Quarter', 'Week', 'DayOfWeek']\n",
        "        for col in categorical_string_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "        print(f\"Enhanced features created. Final shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "# =============================================================================\n",
        "# FAST LAG FEATURES\n",
        "# =============================================================================\n",
        "\n",
        "def create_fast_lag_features(df, include_target_lags=True):\n",
        "    \"\"\"Super fast vectorized lag feature creation\"\"\"\n",
        "    print(\"Creating FAST lag features...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['group_id', 'Date']).reset_index(drop=True)\n",
        "\n",
        "    # Only create lag features if we have the target variable\n",
        "    if include_target_lags and 'Weekly_Sales' in df.columns:\n",
        "        print(\"Creating target-based lag features...\")\n",
        "\n",
        "        # VECTORIZED LAG CREATION - Much faster than loops\n",
        "        lag_windows = [1, 2, 4, 8, 12]\n",
        "        rolling_windows = [4, 8, 12]\n",
        "\n",
        "        # Create lag features using groupby.shift (vectorized)\n",
        "        for lag in lag_windows:\n",
        "            df[f'sales_lag_{lag}'] = df.groupby('group_id')['Weekly_Sales'].shift(lag)\n",
        "\n",
        "        # Create rolling features using groupby.rolling (vectorized)\n",
        "        for window in rolling_windows:\n",
        "            df[f'sales_rolling_mean_{window}'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window, min_periods=1).mean()\n",
        "            )\n",
        "            df[f'sales_rolling_std_{window}'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "                lambda x: x.rolling(window, min_periods=1).std()\n",
        "            )\n",
        "\n",
        "        # Simple trend features (vectorized)\n",
        "        df['sales_trend_4w'] = df.groupby('group_id')['Weekly_Sales'].transform(\n",
        "            lambda x: x.pct_change(periods=4).fillna(0).clip(-1, 1)\n",
        "        )\n",
        "\n",
        "        # Fill NaN values with intelligent defaults\n",
        "        dept_medians = df.groupby('DeptCategory')['Weekly_Sales'].median().to_dict()\n",
        "        global_median = df['Weekly_Sales'].median()\n",
        "\n",
        "        # Fast NaN filling\n",
        "        for lag in lag_windows:\n",
        "            col = f'sales_lag_{lag}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, 'DeptCategory'].map(dept_medians).fillna(global_median)\n",
        "\n",
        "        for window in rolling_windows:\n",
        "            # Fill rolling mean\n",
        "            col = f'sales_rolling_mean_{window}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, 'DeptCategory'].map(dept_medians).fillna(global_median)\n",
        "\n",
        "            # Fill rolling std\n",
        "            col = f'sales_rolling_std_{window}'\n",
        "            mask = df[col].isna()\n",
        "            df.loc[mask, col] = df.loc[mask, col.replace('_std_', '_mean_')] * 0.3\n",
        "\n",
        "        df['sales_trend_4w'] = df['sales_trend_4w'].fillna(0)\n",
        "\n",
        "    else:\n",
        "        print(\"Creating lag features from historical patterns (no target)...\")\n",
        "        # For test data, use historical patterns from dept/store means\n",
        "\n",
        "        lag_cols = ['sales_lag_1', 'sales_lag_2', 'sales_lag_4', 'sales_lag_8', 'sales_lag_12']\n",
        "        rolling_cols = ['sales_rolling_mean_4', 'sales_rolling_mean_8', 'sales_rolling_mean_12',\n",
        "                       'sales_rolling_std_4', 'sales_rolling_std_8', 'sales_rolling_std_12']\n",
        "        trend_cols = ['sales_trend_4w']\n",
        "\n",
        "        # Use department historical means as base\n",
        "        base_values = {\n",
        "            'High_Volume': 20000,\n",
        "            'Volatile': 12000,\n",
        "            'Seasonal': 8000,\n",
        "            'Standard': 15000\n",
        "        }\n",
        "\n",
        "        for col in lag_cols + rolling_cols:\n",
        "            if 'std' in col:\n",
        "                df[col] = df['DeptCategory'].map(base_values).fillna(15000) * 0.3\n",
        "            else:\n",
        "                df[col] = df['DeptCategory'].map(base_values).fillna(15000)\n",
        "\n",
        "        for col in trend_cols:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Fast lag features created in {elapsed:.1f} seconds. Shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL FIXED TFT MODEL WITH PROPER PREDICTION HANDLING\n",
        "# =============================================================================\n",
        "\n",
        "class FinalTFTModel(BaseEstimator):\n",
        "    \"\"\"TFT model with proper prediction dataset creation\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 max_prediction_length=8,\n",
        "                 max_encoder_length=20,\n",
        "                 hidden_size=32,\n",
        "                 attention_head_size=1,\n",
        "                 dropout=0.1,\n",
        "                 learning_rate=0.01,\n",
        "                 max_epochs=2,\n",
        "                 patience=5,\n",
        "                 batch_size=512):\n",
        "\n",
        "        self.max_prediction_length = max_prediction_length\n",
        "        self.max_encoder_length = max_encoder_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention_head_size = attention_head_size\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_epochs = max_epochs\n",
        "        self.patience = patience\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = None\n",
        "        self.training_dataset = None\n",
        "        self.train_groups = set()\n",
        "\n",
        "    def create_tft_dataset(self, df, is_train=True):\n",
        "        \"\"\"Create TFT dataset with minimal features\"\"\"\n",
        "\n",
        "        # MINIMAL FEATURES FOR STABILITY\n",
        "        static_categoricals = ['Store', 'Dept', 'Type']\n",
        "\n",
        "        time_varying_known_categoricals = ['Month', 'Quarter']\n",
        "\n",
        "        time_varying_known_reals = [\n",
        "            'Temperature', 'Fuel_Price', 'CPI', 'Size',\n",
        "            'TotalMarkDown', 'IsQ4',\n",
        "            'Month_sin', 'Month_cos',\n",
        "            'sales_lag_1', 'sales_lag_2',\n",
        "            'sales_rolling_mean_4'\n",
        "        ]\n",
        "\n",
        "        # Filter to existing columns\n",
        "        static_categoricals = [col for col in static_categoricals if col in df.columns]\n",
        "        time_varying_known_categoricals = [col for col in time_varying_known_categoricals if col in df.columns]\n",
        "        time_varying_known_reals = [col for col in time_varying_known_reals if col in df.columns]\n",
        "\n",
        "        print(f\"TFT Dataset - Static: {len(static_categoricals)}, Time cats: {len(time_varying_known_categoricals)}, Time reals: {len(time_varying_known_reals)}\")\n",
        "\n",
        "        if is_train:\n",
        "            training = TimeSeriesDataSet(\n",
        "                df,\n",
        "                time_idx=\"time_idx\",\n",
        "                target=\"Weekly_Sales\",\n",
        "                group_ids=[\"group_id\"],\n",
        "                min_encoder_length=6,\n",
        "                max_encoder_length=self.max_encoder_length,\n",
        "                min_prediction_length=1,\n",
        "                max_prediction_length=self.max_prediction_length,\n",
        "                static_categoricals=static_categoricals,\n",
        "                time_varying_known_categoricals=time_varying_known_categoricals,\n",
        "                time_varying_known_reals=time_varying_known_reals,\n",
        "                target_normalizer=GroupNormalizer(\n",
        "                    groups=[\"group_id\"],\n",
        "                    transformation=\"softplus\"\n",
        "                ),\n",
        "                add_relative_time_idx=True,\n",
        "                add_target_scales=True,\n",
        "                add_encoder_length=True,\n",
        "                allow_missing_timesteps=True\n",
        "            )\n",
        "            return training\n",
        "        return None\n",
        "\n",
        "    def extract_batch_data(self, batch):\n",
        "        \"\"\"Properly extract inputs and targets from batch\"\"\"\n",
        "        try:\n",
        "            # Handle different batch formats\n",
        "            if isinstance(batch, (tuple, list)):\n",
        "                if len(batch) >= 2:\n",
        "                    # Standard format: (inputs, targets)\n",
        "                    inputs = batch[0]\n",
        "                    targets = batch[1]\n",
        "\n",
        "                    # If targets is a dict, extract the actual target values\n",
        "                    if isinstance(targets, dict):\n",
        "                        if 'Weekly_Sales' in targets:\n",
        "                            targets = targets['Weekly_Sales']\n",
        "                        elif 'target' in targets:\n",
        "                            targets = targets['target']\n",
        "                        else:\n",
        "                            # Take first tensor value\n",
        "                            targets = list(targets.values())[0]\n",
        "                    elif isinstance(targets, (tuple, list)):\n",
        "                        # If targets is a tuple/list, take the first element\n",
        "                        targets = targets[0] if len(targets) > 0 else None\n",
        "\n",
        "                    return inputs, targets\n",
        "                else:\n",
        "                    # Single element batch\n",
        "                    return batch[0], None\n",
        "\n",
        "            elif isinstance(batch, dict):\n",
        "                # Batch is a dictionary\n",
        "                if 'Weekly_Sales' in batch:\n",
        "                    targets = batch['Weekly_Sales']\n",
        "                    inputs = {k: v for k, v in batch.items() if k != 'Weekly_Sales'}\n",
        "                    return inputs, targets\n",
        "                else:\n",
        "                    # No clear target, return batch as inputs\n",
        "                    return batch, None\n",
        "            else:\n",
        "                # Unknown format\n",
        "                return batch, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting batch data: {e}\")\n",
        "            return batch, None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"🚀 Training FINAL TFT with proper handling...\")\n",
        "\n",
        "        # Filter groups\n",
        "        min_required = self.max_encoder_length + self.max_prediction_length + 5\n",
        "        group_counts = X['group_id'].value_counts()\n",
        "\n",
        "        valid_groups = group_counts[group_counts >= min_required].index\n",
        "        print(f\"Groups with {min_required}+ samples: {len(valid_groups)}\")\n",
        "\n",
        "        if len(valid_groups) > 1000:\n",
        "            valid_groups = group_counts.head(1000).index\n",
        "            print(f\"Limited to top 1000 groups\")\n",
        "\n",
        "        self.train_groups = set(valid_groups)\n",
        "\n",
        "        filtered_data = X[X['group_id'].isin(valid_groups)].copy()\n",
        "        print(f\"Training on {len(valid_groups)} groups with {len(filtered_data)} samples\")\n",
        "\n",
        "        # Create dataset\n",
        "        self.training_dataset = self.create_tft_dataset(filtered_data, is_train=True)\n",
        "        print(\"✅ Training dataset created\")\n",
        "\n",
        "        # Create validation dataset\n",
        "        validation = TimeSeriesDataSet.from_dataset(\n",
        "            self.training_dataset,\n",
        "            filtered_data,\n",
        "            predict=True,\n",
        "            stop_randomization=True\n",
        "        )\n",
        "        print(\"✅ Validation dataset created\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataloader = self.training_dataset.to_dataloader(\n",
        "            train=True,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=0,\n",
        "            shuffle=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        val_dataloader = validation.to_dataloader(\n",
        "            train=False,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=0,\n",
        "            drop_last=False\n",
        "        )\n",
        "\n",
        "        print(f\"✅ Data loaders created - Train: {len(train_dataloader)}, Val: {len(val_dataloader)}\")\n",
        "\n",
        "        # Create TFT model\n",
        "        self.model = TemporalFusionTransformer.from_dataset(\n",
        "            self.training_dataset,\n",
        "            learning_rate=self.learning_rate,\n",
        "            hidden_size=self.hidden_size,\n",
        "            attention_head_size=self.attention_head_size,\n",
        "            dropout=self.dropout,\n",
        "            hidden_continuous_size=16,\n",
        "            output_size=7,\n",
        "            loss=QuantileLoss([0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]),\n",
        "            log_interval=50,\n",
        "            reduce_on_plateau_patience=2\n",
        "        )\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(f\"✅ TFT model created - {total_params:,} parameters\")\n",
        "\n",
        "        # TRAINING LOOP\n",
        "        print(\"🔥 Starting training loop...\")\n",
        "\n",
        "        # Set up optimizer\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Training variables\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Get loss function from model\n",
        "        loss_fn = self.model.loss\n",
        "\n",
        "        for epoch in range(self.max_epochs):\n",
        "            # Training phase\n",
        "            train_losses = []\n",
        "            self.model.train()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.max_epochs} - Training...\")\n",
        "\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                try:\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    inputs, targets = self.extract_batch_data(batch)\n",
        "\n",
        "                    # DIRECT FORWARD PASS\n",
        "                    predictions = self.model(inputs)\n",
        "\n",
        "                    # Handle predictions format\n",
        "                    if isinstance(predictions, dict):\n",
        "                        predictions = predictions['prediction']\n",
        "                    elif isinstance(predictions, tuple):\n",
        "                        predictions = predictions[0]\n",
        "\n",
        "                    # Calculate loss\n",
        "                    if targets is not None:\n",
        "                        loss = loss_fn(predictions, targets)\n",
        "                    else:\n",
        "                        # Skip this batch if no targets\n",
        "                        continue\n",
        "\n",
        "                    # Backward pass\n",
        "                    loss.backward()\n",
        "\n",
        "                    # Gradient clipping\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                    train_losses.append(loss.item())\n",
        "\n",
        "                    # Print progress every 50 batches\n",
        "                    if batch_idx % 50 == 0:\n",
        "                        print(f\"  Batch {batch_idx}/{len(train_dataloader)}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Training step error at batch {batch_idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Validation phase\n",
        "            val_losses = []\n",
        "            self.model.eval()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.max_epochs} - Validating...\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, batch in enumerate(val_dataloader):\n",
        "                    try:\n",
        "                        inputs, targets = self.extract_batch_data(batch)\n",
        "\n",
        "                        # DIRECT FORWARD PASS\n",
        "                        predictions = self.model(inputs)\n",
        "\n",
        "                        # Handle predictions format\n",
        "                        if isinstance(predictions, dict):\n",
        "                            predictions = predictions['prediction']\n",
        "                        elif isinstance(predictions, tuple):\n",
        "                            predictions = predictions[0]\n",
        "\n",
        "                        # Calculate loss\n",
        "                        if targets is not None:\n",
        "                            val_loss = loss_fn(predictions, targets)\n",
        "                            val_losses.append(val_loss.item())\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Validation step error at batch {batch_idx}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            # Calculate average losses\n",
        "            avg_train_loss = np.mean(train_losses) if train_losses else float('inf')\n",
        "            avg_val_loss = np.mean(val_losses) if val_losses else float('inf')\n",
        "\n",
        "            print(f\"✅ Epoch {epoch+1} Complete - Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model state\n",
        "                self.best_model_state = self.model.state_dict().copy()\n",
        "                print(f\"  🎯 New best validation loss: {best_val_loss:.6f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                print(f\"  ⏰ Patience: {patience_counter}/{self.patience}\")\n",
        "\n",
        "            if patience_counter >= self.patience:\n",
        "                print(f\"🛑 Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Load best model\n",
        "        if hasattr(self, 'best_model_state'):\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "            print(f\"✅ Loaded best model with validation loss: {best_val_loss:.6f}\")\n",
        "\n",
        "        print(f\"🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Generate predictions with FIXED prediction dataset creation\"\"\"\n",
        "        print(f\"🔮 Generating TFT predictions for {len(X)} samples...\")\n",
        "\n",
        "        if self.model is None or self.training_dataset is None:\n",
        "            raise ValueError(\"Model not trained! Call fit() first.\")\n",
        "\n",
        "        # Filter to known groups\n",
        "        test_groups = set(X['group_id'].unique())\n",
        "        known_groups = test_groups.intersection(self.train_groups)\n",
        "        unknown_groups = test_groups - self.train_groups\n",
        "\n",
        "        print(f\"Known groups: {len(known_groups)}, Unknown groups: {len(unknown_groups)}\")\n",
        "\n",
        "        predictions = np.zeros(len(X))\n",
        "\n",
        "        # Predict for known groups\n",
        "        if len(known_groups) > 0:\n",
        "            known_mask = X['group_id'].isin(known_groups)\n",
        "            known_data = X[known_mask].copy()\n",
        "\n",
        "            try:\n",
        "                print(f\"🚀 Creating prediction dataset for {len(known_data)} samples...\")\n",
        "\n",
        "                # CRITICAL FIX: Add dummy Weekly_Sales column for prediction dataset\n",
        "                print(\"🔧 Adding dummy Weekly_Sales column for prediction...\")\n",
        "                known_data_with_target = known_data.copy()\n",
        "\n",
        "                # Add dummy target column filled with zeros (TFT needs this for dataset creation)\n",
        "                known_data_with_target['Weekly_Sales'] = 0.0\n",
        "\n",
        "                print(f\"Data with dummy target shape: {known_data_with_target.shape}\")\n",
        "                print(f\"Columns: {known_data_with_target.columns.tolist()}\")\n",
        "\n",
        "                # Create prediction dataset with dummy target\n",
        "                prediction_data = TimeSeriesDataSet.from_dataset(\n",
        "                    self.training_dataset,\n",
        "                    known_data_with_target,\n",
        "                    predict=True,\n",
        "                    stop_randomization=True\n",
        "                )\n",
        "\n",
        "                predict_dataloader = prediction_data.to_dataloader(\n",
        "                    train=False,\n",
        "                    batch_size=self.batch_size,\n",
        "                    num_workers=0\n",
        "                )\n",
        "\n",
        "                if len(predict_dataloader) > 0:\n",
        "                    print(f\"🚀 Running TFT inference on {len(predict_dataloader)} batches...\")\n",
        "\n",
        "                    self.model.eval()\n",
        "                    all_predictions = []\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        for batch_idx, batch in enumerate(predict_dataloader):\n",
        "                            try:\n",
        "                                inputs, _ = self.extract_batch_data(batch)\n",
        "\n",
        "                                # DIRECT MODEL PREDICTION\n",
        "                                pred = self.model(inputs)\n",
        "\n",
        "                                # Handle different prediction formats\n",
        "                                if isinstance(pred, dict):\n",
        "                                    pred = pred['prediction']\n",
        "                                elif isinstance(pred, tuple):\n",
        "                                    pred = pred[0]\n",
        "\n",
        "                                # Get median quantile (index 3 for 7 quantiles)\n",
        "                                if pred.dim() == 3:  # [batch, time, quantiles]\n",
        "                                    pred = pred[:, -1, 3]  # Last timestep, median quantile\n",
        "                                elif pred.dim() == 2:  # [batch, quantiles]\n",
        "                                    pred = pred[:, 3]  # Median quantile\n",
        "\n",
        "                                all_predictions.append(pred.cpu().numpy())\n",
        "\n",
        "                                if batch_idx % 20 == 0:\n",
        "                                    print(f\"  Processed batch {batch_idx}/{len(predict_dataloader)}\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                print(f\"Prediction batch {batch_idx} error: {e}\")\n",
        "                                continue\n",
        "\n",
        "                    if all_predictions:\n",
        "                        tft_preds = np.concatenate(all_predictions)\n",
        "                        tft_preds = np.clip(tft_preds, 10, 100000)\n",
        "\n",
        "                        if len(tft_preds) == len(known_data):\n",
        "                            predictions[known_mask] = tft_preds\n",
        "                            print(f\"✅ TFT predictions generated for {len(tft_preds)} samples\")\n",
        "                        else:\n",
        "                            print(f\"⚠️ Prediction length mismatch: {len(tft_preds)} vs {len(known_data)}\")\n",
        "                            # Use partial predictions if available\n",
        "                            min_len = min(len(tft_preds), len(known_data))\n",
        "                            predictions[known_mask][:min_len] = tft_preds[:min_len]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ TFT prediction error: {e}\")\n",
        "                import traceback\n",
        "                print(f\"Full traceback: {traceback.format_exc()}\")\n",
        "\n",
        "        # Fallback for remaining samples\n",
        "        remaining_mask = predictions == 0\n",
        "        if remaining_mask.sum() > 0:\n",
        "            print(f\"Using fallback for {remaining_mask.sum()} samples\")\n",
        "            fallback_preds = self._intelligent_fallback(X[remaining_mask])\n",
        "            predictions[remaining_mask] = fallback_preds\n",
        "\n",
        "        print(f\"✅ ALL PREDICTIONS GENERATED - TFT: {(~remaining_mask).sum()}, Fallback: {remaining_mask.sum()}\")\n",
        "        return predictions\n",
        "\n",
        "    def _intelligent_fallback(self, X):\n",
        "        \"\"\"Smart fallback predictions\"\"\"\n",
        "\n",
        "        base_values = {\n",
        "            'High_Volume': 20000,\n",
        "            'Volatile': 12000,\n",
        "            'Seasonal': 8000,\n",
        "            'Standard': 15000\n",
        "        }\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for _, row in X.iterrows():\n",
        "            # Base prediction from department category\n",
        "            base_pred = base_values.get(row.get('DeptCategory', 'Standard'), 15000)\n",
        "\n",
        "            # Use lag features if available\n",
        "            if 'sales_lag_1' in row and pd.notna(row['sales_lag_1']) and row['sales_lag_1'] > 0:\n",
        "                base_pred = 0.6 * base_pred + 0.4 * row['sales_lag_1']\n",
        "\n",
        "            if 'sales_rolling_mean_4' in row and pd.notna(row['sales_rolling_mean_4']):\n",
        "                base_pred = 0.5 * base_pred + 0.5 * row['sales_rolling_mean_4']\n",
        "\n",
        "            # Apply seasonal adjustments\n",
        "            seasonal_mult = 1.0\n",
        "\n",
        "            if row.get('IsQ4', 0) == 1:\n",
        "                seasonal_mult *= 1.2\n",
        "\n",
        "            if row.get('IsBackToSchool', 0) == 1:\n",
        "                seasonal_mult *= 1.1\n",
        "\n",
        "            if row.get('HasAnyPromo', 0) == 1:\n",
        "                seasonal_mult *= 1.05\n",
        "\n",
        "            final_pred = base_pred * seasonal_mult\n",
        "            final_pred = np.clip(final_pred, 10, 80000)\n",
        "\n",
        "            predictions.append(final_pred)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL TRAINING FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def train_final_tft_model():\n",
        "    \"\"\"Final training pipeline with all fixes\"\"\"\n",
        "\n",
        "    wandb.init(project=\"walmart-final-tft\", name=\"final_fixed_prediction\")\n",
        "\n",
        "    print(\"🚀 === FINAL TFT TRAINING (ALL FIXES APPLIED) ===\")\n",
        "\n",
        "    # Load data\n",
        "    print(\"📊 Loading data...\")\n",
        "    train_df = pd.read_csv(\"/content/train.csv\")\n",
        "    test_df = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "    # Convert dates\n",
        "    for df in [train_df, test_df]:\n",
        "        if 'Date' in df.columns:\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    print(f\"Data shapes - Train: {train_df.shape}, Test: {test_df.shape}\")\n",
        "\n",
        "    # Feature engineering\n",
        "    print(\"\\n🔧 === FEATURE ENGINEERING ===\")\n",
        "    feature_engineer = EnhancedWalmartFeatureEngineer()\n",
        "    feature_engineer.fit(train_df)\n",
        "\n",
        "    processed_train = feature_engineer.transform(train_df)\n",
        "    processed_test = feature_engineer.transform(test_df)\n",
        "\n",
        "    # Add lag features\n",
        "    print(\"⏰ Adding lag features...\")\n",
        "    processed_train = create_fast_lag_features(processed_train, include_target_lags=True)\n",
        "    processed_test = create_fast_lag_features(processed_test, include_target_lags=False)\n",
        "\n",
        "    print(f\"Final shapes - Train: {processed_train.shape}, Test: {processed_test.shape}\")\n",
        "\n",
        "    # Simple train/val split by time\n",
        "    print(\"\\n✂️ === VALIDATION SPLIT ===\")\n",
        "    cutoff_date = processed_train['Date'].quantile(0.8)\n",
        "\n",
        "    train_data = processed_train[processed_train['Date'] <= cutoff_date].copy()\n",
        "    val_data = processed_train[processed_train['Date'] > cutoff_date].copy()\n",
        "\n",
        "    print(f\"Train: {len(train_data):,}, Val: {len(val_data):,}\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\n🤖 === TRAINING FINAL TFT ===\")\n",
        "    model = FinalTFTModel(\n",
        "        max_prediction_length=8,\n",
        "        max_encoder_length=20,\n",
        "        hidden_size=32,\n",
        "        attention_head_size=1,\n",
        "        learning_rate=0.01,\n",
        "        max_epochs=2,\n",
        "        patience=5,\n",
        "        batch_size=256  # Smaller batch size for stability\n",
        "    )\n",
        "\n",
        "    # Train with all fixes\n",
        "    model.fit(train_data)\n",
        "\n",
        "    # Validate\n",
        "    print(\"\\n📊 === VALIDATION ===\")\n",
        "    val_pred = model.predict(val_data)\n",
        "    val_actual = val_data['Weekly_Sales'].values\n",
        "\n",
        "    val_mae = np.mean(np.abs(val_pred - val_actual))\n",
        "    val_wmae = np.sum(np.abs(val_pred - val_actual)) / np.sum(val_actual)\n",
        "\n",
        "    print(f\"✅ Validation MAE: {val_mae:,.2f}\")\n",
        "    print(f\"✅ Validation WMAE: {val_wmae:.4f}\")\n",
        "\n",
        "    wandb.log({\"val_mae\": val_mae, \"val_wmae\": val_wmae})\n",
        "\n",
        "    # Final training on all data\n",
        "    print(\"\\n🎯 === FINAL TRAINING ON ALL DATA ===\")\n",
        "    final_model = FinalTFTModel(\n",
        "        max_prediction_length=8,\n",
        "        max_encoder_length=20,\n",
        "        hidden_size=32,\n",
        "        attention_head_size=1,\n",
        "        learning_rate=0.01,\n",
        "        max_epochs=2,\n",
        "        patience=4,\n",
        "        batch_size=256\n",
        "    )\n",
        "\n",
        "    final_model.fit(processed_train)\n",
        "\n",
        "    # Generate predictions\n",
        "    print(\"\\n🔮 === GENERATING FINAL TFT PREDICTIONS ===\")\n",
        "    test_predictions = final_model.predict(processed_test)\n",
        "\n",
        "    # Create submission\n",
        "    submission = pd.DataFrame({\n",
        "        'Id': range(1, len(test_predictions) + 1),\n",
        "        'Weekly_Sales': test_predictions\n",
        "    })\n",
        "\n",
        "    submission_file = \"final_tft_submission.csv\"\n",
        "    submission.to_csv(submission_file, index=False)\n",
        "\n",
        "    print(f\"\\n🎉 === FINAL SUCCESS WITH TFT! ===\")\n",
        "    print(f\"✅ SUBMISSION SAVED: {submission_file}\")\n",
        "    print(f\"📈 Prediction stats:\")\n",
        "    print(f\"  Mean: {test_predictions.mean():,.2f}\")\n",
        "    print(f\"  Std: {test_predictions.std():,.2f}\")\n",
        "    print(f\"  Min: {test_predictions.min():,.2f}\")\n",
        "    print(f\"  Max: {test_predictions.max():,.2f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"test_pred_mean\": test_predictions.mean(),\n",
        "        \"test_pred_std\": test_predictions.std(),\n",
        "        \"test_pred_min\": test_predictions.min(),\n",
        "        \"test_pred_max\": test_predictions.max()\n",
        "    })\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    return final_model, submission_file\n",
        "\n",
        "# Run the final training\n",
        "print(\"🚀 FINAL TFT SETUP COMPLETE!\")\n",
        "model, submission_file = train_final_tft_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "er4a-hNG-ik-",
        "outputId": "f8242873-ea45-4372-9144-fc7fb5265e53"
      },
      "id": "er4a-hNG-ik-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 FINAL TFT SETUP COMPLETE!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdshan21\u001b[0m (\u001b[33mdshan21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250802_205518-l8vjtoqe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft/runs/l8vjtoqe' target=\"_blank\">final_fixed_prediction</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft/runs/l8vjtoqe' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft/runs/l8vjtoqe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 === FINAL TFT TRAINING (ALL FIXES APPLIED) ===\n",
            "📊 Loading data...\n",
            "Data shapes - Train: (421570, 5), Test: (115064, 4)\n",
            "\n",
            "🔧 === FEATURE ENGINEERING ===\n",
            "Learning advanced retail patterns...\n",
            "Starting transform with shape: (421570, 5)\n",
            "Enhanced features created. Final shape: (421570, 50)\n",
            "Starting transform with shape: (115064, 4)\n",
            "Enhanced features created. Final shape: (115064, 49)\n",
            "⏰ Adding lag features...\n",
            "Creating FAST lag features...\n",
            "Creating target-based lag features...\n",
            "Fast lag features created in 10.5 seconds. Shape: (421570, 62)\n",
            "Creating FAST lag features...\n",
            "Creating lag features from historical patterns (no target)...\n",
            "Fast lag features created in 0.3 seconds. Shape: (115064, 61)\n",
            "Final shapes - Train: (421570, 62), Test: (115064, 61)\n",
            "\n",
            "✂️ === VALIDATION SPLIT ===\n",
            "Train: 338,738, Val: 82,832\n",
            "\n",
            "🤖 === TRAINING FINAL TFT ===\n",
            "🚀 Training FINAL TFT with proper handling...\n",
            "Groups with 33+ samples: 3003\n",
            "Limited to top 1000 groups\n",
            "Training on 1000 groups with 115000 samples\n",
            "TFT Dataset - Static: 3, Time cats: 2, Time reals: 11\n",
            "✅ Training dataset created\n",
            "✅ Validation dataset created\n",
            "✅ Data loaders created - Train: 507, Val: 4\n",
            "✅ TFT model created - 114,122 parameters\n",
            "🔥 Starting training loop...\n",
            "Epoch 1/2 - Training...\n",
            "  Batch 0/507, Loss: 5583.361328\n",
            "  Batch 50/507, Loss: 1495.462891\n",
            "  Batch 100/507, Loss: 1518.289795\n",
            "  Batch 150/507, Loss: 1216.365356\n",
            "  Batch 200/507, Loss: 1303.589722\n",
            "  Batch 250/507, Loss: 1057.184082\n",
            "  Batch 300/507, Loss: 1027.668945\n",
            "  Batch 350/507, Loss: 997.103882\n",
            "  Batch 400/507, Loss: 850.309387\n",
            "  Batch 450/507, Loss: 1181.403442\n",
            "  Batch 500/507, Loss: 736.889404\n",
            "Epoch 1/2 - Validating...\n",
            "✅ Epoch 1 Complete - Train Loss: 1172.214117, Val Loss: 669.883774\n",
            "  🎯 New best validation loss: 669.883774\n",
            "Epoch 2/2 - Training...\n",
            "  Batch 0/507, Loss: 810.492310\n",
            "  Batch 50/507, Loss: 729.965332\n",
            "  Batch 100/507, Loss: 695.359741\n",
            "  Batch 150/507, Loss: 607.008789\n",
            "  Batch 200/507, Loss: 868.054932\n",
            "  Batch 250/507, Loss: 708.029358\n",
            "  Batch 300/507, Loss: 906.747864\n",
            "  Batch 350/507, Loss: 783.916504\n",
            "  Batch 400/507, Loss: 683.447021\n",
            "  Batch 450/507, Loss: 743.353516\n",
            "  Batch 500/507, Loss: 774.140564\n",
            "Epoch 2/2 - Validating...\n",
            "✅ Epoch 2 Complete - Train Loss: 746.602858, Val Loss: 661.569809\n",
            "  🎯 New best validation loss: 661.569809\n",
            "✅ Loaded best model with validation loss: 661.569809\n",
            "🎉 TRAINING COMPLETED SUCCESSFULLY!\n",
            "\n",
            "📊 === VALIDATION ===\n",
            "🔮 Generating TFT predictions for 82832 samples...\n",
            "Known groups: 1000, Unknown groups: 2165\n",
            "🚀 Creating prediction dataset for 27985 samples...\n",
            "🔧 Adding dummy Weekly_Sales column for prediction...\n",
            "Data with dummy target shape: (27985, 62)\n",
            "Columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday_x', 'DayOfWeek', 'DayOfMonth', 'DayOfYear', 'WeekOfYear', 'Year', 'Month', 'Quarter', 'Week', 'time_idx', 'group_id', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y', 'Type', 'Size', 'IsHoliday', 'HasMarkDown1', 'HasMarkDown2', 'HasMarkDown3', 'HasMarkDown4', 'HasMarkDown5', 'TotalMarkDown', 'HasAnyPromo', 'PromoIntensity', 'IsQ4', 'IsQ1', 'IsBackToSchool', 'TempCategory', 'StoreSize_Cat', 'DeptCategory', 'Month_sin', 'Month_cos', 'Week_sin', 'Week_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Dept_HistoricalMean', 'Store_HistoricalMean', 'sales_lag_1', 'sales_lag_2', 'sales_lag_4', 'sales_lag_8', 'sales_lag_12', 'sales_rolling_mean_4', 'sales_rolling_std_4', 'sales_rolling_mean_8', 'sales_rolling_std_8', 'sales_rolling_mean_12', 'sales_rolling_std_12', 'sales_trend_4w']\n",
            "🚀 Running TFT inference on 4 batches...\n",
            "  Processed batch 0/4\n",
            "⚠️ Prediction length mismatch: 1000 vs 27985\n",
            "Using fallback for 82832 samples\n",
            "✅ ALL PREDICTIONS GENERATED - TFT: 0, Fallback: 82832\n",
            "✅ Validation MAE: 4,536.93\n",
            "✅ Validation WMAE: 0.2874\n",
            "\n",
            "🎯 === FINAL TRAINING ON ALL DATA ===\n",
            "🚀 Training FINAL TFT with proper handling...\n",
            "Groups with 33+ samples: 3044\n",
            "Limited to top 1000 groups\n",
            "Training on 1000 groups with 143000 samples\n",
            "TFT Dataset - Static: 3, Time cats: 2, Time reals: 11\n",
            "✅ Training dataset created\n",
            "✅ Validation dataset created\n",
            "✅ Data loaders created - Train: 617, Val: 4\n",
            "✅ TFT model created - 114,113 parameters\n",
            "🔥 Starting training loop...\n",
            "Epoch 1/2 - Training...\n",
            "  Batch 0/617, Loss: 4314.887695\n",
            "  Batch 50/617, Loss: 1519.095825\n",
            "  Batch 100/617, Loss: 1334.232422\n",
            "  Batch 150/617, Loss: 1511.864502\n",
            "  Batch 200/617, Loss: 1210.112061\n",
            "  Batch 250/617, Loss: 1206.208008\n",
            "  Batch 300/617, Loss: 1305.489624\n",
            "  Batch 350/617, Loss: 1180.348999\n",
            "  Batch 400/617, Loss: 1041.729736\n",
            "  Batch 450/617, Loss: 1057.514282\n",
            "  Batch 500/617, Loss: 832.630676\n",
            "  Batch 550/617, Loss: 1269.250977\n",
            "  Batch 600/617, Loss: 1051.912476\n",
            "Epoch 1/2 - Validating...\n",
            "✅ Epoch 1 Complete - Train Loss: 1239.939919, Val Loss: 852.843765\n",
            "  🎯 New best validation loss: 852.843765\n",
            "Epoch 2/2 - Training...\n",
            "  Batch 0/617, Loss: 916.142029\n",
            "  Batch 50/617, Loss: 790.451782\n",
            "  Batch 100/617, Loss: 824.745789\n",
            "  Batch 150/617, Loss: 849.188110\n",
            "  Batch 200/617, Loss: 779.578186\n",
            "  Batch 250/617, Loss: 935.874207\n",
            "  Batch 300/617, Loss: 674.951599\n",
            "  Batch 350/617, Loss: 643.621643\n",
            "  Batch 400/617, Loss: 532.041138\n",
            "  Batch 450/617, Loss: 660.128540\n",
            "  Batch 500/617, Loss: 581.623230\n",
            "  Batch 550/617, Loss: 600.721741\n",
            "  Batch 600/617, Loss: 656.683594\n",
            "Epoch 2/2 - Validating...\n",
            "✅ Epoch 2 Complete - Train Loss: 752.746502, Val Loss: 474.472527\n",
            "  🎯 New best validation loss: 474.472527\n",
            "✅ Loaded best model with validation loss: 474.472527\n",
            "🎉 TRAINING COMPLETED SUCCESSFULLY!\n",
            "\n",
            "🔮 === GENERATING FINAL TFT PREDICTIONS ===\n",
            "🔮 Generating TFT predictions for 115064 samples...\n",
            "Known groups: 1000, Unknown groups: 2169\n",
            "🚀 Creating prediction dataset for 38995 samples...\n",
            "🔧 Adding dummy Weekly_Sales column for prediction...\n",
            "Data with dummy target shape: (38995, 62)\n",
            "Columns: ['Store', 'Dept', 'Date', 'IsHoliday_x', 'DayOfWeek', 'DayOfMonth', 'DayOfYear', 'WeekOfYear', 'Year', 'Month', 'Quarter', 'Week', 'time_idx', 'group_id', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y', 'Type', 'Size', 'IsHoliday', 'HasMarkDown1', 'HasMarkDown2', 'HasMarkDown3', 'HasMarkDown4', 'HasMarkDown5', 'TotalMarkDown', 'HasAnyPromo', 'PromoIntensity', 'IsQ4', 'IsQ1', 'IsBackToSchool', 'TempCategory', 'StoreSize_Cat', 'DeptCategory', 'Month_sin', 'Month_cos', 'Week_sin', 'Week_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Dept_HistoricalMean', 'Store_HistoricalMean', 'sales_lag_1', 'sales_lag_2', 'sales_lag_4', 'sales_lag_8', 'sales_lag_12', 'sales_rolling_mean_4', 'sales_rolling_mean_8', 'sales_rolling_mean_12', 'sales_rolling_std_4', 'sales_rolling_std_8', 'sales_rolling_std_12', 'sales_trend_4w', 'Weekly_Sales']\n",
            "🚀 Running TFT inference on 4 batches...\n",
            "  Processed batch 0/4\n",
            "⚠️ Prediction length mismatch: 1000 vs 38995\n",
            "Using fallback for 115064 samples\n",
            "✅ ALL PREDICTIONS GENERATED - TFT: 0, Fallback: 115064\n",
            "\n",
            "🎉 === FINAL SUCCESS WITH TFT! ===\n",
            "✅ SUBMISSION SAVED: final_tft_submission.csv\n",
            "📈 Prediction stats:\n",
            "  Mean: 15,624.69\n",
            "  Std: 4,338.43\n",
            "  Min: 8,000.00\n",
            "  Max: 25,200.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_pred_max</td><td>▁</td></tr><tr><td>test_pred_mean</td><td>▁</td></tr><tr><td>test_pred_min</td><td>▁</td></tr><tr><td>test_pred_std</td><td>▁</td></tr><tr><td>val_mae</td><td>▁</td></tr><tr><td>val_wmae</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_pred_max</td><td>25200</td></tr><tr><td>test_pred_mean</td><td>15624.68843</td></tr><tr><td>test_pred_min</td><td>8000</td></tr><tr><td>test_pred_std</td><td>4338.43436</td></tr><tr><td>val_mae</td><td>4536.92757</td></tr><tr><td>val_wmae</td><td>0.28741</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">final_fixed_prediction</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft/runs/l8vjtoqe' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft/runs/l8vjtoqe</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-final-tft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250802_205518-l8vjtoqe/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def fix_id_column_type(filename: str, output_filename: str = None):\n",
        "    # Load the CSV\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    # Check if 'Id' column exists\n",
        "    if 'Id' not in df.columns:\n",
        "        raise ValueError(\"CSV file does not contain 'Id' column\")\n",
        "\n",
        "    # Convert 'Id' column to string\n",
        "    df['Id'] = df['Id'].astype(str)\n",
        "\n",
        "    # Determine output filename\n",
        "    if output_filename is None:\n",
        "        output_filename = filename.replace('.csv', '_fixed.csv')\n",
        "\n",
        "    # Save the fixed CSV\n",
        "    df.to_csv(output_filename, index=False)\n",
        "    print(f\"Fixed CSV saved to {output_filename}\")\n",
        "\n",
        "# Example usage:\n",
        "fix_id_column_type(\"fixed_tft_submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9X-mkNZiloR",
        "outputId": "b6d9b8dd-01f1-494c-f348-689f2787ea37"
      },
      "id": "R9X-mkNZiloR",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed CSV saved to fixed_tft_submission_fixed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAFE SUBMISSION CREATION USING SAMPLE SUBMISSION FORMAT\n",
        "\n",
        "def create_safe_submission(test_predictions):\n",
        "    \"\"\"Create submission using sample submission format\"\"\"\n",
        "\n",
        "    print(f\"🔧 Creating safe submission for {len(test_predictions)} predictions...\")\n",
        "\n",
        "    try:\n",
        "        # Try to load sample submission for format reference\n",
        "        sample_sub = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "        print(f\"Sample submission shape: {sample_sub.shape}\")\n",
        "        print(f\"Sample submission columns: {sample_sub.columns.tolist()}\")\n",
        "        print(f\"Sample submission Id type: {sample_sub['Id'].dtype}\")\n",
        "        print(f\"First few sample Ids: {sample_sub['Id'].head().tolist()}\")\n",
        "\n",
        "        # Create submission matching sample format exactly\n",
        "        if len(test_predictions) == len(sample_sub):\n",
        "            # Perfect match - use sample submission structure\n",
        "            submission = sample_sub.copy()\n",
        "            submission['Weekly_Sales'] = test_predictions\n",
        "            print(\"✅ Using exact sample submission format\")\n",
        "        else:\n",
        "            print(f\"⚠️ Length mismatch: predictions={len(test_predictions)}, sample={len(sample_sub)}\")\n",
        "            # Create submission with sample format but our length\n",
        "            submission = pd.DataFrame({\n",
        "                'Id': sample_sub['Id'].iloc[:len(test_predictions)].copy(),\n",
        "                'Weekly_Sales': test_predictions\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not load sample submission: {e}\")\n",
        "\n",
        "        # Fallback: create submission with string IDs\n",
        "        submission = pd.DataFrame({\n",
        "            'Id': [str(i) for i in range(1, len(test_predictions) + 1)],\n",
        "            'Weekly_Sales': test_predictions\n",
        "        })\n",
        "\n",
        "    # Ensure proper data types\n",
        "    submission['Id'] = submission['Id'].astype(str)\n",
        "    submission['Weekly_Sales'] = submission['Weekly_Sales'].astype(float)\n",
        "\n",
        "    # Validate submission\n",
        "    print(f\"Final submission shape: {submission.shape}\")\n",
        "    print(f\"Final Id type: {submission['Id'].dtype}\")\n",
        "    print(f\"Final Weekly_Sales type: {submission['Weekly_Sales'].dtype}\")\n",
        "    print(f\"First few rows:\")\n",
        "    print(submission.head())\n",
        "\n",
        "    return submission\n",
        "\n",
        "# Also check our test data length\n",
        "print(f\"🔍 DEBUGGING SUBMISSION SIZE ISSUE:\")\n",
        "print(f\"Test predictions length: {len(test_predictions)}\")\n",
        "\n",
        "# Load original test data to compare\n",
        "original_test = pd.read_csv(\"/content/test.csv\")\n",
        "print(f\"Original test data shape: {original_test.shape}\")\n",
        "\n",
        "# Check if we lost rows during processing\n",
        "print(f\"Processed test data shape: {processed_test.shape}\")\n",
        "\n",
        "# Create safe submission\n",
        "submission = create_safe_submission(test_predictions)\n",
        "\n",
        "# Save with validation\n",
        "submission_file = \"safe_tft_submission.csv\"\n",
        "submission.to_csv(submission_file, index=False)\n",
        "\n",
        "# Double-check the saved file\n",
        "saved_check = pd.read_csv(submission_file)\n",
        "print(f\"\\n✅ SAVED FILE VALIDATION:\")\n",
        "print(f\"Saved file shape: {saved_check.shape}\")\n",
        "print(f\"Saved Id type: {saved_check['Id'].dtype}\")\n",
        "print(f\"File size: {os.path.getsize(submission_file) / (1024*1024):.2f} MB\")\n",
        "\n",
        "print(f\"✅ SAFE SUBMISSION SAVED: {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "u1AxVEiQqNoH",
        "outputId": "a1a05855-3be5-41f9-b783-022f3381e76d"
      },
      "id": "u1AxVEiQqNoH",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 DEBUGGING SUBMISSION SIZE ISSUE:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_predictions' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3239525216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Also check our test data length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🔍 DEBUGGING SUBMISSION SIZE ISSUE:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test predictions length: {len(test_predictions)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Load original test data to compare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_predictions' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}