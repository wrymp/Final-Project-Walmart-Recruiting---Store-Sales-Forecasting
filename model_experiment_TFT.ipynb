{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrymp/Final-Project-Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_TFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "title_cell"
      },
      "cell_type": "markdown",
      "source": [
        "# Temporal Fusion Transformer Implementation for Walmart Sales Forecasting\n",
        "\n",
        "This notebook implements Temporal Fusion Transformer (TFT) for Walmart sales forecasting following the exact pipeline structure from N-BEATS experiments."
      ],
      "id": "title_cell"
    },
    {
      "metadata": {
        "id": "setup_cell"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup & Data Download"
      ],
      "id": "setup_cell"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_mount",
        "outputId": "61e6903c-f9c3-4e1d-ee56-0cc61cdeb1ed"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "!pip install wandb -q\n",
        "!pip install kaggle -q\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "execution_count": 1,
      "id": "setup_mount"
    },
    {
      "metadata": {
        "id": "data_download"
      },
      "cell_type": "code",
      "source": [
        "# Uncomment to download data if needed\n",
        "# !kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "# !unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# !unzip /content/train.csv.zip\n",
        "# !unzip /content/test.csv.zip\n",
        "# !unzip /content/features.csv.zip\n",
        "# !unzip /content/sampleSubmission.csv.zip"
      ],
      "outputs": [],
      "execution_count": 2,
      "id": "data_download"
    },
    {
      "metadata": {
        "id": "imports_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup & Imports"
      ],
      "id": "imports_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_imports",
        "outputId": "58a0e17c-4269-4847-dd56-73356b3d12a7"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import gc\n",
        "import os\n",
        "import pickle\n",
        "import cloudpickle\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 2,
      "id": "setup_imports"
    },
    {
      "metadata": {
        "id": "wandb_setup"
      },
      "cell_type": "markdown",
      "source": [
        "# Wandb Initialization"
      ],
      "id": "wandb_setup"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "wandb_init",
        "outputId": "21b5ccf5-0819-4bf1-e4a0-29cb09c7c5af"
      },
      "cell_type": "code",
      "source": [
        "# Initialize Wandb project\n",
        "wandb.login()\n",
        "try:\n",
        "    wandb.init(\n",
        "        project=\"walmart-sales-forecasting\",\n",
        "        name=\"TFT_Initial_Setup\",\n",
        "        config={\n",
        "            \"model_type\": \"TFT\",\n",
        "            \"framework\": \"PyTorch\",\n",
        "            \"device\": str(device),\n",
        "            \"random_seed\": 42\n",
        "        }\n",
        "    )\n",
        "    print(\"✓ Wandb initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Wandb initialization failed: {e}\")\n",
        "    print(\"Continuing without wandb logging...\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqitiashvili13\u001b[0m (\u001b[33mdshan21-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_181702-u2qqcb9r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/u2qqcb9r' target=\"_blank\">TFT_Initial_Setup</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/u2qqcb9r' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/u2qqcb9r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Wandb initialized successfully!\n"
          ]
        }
      ],
      "execution_count": 3,
      "id": "wandb_init"
    },
    {
      "metadata": {
        "id": "data_loading"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "id": "data_loading"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "data_loading_code",
        "outputId": "1e922a56-b41c-4cdc-d806-0f0d33ba60dc"
      },
      "cell_type": "code",
      "source": [
        "# Load Walmart datasets\n",
        "print(\"Loading Walmart datasets...\")\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/walmart-recruiting-store-sales-forecasting/train.csv/train.csv')\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/walmart-recruiting-store-sales-forecasting/test.csv/test.csv')\n",
        "    stores_df = pd.read_csv('/content/drive/MyDrive/walmart-recruiting-store-sales-forecasting/stores.csv')\n",
        "    features_df = pd.read_csv('/content/drive/MyDrive/walmart-recruiting-store-sales-forecasting/features.csv/features.csv')\n",
        "\n",
        "    print(f\"✓ Train data shape: {train_df.shape}\")\n",
        "    print(f\"✓ Test data shape: {test_df.shape}\")\n",
        "    print(f\"✓ Stores data shape: {stores_df.shape}\")\n",
        "    print(f\"✓ Features data shape: {features_df.shape}\")\n",
        "\n",
        "    # Log basic dataset info\n",
        "    wandb.log({\n",
        "        \"train_samples\": len(train_df),\n",
        "        \"test_samples\": len(test_df),\n",
        "        \"num_stores\": stores_df['Store'].nunique(),\n",
        "        \"num_departments\": train_df['Dept'].nunique(),\n",
        "        \"date_range_train\": f\"{train_df['Date'].min()} to {train_df['Date'].max()}\"\n",
        "    })\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error loading data: {e}\")\n",
        "    print(\"Please ensure data files are in the correct directory\")\n",
        "    raise"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Walmart datasets...\n",
            "✓ Train data shape: (421570, 5)\n",
            "✓ Test data shape: (115064, 4)\n",
            "✓ Stores data shape: (45, 3)\n",
            "✓ Features data shape: (8190, 12)\n"
          ]
        }
      ],
      "execution_count": 4,
      "id": "data_loading_code"
    },
    {
      "metadata": {
        "id": "exploration_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Exploration Run"
      ],
      "id": "exploration_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "exploration_code",
        "outputId": "8dbd143e-85a5-4c72-a932-572925039eb6"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for exploration\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"TFT_Exploration\",\n",
        "    config={\"stage\": \"exploration\"}\n",
        ")\n",
        "\n",
        "print(\"\\n=== DATA EXPLORATION ===\")\n",
        "\n",
        "# Convert date columns\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nTrain Data Info:\")\n",
        "print(f\"Date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "print(f\"Unique stores: {train_df['Store'].nunique()}\")\n",
        "print(f\"Unique departments: {train_df['Dept'].nunique()}\")\n",
        "print(f\"Total store-dept combinations: {train_df.groupby(['Store', 'Dept']).ngroups}\")\n",
        "\n",
        "# Sales statistics\n",
        "print(f\"\\nSales Statistics:\")\n",
        "print(f\"Mean weekly sales: ${train_df['Weekly_Sales'].mean():,.2f}\")\n",
        "print(f\"Median weekly sales: ${train_df['Weekly_Sales'].median():,.2f}\")\n",
        "print(f\"Min weekly sales: ${train_df['Weekly_Sales'].min():,.2f}\")\n",
        "print(f\"Max weekly sales: ${train_df['Weekly_Sales'].max():,.2f}\")\n",
        "\n",
        "# Holiday impact\n",
        "holiday_sales = train_df.groupby('IsHoliday')['Weekly_Sales'].agg(['mean', 'count'])\n",
        "print(f\"\\nHoliday Impact:\")\n",
        "print(holiday_sales)\n",
        "\n",
        "# Store types\n",
        "store_types = stores_df['Type'].value_counts()\n",
        "print(f\"\\nStore Types:\")\n",
        "print(store_types)\n",
        "\n",
        "# Missing values in features\n",
        "print(f\"\\nMissing Values in Features:\")\n",
        "missing_pct = (features_df.isnull().sum() / len(features_df)) * 100\n",
        "print(missing_pct[missing_pct > 0].sort_values(ascending=False))\n",
        "\n",
        "# TFT-specific analysis\n",
        "print(f\"\\nTFT-Specific Analysis:\")\n",
        "print(f\"Time series length per store-dept: {train_df.groupby(['Store', 'Dept']).size().describe()}\")\n",
        "\n",
        "# Log exploration metrics\n",
        "wandb.log({\n",
        "    \"unique_stores\": train_df['Store'].nunique(),\n",
        "    \"unique_departments\": train_df['Dept'].nunique(),\n",
        "    \"total_timeseries\": train_df.groupby(['Store', 'Dept']).ngroups,\n",
        "    \"avg_weekly_sales\": train_df['Weekly_Sales'].mean(),\n",
        "    \"median_weekly_sales\": train_df['Weekly_Sales'].median(),\n",
        "    \"sales_std\": train_df['Weekly_Sales'].std(),\n",
        "    \"holiday_sales_boost\": holiday_sales.loc[True, 'mean'] / holiday_sales.loc[False, 'mean'],\n",
        "    \"missing_markdown1_pct\": missing_pct['MarkDown1'],\n",
        "    \"missing_markdown2_pct\": missing_pct['MarkDown2'],\n",
        "    \"missing_markdown3_pct\": missing_pct['MarkDown3'],\n",
        "    \"missing_markdown4_pct\": missing_pct['MarkDown4'],\n",
        "    \"missing_markdown5_pct\": missing_pct['MarkDown5'],\n",
        "    \"avg_timeseries_length\": train_df.groupby(['Store', 'Dept']).size().mean()\n",
        "})\n",
        "\n",
        "print(\"\\n✓ Exploration completed and logged to wandb\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>num_departments</td><td>▁</td></tr><tr><td>num_stores</td><td>▁</td></tr><tr><td>test_samples</td><td>▁</td></tr><tr><td>train_samples</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>date_range_train</td><td>2010-02-05 to 2012-1...</td></tr><tr><td>num_departments</td><td>81</td></tr><tr><td>num_stores</td><td>45</td></tr><tr><td>test_samples</td><td>115064</td></tr><tr><td>train_samples</td><td>421570</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">TFT_Initial_Setup</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/u2qqcb9r' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/u2qqcb9r</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_181702-u2qqcb9r/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_181713-ec1vmor2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/ec1vmor2' target=\"_blank\">TFT_Exploration</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/ec1vmor2' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/ec1vmor2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA EXPLORATION ===\n",
            "\n",
            "Train Data Info:\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Unique stores: 45\n",
            "Unique departments: 81\n",
            "Total store-dept combinations: 3331\n",
            "\n",
            "Sales Statistics:\n",
            "Mean weekly sales: $15,981.26\n",
            "Median weekly sales: $7,612.03\n",
            "Min weekly sales: $-4,988.94\n",
            "Max weekly sales: $693,099.36\n",
            "\n",
            "Holiday Impact:\n",
            "                   mean   count\n",
            "IsHoliday                      \n",
            "False      15901.445069  391909\n",
            "True       17035.823187   29661\n",
            "\n",
            "Store Types:\n",
            "Type\n",
            "A    22\n",
            "B    17\n",
            "C     6\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Missing Values in Features:\n",
            "MarkDown2       64.334554\n",
            "MarkDown4       57.704518\n",
            "MarkDown3       55.885226\n",
            "MarkDown1       50.769231\n",
            "MarkDown5       50.549451\n",
            "CPI              7.142857\n",
            "Unemployment     7.142857\n",
            "dtype: float64\n",
            "\n",
            "TFT-Specific Analysis:\n",
            "Time series length per store-dept: count    3331.000000\n",
            "mean      126.559592\n",
            "std        40.212763\n",
            "min         1.000000\n",
            "25%       143.000000\n",
            "50%       143.000000\n",
            "75%       143.000000\n",
            "max       143.000000\n",
            "dtype: float64\n",
            "\n",
            "✓ Exploration completed and logged to wandb\n"
          ]
        }
      ],
      "execution_count": 5,
      "id": "exploration_code"
    },
    {
      "metadata": {
        "id": "transformers_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Custom Transformers for TFT Pipeline"
      ],
      "id": "transformers_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "transformers_code",
        "outputId": "a0b1aaa8-886c-49b0-dd3f-1d1bfe0e082a"
      },
      "cell_type": "code",
      "source": [
        "class TFTDataProcessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Processes raw Walmart data into TFT format with static and time-varying features\"\"\"\n",
        "\n",
        "    def __init__(self, lookback_window=52, forecast_horizon=1):\n",
        "        self.lookback_window = lookback_window\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.store_dept_combinations = None\n",
        "        self.date_range = None\n",
        "        self.scalers = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Learn the store-department combinations, date range, and scalers\"\"\"\n",
        "        self.store_dept_combinations = X.groupby(['Store', 'Dept']).size().index.tolist()\n",
        "        self.date_range = sorted(X['Date'].unique())\n",
        "\n",
        "        # Fit scalers for numerical features\n",
        "        numerical_cols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment'] + \\\n",
        "                        [col for col in X.columns if 'MarkDown' in col]\n",
        "\n",
        "        for col in numerical_cols:\n",
        "            if col in X.columns:\n",
        "                scaler = StandardScaler()\n",
        "                valid_data = X[col].dropna()\n",
        "                if len(valid_data) > 0:\n",
        "                    scaler.fit(valid_data.values.reshape(-1, 1))\n",
        "                    self.scalers[col] = scaler\n",
        "\n",
        "        print(f\"Found {len(self.store_dept_combinations)} store-dept combinations\")\n",
        "        print(f\"Date range: {self.date_range[0]} to {self.date_range[-1]}\")\n",
        "        print(f\"Fitted scalers for: {list(self.scalers.keys())}\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data into TFT format with static and time-varying features\"\"\"\n",
        "        sequences = []\n",
        "        targets = []\n",
        "        static_features = []\n",
        "        metadata = []\n",
        "\n",
        "        for store, dept in self.store_dept_combinations:\n",
        "            # Get time series for this store-dept combination\n",
        "            series_data = X[(X['Store'] == store) & (X['Dept'] == dept)].copy()\n",
        "            series_data = series_data.sort_values('Date')\n",
        "\n",
        "            if len(series_data) < self.lookback_window + self.forecast_horizon:\n",
        "                continue\n",
        "\n",
        "            # Static features (constant for each store-dept combination)\n",
        "            static_feat = []\n",
        "            if 'Type' in series_data.columns:\n",
        "                # Encode store type: A=0, B=1, C=2\n",
        "                type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "                static_feat.append(type_map.get(series_data['Type'].iloc[0], 0))\n",
        "            if 'Size' in series_data.columns:\n",
        "                # Normalize store size\n",
        "                size_val = series_data['Size'].iloc[0]\n",
        "                static_feat.append(size_val / 200000.0)  # Rough normalization\n",
        "\n",
        "            # Store and dept as categorical features\n",
        "            static_feat.extend([store / 45.0, dept / 100.0])  # Normalize to [0,1]\n",
        "\n",
        "            # Create sliding windows\n",
        "            for i in range(len(series_data) - self.lookback_window - self.forecast_horizon + 1):\n",
        "                window_data = series_data.iloc[i:i + self.lookback_window]\n",
        "\n",
        "                # Time-varying features\n",
        "                time_varying_feat = []\n",
        "\n",
        "                # Sales (target variable)\n",
        "                sales_sequence = window_data['Weekly_Sales'].values\n",
        "                if np.any(np.isnan(sales_sequence)) or np.any(np.isinf(sales_sequence)):\n",
        "                    continue\n",
        "\n",
        "                # Scale sales\n",
        "                if 'Weekly_Sales' in self.scalers:\n",
        "                    sales_scaled = self.scalers['Weekly_Sales'].transform(sales_sequence.reshape(-1, 1)).flatten()\n",
        "                else:\n",
        "                    sales_scaled = sales_sequence\n",
        "\n",
        "                time_varying_feat.append(sales_scaled)\n",
        "\n",
        "                # External time-varying features\n",
        "                feature_names = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                               'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "                for feat_name in feature_names:\n",
        "                    if feat_name in window_data.columns:\n",
        "                        feat_vals = window_data[feat_name].fillna(method='ffill').fillna(method='bfill').fillna(0).values\n",
        "\n",
        "                        # Scale feature if scaler exists\n",
        "                        if feat_name in self.scalers:\n",
        "                            feat_vals = self.scalers[feat_name].transform(feat_vals.reshape(-1, 1)).flatten()\n",
        "\n",
        "                        time_varying_feat.append(feat_vals)\n",
        "\n",
        "                # Holiday indicator\n",
        "                if 'IsHoliday_x' in window_data.columns:\n",
        "                    holiday_vals = window_data['IsHoliday_x'].astype(float).values\n",
        "                    time_varying_feat.append(holiday_vals)\n",
        "                elif 'IsHoliday' in window_data.columns:\n",
        "                    holiday_vals = window_data['IsHoliday'].astype(float).values\n",
        "                    time_varying_feat.append(holiday_vals)\n",
        "\n",
        "                # Date features (cyclical encoding)\n",
        "                dates = pd.to_datetime(window_data['Date'])\n",
        "                week_of_year = dates.dt.isocalendar().week.values\n",
        "                month = dates.dt.month.values\n",
        "\n",
        "                # Cyclical encoding\n",
        "                week_sin = np.sin(2 * np.pi * week_of_year / 52)\n",
        "                week_cos = np.cos(2 * np.pi * week_of_year / 52)\n",
        "                month_sin = np.sin(2 * np.pi * month / 12)\n",
        "                month_cos = np.cos(2 * np.pi * month / 12)\n",
        "\n",
        "                time_varying_feat.extend([week_sin, week_cos, month_sin, month_cos])\n",
        "\n",
        "                # Stack time-varying features\n",
        "                try:\n",
        "                    feature_matrix = np.column_stack(time_varying_feat)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "                sequences.append(feature_matrix)\n",
        "                static_features.append(np.array(static_feat))\n",
        "\n",
        "                # Target (next forecast_horizon values)\n",
        "                target_data = series_data.iloc[i + self.lookback_window:i + self.lookback_window + self.forecast_horizon]\n",
        "                target_sales = target_data['Weekly_Sales'].values\n",
        "\n",
        "                if np.any(np.isnan(target_sales)) or np.any(np.isinf(target_sales)):\n",
        "                    sequences.pop()  # Remove the last added sequence\n",
        "                    static_features.pop()  # Remove the last added static feature\n",
        "                    continue\n",
        "\n",
        "                # Scale target\n",
        "                if 'Weekly_Sales' in self.scalers:\n",
        "                    target_scaled = self.scalers['Weekly_Sales'].transform(target_sales.reshape(-1, 1)).flatten()\n",
        "                else:\n",
        "                    target_scaled = target_sales\n",
        "\n",
        "                targets.append(target_scaled)\n",
        "\n",
        "                # Metadata\n",
        "                metadata.append({\n",
        "                    'store': store,\n",
        "                    'dept': dept,\n",
        "                    'start_date': window_data['Date'].iloc[0],\n",
        "                    'end_date': window_data['Date'].iloc[-1],\n",
        "                    'forecast_date': target_data['Date'].iloc[0] if len(target_data) > 0 else None\n",
        "                })\n",
        "\n",
        "        print(f\"Generated {len(sequences)} valid sequences from {len(self.store_dept_combinations)} store-dept combinations\")\n",
        "\n",
        "        if len(sequences) > 0:\n",
        "            print(f\"Time-varying features shape: {sequences[0].shape}\")\n",
        "            print(f\"Static features shape: {static_features[0].shape}\")\n",
        "\n",
        "        return {\n",
        "            'sequences': np.array(sequences, dtype=object),\n",
        "            'targets': np.array(targets, dtype=object),\n",
        "            'static_features': np.array(static_features, dtype=object),\n",
        "            'metadata': metadata\n",
        "        }\n",
        "\n",
        "class FeatureMerger(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Merges train/test data with stores and features data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stores_data = None\n",
        "        self.features_data = None\n",
        "\n",
        "    def fit(self, X, y=None, stores_df=None, features_df=None):\n",
        "        \"\"\"Store the auxiliary dataframes\"\"\"\n",
        "        self.stores_data = stores_df.copy() if stores_df is not None else None\n",
        "        self.features_data = features_df.copy() if features_df is not None else None\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Merge main data with stores and features\"\"\"\n",
        "        result = X.copy()\n",
        "\n",
        "        # Merge with stores data\n",
        "        if self.stores_data is not None:\n",
        "            result = result.merge(self.stores_data, on='Store', how='left')\n",
        "\n",
        "        # Merge with features data\n",
        "        if self.features_data is not None:\n",
        "            result = result.merge(self.features_data, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        return result\n",
        "\n",
        "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Handle missing values in time-series data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Learn fill values for missing data\"\"\"\n",
        "        # For MarkDown columns, fill with 0 (no markdown)\n",
        "        markdown_cols = [col for col in X.columns if 'MarkDown' in col]\n",
        "        for col in markdown_cols:\n",
        "            self.fill_values[col] = 0.0\n",
        "\n",
        "        # For other numerical columns, use median\n",
        "        numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        for col in numerical_cols:\n",
        "            if col not in self.fill_values and X[col].isnull().any():\n",
        "                self.fill_values[col] = X[col].median()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Fill missing values\"\"\"\n",
        "        result = X.copy()\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in result.columns:\n",
        "                result[col] = result[col].fillna(fill_value)\n",
        "        return result\n",
        "\n",
        "print(\"✓ Custom transformers defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Custom transformers defined\n"
          ]
        }
      ],
      "execution_count": 6,
      "id": "transformers_code"
    },
    {
      "metadata": {
        "id": "tft_model"
      },
      "cell_type": "markdown",
      "source": [
        "# Temporal Fusion Transformer Model Implementation"
      ],
      "id": "tft_model"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tft_model_code",
        "outputId": "04f43271-98a1-480d-ae14-f90cf4383f71"
      },
      "cell_type": "code",
      "source": [
        "class VariableSelectionNetwork(nn.Module):\n",
        "    \"\"\"Variable selection network for TFT\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_inputs, hidden_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_inputs = num_inputs # Number of variables/features\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Flatten and process\n",
        "        # The linear layer should expect input_dim * num_inputs, but here num_inputs refers to the number of variables,\n",
        "        # and input_dim refers to the dimension of each variable at a given time step (usually 1).\n",
        "        # The flattening should be across the sequence length and the input_dim of each variable.\n",
        "        # The input x has shape (batch_size, sequence_length, num_variables, input_dim) conceptually,\n",
        "        # but is passed as (batch_size, sequence_length, num_variables * input_dim) or simpler (batch_size, sequence_length, num_features)\n",
        "        # where num_features = num_variables * input_dim.\n",
        "        # The current implementation of TFTDataProcessor provides (batch_size, sequence_length, num_time_features)\n",
        "        # So num_features = num_time_features, and input_dim is effectively num_features/num_inputs\n",
        "        # Let's assume input_dim=1 as per standard TFT, and num_inputs = num_time_features\n",
        "        # The flattening should be (batch_size, sequence_length * num_time_features)\n",
        "\n",
        "        # Corrected linear layer input dimension\n",
        "        # This will be set dynamically based on the actual input shape during the forward pass\n",
        "        self.flattened_linear1 = nn.Linear(1, hidden_dim) # Placeholder, will be replaced\n",
        "        self.flattened_linear2 = nn.Linear(hidden_dim, num_inputs) # num_inputs is num_time_features here\n",
        "\n",
        "        self.flattened_grn = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            self.flattened_linear2,\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "\n",
        "        # Individual processing for each variable\n",
        "        # Each GRN processes the sequence for a single variable (input_dim=1)\n",
        "        self.single_variable_grns = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.input_dim, hidden_dim), # Should be input_dim\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(hidden_dim, hidden_dim)\n",
        "            ) for _ in range(num_inputs) # num_inputs is num_time_features\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, num_features)\n",
        "        # where num_features is effectively num_time_features in this context\n",
        "        batch_size, seq_len, num_features = x.shape\n",
        "        num_variables = self.num_inputs # num_inputs is num_time_features\n",
        "\n",
        "        # Ensure num_features matches expected num_variables * input_dim\n",
        "        # In this case, input_dim is 1, so num_features should be num_variables\n",
        "        if num_features != num_variables * self.input_dim:\n",
        "             # This could happen if input_dim is not 1 or num_features is incorrect\n",
        "             # Given the TFTDataProcessor, num_features should be num_time_features,\n",
        "             # and input_dim is implicitly 1 per feature.\n",
        "             # Let's adjust the linear layer size dynamically or raise error\n",
        "             # For now, assume input_dim=1 and num_features = num_variables\n",
        "             if num_features != num_variables:\n",
        "                 raise ValueError(f\"Input features {num_features} does not match expected num_variables ({num_variables}) * input_dim ({self.input_dim})\")\n",
        "             # If num_features == num_variables, it means input_dim is 1\n",
        "\n",
        "\n",
        "        # Dynamically set the input size of the first flattened linear layer\n",
        "        if self.flattened_linear1.in_features != seq_len * num_features:\n",
        "             self.flattened_linear1 = nn.Linear(seq_len * num_features, self.hidden_dim).to(x.device)\n",
        "             self.flattened_grn = nn.Sequential(\n",
        "                self.flattened_linear1,\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(self.flattened_grn[2].p), # Keep dropout rate\n",
        "                self.flattened_linear2,\n",
        "                nn.Softmax(dim=-1)\n",
        "             ).to(x.device)\n",
        "\n",
        "\n",
        "        # Flatten for variable selection across sequence length\n",
        "        flattened = x.view(batch_size, -1) # (batch_size, sequence_length * num_features)\n",
        "\n",
        "        # Apply the dynamically sized flattened_grn\n",
        "        variable_weights = self.flattened_grn(flattened)  # (batch_size, num_variables)\n",
        "\n",
        "\n",
        "        # Process each variable separately\n",
        "        processed_variables = []\n",
        "        # Iterate through each variable (column) in the time-varying input\n",
        "        for i in range(num_variables):\n",
        "            # Select the i-th variable across the sequence length\n",
        "            var_input = x[:, :, i:i+self.input_dim] # (batch_size, seq_len, input_dim)\n",
        "            # Apply the GRN for this variable across the sequence length\n",
        "            processed = self.single_variable_grns[i](var_input) # (batch_size, seq_len, hidden_dim)\n",
        "            processed_variables.append(processed)\n",
        "\n",
        "        if len(processed_variables) == 0:\n",
        "            # Handle case with no variables\n",
        "            return torch.zeros(batch_size, seq_len, self.hidden_dim, device=x.device)\n",
        "\n",
        "        # Stack processed variables\n",
        "        # stacked shape: (batch_size, seq_len, hidden_dim, num_variables)\n",
        "        stacked = torch.stack(processed_variables, dim=-1)\n",
        "\n",
        "        # Apply variable weights\n",
        "        # weights_expanded shape: (batch_size, 1, 1, num_variables)\n",
        "        weights_expanded = variable_weights.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "        # Weighted sum over variables\n",
        "        # output shape: (batch_size, seq_len, hidden_dim)\n",
        "        output = (stacked * weights_expanded).sum(dim=-1)\n",
        "\n",
        "        return output\n",
        "\n",
        "class GatedResidualNetwork(nn.Module):\n",
        "    \"\"\"Gated Residual Network component\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Skip connection projection if dimensions don't match\n",
        "        if input_dim != hidden_dim:\n",
        "            self.skip_projection = nn.Linear(input_dim, hidden_dim)\n",
        "        else:\n",
        "            self.skip_projection = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Primary path\n",
        "        # Apply fc1 along the last dimension\n",
        "        y = F.relu(self.fc1(x))\n",
        "        y = self.dropout(y)\n",
        "        y = self.fc2(y)\n",
        "\n",
        "        # Gating\n",
        "        gate = self.gate(y)\n",
        "        y = y * gate\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_projection is not None:\n",
        "            # Apply skip projection along the last dimension\n",
        "            x = self.skip_projection(x)\n",
        "\n",
        "        # Only add skip connection if dimensions match\n",
        "        # This check is important if x has sequence length dimension\n",
        "        if x.shape[-1] == y.shape[-1] and x.shape[:-1] == y.shape[:-1]: # Check all but last dim\n",
        "            y = y + x\n",
        "\n",
        "        y = self.layer_norm(y)\n",
        "        return y\n",
        "\n",
        "class TemporalFusionTransformer(nn.Module):\n",
        "    \"\"\"Temporal Fusion Transformer model\"\"\"\n",
        "\n",
        "    def __init__(self, num_time_features, num_static_features, hidden_dim=128,\n",
        "                 num_attention_heads=4, dropout_rate=0.1, forecast_horizon=1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.num_time_features = num_time_features # Store this\n",
        "\n",
        "        # Variable selection networks\n",
        "        # Assuming input_dim=1 for each time-varying feature\n",
        "        self.temporal_vsn = VariableSelectionNetwork(\n",
        "            input_dim=1, # Dimension of each variable at a time step\n",
        "            num_inputs=num_time_features, # Number of time-varying variables\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout_rate=dropout_rate\n",
        "        )\n",
        "\n",
        "        # Static feature processing\n",
        "        if num_static_features > 0:\n",
        "            self.static_grn = GatedResidualNetwork(\n",
        "                input_dim=num_static_features, hidden_dim=hidden_dim, dropout_rate=dropout_rate\n",
        "            )\n",
        "        else:\n",
        "            self.static_grn = None\n",
        "\n",
        "        # LSTM encoder\n",
        "        self.encoder_lstm = nn.LSTM(\n",
        "            input_size=hidden_dim, hidden_size=hidden_dim,\n",
        "            batch_first=True, dropout=dropout_rate if dropout_rate > 0 else 0 # Avoid dropout=0 in LSTM\n",
        "        )\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.multihead_attn = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim, num_heads=num_attention_heads,\n",
        "            dropout=dropout_rate, batch_first=True\n",
        "        )\n",
        "\n",
        "        # Position-wise feed forward\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Output layers\n",
        "        self.output_grn = GatedResidualNetwork(\n",
        "            input_dim=hidden_dim, hidden_dim=hidden_dim, dropout_rate=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.output_projection = nn.Linear(hidden_dim, forecast_horizon)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, time_varying_inputs, static_inputs=None):\n",
        "        # time_varying_inputs: (batch_size, sequence_length, num_time_features)\n",
        "        batch_size, seq_len, num_features = time_varying_inputs.shape\n",
        "\n",
        "        # Variable selection for temporal features\n",
        "        temporal_features = self.temporal_vsn(time_varying_inputs) # Output: (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # Process static features if available\n",
        "        if static_inputs is not None and self.static_grn is not None:\n",
        "            static_features = self.static_grn(static_inputs)  # (batch_size, hidden_dim)\n",
        "            # Expand to sequence length and combine with temporal features\n",
        "            static_features_expanded = static_features.unsqueeze(1).expand(-1, seq_len, -1)\n",
        "            combined_features = temporal_features + static_features_expanded\n",
        "        else:\n",
        "            combined_features = temporal_features\n",
        "\n",
        "        # LSTM encoding\n",
        "        # combined_features shape: (batch_size, seq_len, hidden_dim)\n",
        "        lstm_out, (hidden, cell) = self.encoder_lstm(combined_features) # lstm_out shape: (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # Multi-head attention\n",
        "        # attn_out shape: (batch_size, seq_len, hidden_dim)\n",
        "        attn_out, _ = self.multihead_attn(lstm_out, lstm_out, lstm_out)\n",
        "\n",
        "        # Add & Norm (Residual connection)\n",
        "        # Ensure shapes match for addition\n",
        "        if attn_out.shape == lstm_out.shape:\n",
        "             attn_out = self.layer_norm1(attn_out + lstm_out)\n",
        "        else:\n",
        "             # If shapes don't match, just normalize attn_out (shouldn't happen if MHA output dim is hidden_dim)\n",
        "             attn_out = self.layer_norm1(attn_out)\n",
        "\n",
        "\n",
        "        # Feed forward\n",
        "        # ff_out shape: (batch_size, seq_len, hidden_dim)\n",
        "        ff_out = self.feed_forward(attn_out)\n",
        "\n",
        "        # Add & Norm (Residual connection)\n",
        "        # Ensure shapes match for addition\n",
        "        if ff_out.shape == attn_out.shape:\n",
        "             ff_out = self.layer_norm2(ff_out + attn_out)\n",
        "        else:\n",
        "            # If shapes don't match, just normalize ff_out (shouldn't happen if FF output dim is hidden_dim)\n",
        "            ff_out = self.layer_norm2(ff_out)\n",
        "\n",
        "\n",
        "        # Use the last time step for prediction\n",
        "        last_output = ff_out[:, -1, :]  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Final processing\n",
        "        processed_output = self.output_grn(last_output) # (batch_size, hidden_dim)\n",
        "\n",
        "        # Output projection\n",
        "        predictions = self.output_projection(processed_output) # (batch_size, forecast_horizon)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "class WalmartTFTDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for Walmart TFT data\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, targets, static_features):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "        self.static_features = static_features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.FloatTensor(self.sequences[idx])\n",
        "        target = torch.FloatTensor(self.targets[idx])\n",
        "        static = torch.FloatTensor(self.static_features[idx])\n",
        "        return sequence, static, target\n",
        "\n",
        "print(\"✓ TFT model architecture defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ TFT model architecture defined\n"
          ]
        }
      ],
      "execution_count": 7,
      "id": "tft_model_code"
    },
    {
      "metadata": {
        "id": "cleaning_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning Run"
      ],
      "id": "cleaning_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cleaning_code",
        "outputId": "0a3316f0-25bd-43bc-9dda-e7ce40453858"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for data cleaning\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"TFT_Cleaning\",\n",
        "    config={\"stage\": \"cleaning\"}\n",
        ")\n",
        "\n",
        "print(\"\\n=== DATA CLEANING ===\")\n",
        "\n",
        "# Create feature merger and missing value handler\n",
        "feature_merger = FeatureMerger()\n",
        "missing_handler = MissingValueHandler()\n",
        "\n",
        "# Fit the merger with auxiliary data\n",
        "feature_merger.fit(train_df, stores_df=stores_df, features_df=features_df)\n",
        "\n",
        "# Merge train data with stores and features\n",
        "print(\"Merging train data with stores and features...\")\n",
        "train_merged = feature_merger.transform(train_df)\n",
        "print(f\"Train data shape after merging: {train_merged.shape}\")\n",
        "\n",
        "# Fit and transform missing values\n",
        "print(\"Handling missing values...\")\n",
        "missing_handler.fit(train_merged)\n",
        "train_cleaned = missing_handler.transform(train_merged)\n",
        "\n",
        "# Check for remaining missing values\n",
        "remaining_missing = train_cleaned.isnull().sum()\n",
        "remaining_missing = remaining_missing[remaining_missing > 0]\n",
        "\n",
        "print(f\"\\nRemaining missing values after cleaning:\")\n",
        "if len(remaining_missing) > 0:\n",
        "    print(remaining_missing)\n",
        "else:\n",
        "    print(\"No missing values remaining!\")\n",
        "\n",
        "# Basic data quality checks\n",
        "print(f\"\\nData quality checks:\")\n",
        "print(f\"Total records: {len(train_cleaned):,}\")\n",
        "print(f\"Date range: {train_cleaned['Date'].min()} to {train_cleaned['Date'].max()}\")\n",
        "print(f\"Unique store-dept combinations: {train_cleaned.groupby(['Store', 'Dept']).ngroups:,}\")\n",
        "\n",
        "# Check for negative sales (data quality issue)\n",
        "negative_sales = (train_cleaned['Weekly_Sales'] < 0).sum()\n",
        "print(f\"Records with negative sales: {negative_sales:,} ({negative_sales/len(train_cleaned)*100:.2f}%)\")\n",
        "\n",
        "# Log cleaning metrics\n",
        "wandb.log({\n",
        "    \"cleaned_records\": len(train_cleaned),\n",
        "    \"remaining_missing_values\": len(remaining_missing),\n",
        "    \"negative_sales_count\": int(negative_sales),\n",
        "    \"negative_sales_pct\": float(negative_sales/len(train_cleaned)*100),\n",
        "    \"store_dept_combinations\": train_cleaned.groupby(['Store', 'Dept']).ngroups\n",
        "})\n",
        "\n",
        "print(\"\\n✓ Data cleaning completed and logged to wandb\")\n",
        "\n",
        "# Save cleaned data for next steps\n",
        "print(\"\\nSample of cleaned data:\")\n",
        "print(train_cleaned.head())\n",
        "print(f\"\\nColumns: {list(train_cleaned.columns)}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_timeseries_length</td><td>▁</td></tr><tr><td>avg_weekly_sales</td><td>▁</td></tr><tr><td>holiday_sales_boost</td><td>▁</td></tr><tr><td>median_weekly_sales</td><td>▁</td></tr><tr><td>missing_markdown1_pct</td><td>▁</td></tr><tr><td>missing_markdown2_pct</td><td>▁</td></tr><tr><td>missing_markdown3_pct</td><td>▁</td></tr><tr><td>missing_markdown4_pct</td><td>▁</td></tr><tr><td>missing_markdown5_pct</td><td>▁</td></tr><tr><td>sales_std</td><td>▁</td></tr><tr><td>total_timeseries</td><td>▁</td></tr><tr><td>unique_departments</td><td>▁</td></tr><tr><td>unique_stores</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_timeseries_length</td><td>126.55959</td></tr><tr><td>avg_weekly_sales</td><td>15981.25812</td></tr><tr><td>holiday_sales_boost</td><td>1.07134</td></tr><tr><td>median_weekly_sales</td><td>7612.03</td></tr><tr><td>missing_markdown1_pct</td><td>50.76923</td></tr><tr><td>missing_markdown2_pct</td><td>64.33455</td></tr><tr><td>missing_markdown3_pct</td><td>55.88523</td></tr><tr><td>missing_markdown4_pct</td><td>57.70452</td></tr><tr><td>missing_markdown5_pct</td><td>50.54945</td></tr><tr><td>sales_std</td><td>22711.18352</td></tr><tr><td>total_timeseries</td><td>3331</td></tr><tr><td>unique_departments</td><td>81</td></tr><tr><td>unique_stores</td><td>45</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">TFT_Exploration</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/ec1vmor2' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/ec1vmor2</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_181713-ec1vmor2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_181724-qxg54ded</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/qxg54ded' target=\"_blank\">TFT_Cleaning</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/qxg54ded' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/qxg54ded</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA CLEANING ===\n",
            "Merging train data with stores and features...\n",
            "Train data shape after merging: (421570, 17)\n",
            "Handling missing values...\n",
            "\n",
            "Remaining missing values after cleaning:\n",
            "No missing values remaining!\n",
            "\n",
            "Data quality checks:\n",
            "Total records: 421,570\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Unique store-dept combinations: 3,331\n",
            "Records with negative sales: 1,285 (0.30%)\n",
            "\n",
            "✓ Data cleaning completed and logged to wandb\n",
            "\n",
            "Sample of cleaned data:\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday_x Type    Size  \\\n",
            "0      1     1 2010-02-05      24924.50        False    A  151315   \n",
            "1      1     1 2010-02-12      46039.49         True    A  151315   \n",
            "2      1     1 2010-02-19      41595.55        False    A  151315   \n",
            "3      1     1 2010-02-26      19403.54        False    A  151315   \n",
            "4      1     1 2010-03-05      21827.90        False    A  151315   \n",
            "\n",
            "   Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  \\\n",
            "0        42.31       2.572        0.0        0.0        0.0        0.0   \n",
            "1        38.51       2.548        0.0        0.0        0.0        0.0   \n",
            "2        39.93       2.514        0.0        0.0        0.0        0.0   \n",
            "3        46.63       2.561        0.0        0.0        0.0        0.0   \n",
            "4        46.50       2.625        0.0        0.0        0.0        0.0   \n",
            "\n",
            "   MarkDown5         CPI  Unemployment  IsHoliday_y  \n",
            "0        0.0  211.096358         8.106        False  \n",
            "1        0.0  211.242170         8.106         True  \n",
            "2        0.0  211.289143         8.106        False  \n",
            "3        0.0  211.319643         8.106        False  \n",
            "4        0.0  211.350143         8.106        False  \n",
            "\n",
            "Columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday_x', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y']\n"
          ]
        }
      ],
      "execution_count": 8,
      "id": "cleaning_code"
    },
    {
      "metadata": {
        "id": "feature_selection_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Selection Run"
      ],
      "id": "feature_selection_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "feature_selection_code",
        "outputId": "e7daaf80-50a7-4084-85e8-dd8e92ea38a4"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for feature selection\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"TFT_Feature_Selection\",\n",
        "    config={\"stage\": \"feature_selection\"}\n",
        ")\n",
        "\n",
        "print(\"\\n=== FEATURE SELECTION ===\")\n",
        "\n",
        "# For TFT, we categorize features into static and time-varying\n",
        "\n",
        "# Core features\n",
        "core_features = ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday_x']\n",
        "\n",
        "# Static features (constant per store-dept combination)\n",
        "static_features = ['Type', 'Size']\n",
        "\n",
        "# Time-varying features\n",
        "time_varying_features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                        'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "print(f\"Available columns: {list(train_cleaned.columns)}\")\n",
        "\n",
        "# Analyze correlation between features and sales\n",
        "correlation_analysis = {}\n",
        "\n",
        "# Time-varying features correlation\n",
        "for feature in time_varying_features:\n",
        "    if feature in train_cleaned.columns:\n",
        "        corr = train_cleaned['Weekly_Sales'].corr(train_cleaned[feature])\n",
        "        correlation_analysis[feature] = corr\n",
        "        print(f\"Correlation between Weekly_Sales and {feature}: {corr:.4f}\")\n",
        "\n",
        "# Holiday impact analysis\n",
        "# Handle different holiday column names\n",
        "holiday_col = 'IsHoliday_x' if 'IsHoliday_x' in train_cleaned.columns else 'IsHoliday'\n",
        "holiday_impact = train_cleaned.groupby(holiday_col)['Weekly_Sales'].mean()\n",
        "holiday_boost = holiday_impact[True] / holiday_impact[False] - 1\n",
        "print(f\"\\nHoliday sales boost: {holiday_boost:.2%}\")\n",
        "\n",
        "# Store type impact (static feature)\n",
        "if 'Type' in train_cleaned.columns:\n",
        "    store_type_sales = train_cleaned.groupby('Type')['Weekly_Sales'].mean()\n",
        "    print(f\"\\nAverage sales by store type (static feature):\")\n",
        "    print(store_type_sales)\n",
        "\n",
        "# Store size impact (static feature)\n",
        "if 'Size' in train_cleaned.columns:\n",
        "    size_corr = train_cleaned['Weekly_Sales'].corr(train_cleaned['Size'])\n",
        "    print(f\"\\nCorrelation between Weekly_Sales and Store Size: {size_corr:.4f}\")\n",
        "\n",
        "# Select features for TFT\n",
        "selected_core = core_features.copy()\n",
        "selected_static = []\n",
        "selected_time_varying = []\n",
        "\n",
        "# Always include static features for TFT\n",
        "for feature in static_features:\n",
        "    if feature in train_cleaned.columns:\n",
        "        selected_static.append(feature)\n",
        "\n",
        "# Include time-varying features with any correlation\n",
        "for feature, corr in correlation_analysis.items():\n",
        "    if abs(corr) > 0.001:  # Very low threshold for TFT\n",
        "        selected_time_varying.append(feature)\n",
        "        print(f\"Selected time-varying {feature} (correlation: {corr:.4f})\")\n",
        "\n",
        "# Combine all selected features\n",
        "all_selected_features = selected_core + selected_static + selected_time_varying\n",
        "all_selected_features = list(set(all_selected_features))\n",
        "\n",
        "print(f\"\\nSelected Features:\")\n",
        "print(f\"Core features ({len(selected_core)}): {selected_core}\")\n",
        "print(f\"Static features ({len(selected_static)}): {selected_static}\")\n",
        "print(f\"Time-varying features ({len(selected_time_varying)}): {selected_time_varying}\")\n",
        "print(f\"Total selected features: {len(all_selected_features)}\")\n",
        "\n",
        "# Create feature-selected dataset\n",
        "train_selected = train_cleaned[all_selected_features].copy()\n",
        "\n",
        "print(f\"\\nFeature-selected data shape: {train_selected.shape}\")\n",
        "\n",
        "# Log feature selection metrics\n",
        "wandb.log({\n",
        "    \"total_available_features\": len(train_cleaned.columns),\n",
        "    \"selected_features_count\": len(all_selected_features),\n",
        "    \"static_features_count\": len(selected_static),\n",
        "    \"time_varying_features_count\": len(selected_time_varying),\n",
        "    \"holiday_sales_boost\": float(holiday_boost),\n",
        "    \"selected_features\": all_selected_features,\n",
        "    \"static_features\": selected_static,\n",
        "    \"time_varying_features\": selected_time_varying,\n",
        "    **{f\"corr_{k}\": v for k, v in correlation_analysis.items() if not np.isnan(v)}\n",
        "})\n",
        "\n",
        "print(\"\\n✓ Feature selection completed and logged to wandb\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cleaned_records</td><td>▁</td></tr><tr><td>negative_sales_count</td><td>▁</td></tr><tr><td>negative_sales_pct</td><td>▁</td></tr><tr><td>remaining_missing_values</td><td>▁</td></tr><tr><td>store_dept_combinations</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cleaned_records</td><td>421570</td></tr><tr><td>negative_sales_count</td><td>1285</td></tr><tr><td>negative_sales_pct</td><td>0.30481</td></tr><tr><td>remaining_missing_values</td><td>0</td></tr><tr><td>store_dept_combinations</td><td>3331</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">TFT_Cleaning</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/qxg54ded' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/qxg54ded</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_181724-qxg54ded/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_181730-hwoqeh2a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/hwoqeh2a' target=\"_blank\">TFT_Feature_Selection</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/hwoqeh2a' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/hwoqeh2a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FEATURE SELECTION ===\n",
            "Available columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday_x', 'Type', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y']\n",
            "Correlation between Weekly_Sales and Temperature: -0.0023\n",
            "Correlation between Weekly_Sales and Fuel_Price: -0.0001\n",
            "Correlation between Weekly_Sales and CPI: -0.0209\n",
            "Correlation between Weekly_Sales and Unemployment: -0.0259\n",
            "Correlation between Weekly_Sales and MarkDown1: 0.0472\n",
            "Correlation between Weekly_Sales and MarkDown2: 0.0207\n",
            "Correlation between Weekly_Sales and MarkDown3: 0.0386\n",
            "Correlation between Weekly_Sales and MarkDown4: 0.0375\n",
            "Correlation between Weekly_Sales and MarkDown5: 0.0505\n",
            "\n",
            "Holiday sales boost: 7.13%\n",
            "\n",
            "Average sales by store type (static feature):\n",
            "Type\n",
            "A    20099.568043\n",
            "B    12237.075977\n",
            "C     9519.532538\n",
            "Name: Weekly_Sales, dtype: float64\n",
            "\n",
            "Correlation between Weekly_Sales and Store Size: 0.2438\n",
            "Selected time-varying Temperature (correlation: -0.0023)\n",
            "Selected time-varying CPI (correlation: -0.0209)\n",
            "Selected time-varying Unemployment (correlation: -0.0259)\n",
            "Selected time-varying MarkDown1 (correlation: 0.0472)\n",
            "Selected time-varying MarkDown2 (correlation: 0.0207)\n",
            "Selected time-varying MarkDown3 (correlation: 0.0386)\n",
            "Selected time-varying MarkDown4 (correlation: 0.0375)\n",
            "Selected time-varying MarkDown5 (correlation: 0.0505)\n",
            "\n",
            "Selected Features:\n",
            "Core features (5): ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday_x']\n",
            "Static features (2): ['Type', 'Size']\n",
            "Time-varying features (8): ['Temperature', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
            "Total selected features: 15\n",
            "\n",
            "Feature-selected data shape: (421570, 15)\n",
            "\n",
            "✓ Feature selection completed and logged to wandb\n"
          ]
        }
      ],
      "execution_count": 9,
      "id": "feature_selection_code"
    },
    {
      "metadata": {
        "id": "cross_validation_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Cross Validation Run"
      ],
      "id": "cross_validation_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cross_validation_code",
        "outputId": "32d7c111-c6a8-476b-9987-f8c7e224cce8"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for cross validation\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"TFT_Cross_Validation\",\n",
        "    config={\n",
        "        \"stage\": \"cross_validation\",\n",
        "        \"lookback_window\": 52,\n",
        "        \"forecast_horizon\": 1,\n",
        "        \"model_type\": \"TFT\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\n=== CROSS VALIDATION ===\")\n",
        "\n",
        "# Use the same efficient approach as N-BEATS\n",
        "class EfficientTFTDataProcessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Efficient TFT processor - similar to N-BEATS approach\"\"\"\n",
        "\n",
        "    def __init__(self, lookback_window=52, forecast_horizon=1):\n",
        "        self.lookback_window = lookback_window\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.store_dept_combinations = None\n",
        "        self.date_range = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Learn the store-department combinations and date range\"\"\"\n",
        "        self.store_dept_combinations = X.groupby(['Store', 'Dept']).size().index.tolist()\n",
        "        self.date_range = sorted(X['Date'].unique())\n",
        "        print(f\"Found {len(self.store_dept_combinations)} store-dept combinations\")\n",
        "        print(f\"Date range: {self.date_range[0]} to {self.date_range[-1]}\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data into sequences for TFT - efficient like N-BEATS\"\"\"\n",
        "        sequences = []\n",
        "        targets = []\n",
        "        static_features = []\n",
        "        metadata = []\n",
        "\n",
        "        for store, dept in self.store_dept_combinations:\n",
        "            # Get time series for this store-dept combination\n",
        "            series_data = X[(X['Store'] == store) & (X['Dept'] == dept)].copy()\n",
        "            series_data = series_data.sort_values('Date')\n",
        "\n",
        "            if len(series_data) < self.lookback_window + self.forecast_horizon:\n",
        "                continue\n",
        "\n",
        "            # Static features (simple, like N-BEATS)\n",
        "            static_feat = []\n",
        "            if 'Type' in series_data.columns:\n",
        "                type_map = {'A': 0, 'B': 1, 'C': 2}\n",
        "                static_feat.append(type_map.get(series_data['Type'].iloc[0], 0))\n",
        "            if 'Size' in series_data.columns:\n",
        "                static_feat.append(series_data['Size'].iloc[0] / 200000.0)\n",
        "\n",
        "            # Add store/dept as features\n",
        "            static_feat.extend([store / 45.0, dept / 100.0])\n",
        "\n",
        "            # Create sliding windows (same approach as N-BEATS)\n",
        "            for i in range(len(series_data) - self.lookback_window - self.forecast_horizon + 1):\n",
        "                window_data = series_data.iloc[i:i + self.lookback_window]\n",
        "\n",
        "                # Sales sequence (target variable)\n",
        "                sales_sequence = window_data['Weekly_Sales'].values\n",
        "\n",
        "                # Check for valid sales data\n",
        "                if np.any(np.isnan(sales_sequence)) or np.any(np.isinf(sales_sequence)):\n",
        "                    continue\n",
        "\n",
        "                # External features (same as N-BEATS)\n",
        "                external_features = []\n",
        "                if 'Temperature' in window_data.columns:\n",
        "                    temp_vals = window_data['Temperature'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "                    external_features.append(temp_vals.values)\n",
        "                if 'Fuel_Price' in window_data.columns:\n",
        "                    fuel_vals = window_data['Fuel_Price'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "                    external_features.append(fuel_vals.values)\n",
        "                if 'CPI' in window_data.columns:\n",
        "                    cpi_vals = window_data['CPI'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "                    external_features.append(cpi_vals.values)\n",
        "                if 'Unemployment' in window_data.columns:\n",
        "                    unemp_vals = window_data['Unemployment'].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "                    external_features.append(unemp_vals.values)\n",
        "\n",
        "                # Holiday feature (simplified)\n",
        "                if 'IsHoliday_x' in window_data.columns:\n",
        "                    holiday_vals = window_data['IsHoliday_x'].astype(float).values\n",
        "                    external_features.append(holiday_vals)\n",
        "                elif 'IsHoliday' in window_data.columns:\n",
        "                    holiday_vals = window_data['IsHoliday'].astype(float).values\n",
        "                    external_features.append(holiday_vals)\n",
        "\n",
        "                # Combine features (same as N-BEATS approach)\n",
        "                if external_features:\n",
        "                    try:\n",
        "                        feature_matrix = np.column_stack([sales_sequence] + external_features)\n",
        "                    except ValueError:\n",
        "                        feature_matrix = sales_sequence.reshape(-1, 1)\n",
        "                else:\n",
        "                    feature_matrix = sales_sequence.reshape(-1, 1)\n",
        "\n",
        "                sequences.append(feature_matrix)\n",
        "                static_features.append(np.array(static_feat))\n",
        "\n",
        "                # Target (next forecast_horizon values)\n",
        "                target_data = series_data.iloc[i + self.lookback_window:i + self.lookback_window + self.forecast_horizon]\n",
        "                target_sales = target_data['Weekly_Sales'].values\n",
        "\n",
        "                # Check for valid target data\n",
        "                if np.any(np.isnan(target_sales)) or np.any(np.isinf(target_sales)):\n",
        "                    continue\n",
        "\n",
        "                targets.append(target_sales)\n",
        "\n",
        "                # Metadata\n",
        "                metadata.append({\n",
        "                    'store': store,\n",
        "                    'dept': dept,\n",
        "                    'start_date': window_data['Date'].iloc[0],\n",
        "                    'end_date': window_data['Date'].iloc[-1],\n",
        "                    'forecast_date': target_data['Date'].iloc[0] if len(target_data) > 0 else None\n",
        "                })\n",
        "\n",
        "        print(f\"Generated {len(sequences)} valid sequences from {len(self.store_dept_combinations)} store-dept combinations\")\n",
        "\n",
        "        return {\n",
        "            'sequences': np.array(sequences, dtype=object),\n",
        "            'targets': np.array(targets, dtype=object),\n",
        "            'static_features': np.array(static_features, dtype=object),\n",
        "            'metadata': metadata\n",
        "        }\n",
        "\n",
        "# Create efficient TFT data processor\n",
        "tft_processor = EfficientTFTDataProcessor(lookback_window=52, forecast_horizon=1)\n",
        "\n",
        "# Fit and transform the data\n",
        "print(\"Processing time-series data for TFT...\")\n",
        "tft_processor.fit(train_selected)\n",
        "processed_data = tft_processor.transform(train_selected)\n",
        "\n",
        "sequences = processed_data['sequences']\n",
        "targets = processed_data['targets']\n",
        "static_features = processed_data['static_features']\n",
        "metadata = processed_data['metadata']\n",
        "\n",
        "print(f\"Generated {len(sequences)} sequences\")\n",
        "if len(sequences) > 0:\n",
        "    print(f\"Sequence shape example: {sequences[0].shape}\")\n",
        "    print(f\"Target shape example: {targets[0].shape}\")\n",
        "    print(f\"Static features shape example: {static_features[0].shape}\")\n",
        "\n",
        "if len(sequences) == 0:\n",
        "    print(\"❌ No sequences generated. Check data processing.\")\n",
        "    wandb.log({\"sequences_generated\": 0, \"processing_failed\": True})\n",
        "else:\n",
        "    # Convert to consistent numpy arrays (same as N-BEATS)\n",
        "    max_time_features = max([seq.shape[1] if len(seq.shape) > 1 else 1 for seq in sequences])\n",
        "    max_static_features = max([sf.shape[0] if len(sf.shape) > 0 else 1 for sf in static_features])\n",
        "    lookback_length = sequences[0].shape[0]\n",
        "\n",
        "    # Pad sequences to have consistent feature count and convert to float32\n",
        "    padded_sequences = []\n",
        "    padded_static = []\n",
        "    valid_targets = []\n",
        "\n",
        "    for i, (seq, static, tgt) in enumerate(zip(sequences, static_features, targets)):\n",
        "        if len(seq.shape) == 1:\n",
        "            seq = seq.reshape(-1, 1)\n",
        "\n",
        "        # Pad features if necessary\n",
        "        if seq.shape[1] < max_time_features:\n",
        "            padding = np.zeros((seq.shape[0], max_time_features - seq.shape[1]), dtype=np.float32)\n",
        "            seq = np.column_stack([seq, padding]).astype(np.float32)\n",
        "        else:\n",
        "            seq = seq.astype(np.float32)\n",
        "\n",
        "        # Process static features\n",
        "        if len(static.shape) == 0:\n",
        "            static = static.reshape(1)\n",
        "\n",
        "        if static.shape[0] < max_static_features:\n",
        "            padding = np.zeros(max_static_features - static.shape[0], dtype=np.float32)\n",
        "            static = np.concatenate([static, padding]).astype(np.float32)\n",
        "        else:\n",
        "            static = static.astype(np.float32)\n",
        "\n",
        "        padded_sequences.append(seq)\n",
        "        padded_static.append(static)\n",
        "        valid_targets.append(tgt.astype(np.float32))\n",
        "\n",
        "    sequences_np = np.array(padded_sequences, dtype=np.float32)\n",
        "    static_np = np.array(padded_static, dtype=np.float32)\n",
        "    targets_np = np.array(valid_targets, dtype=np.float32)\n",
        "\n",
        "    print(f\"Processed sequences shape: {sequences_np.shape}\")\n",
        "    print(f\"Processed static features shape: {static_np.shape}\")\n",
        "    print(f\"Processed targets shape: {targets_np.shape}\")\n",
        "\n",
        "    # Time-based train-validation split (same as N-BEATS)\n",
        "    split_idx = int(0.8 * len(sequences_np))\n",
        "\n",
        "    X_train_cv = sequences_np[:split_idx]\n",
        "    X_static_train_cv = static_np[:split_idx]\n",
        "    y_train_cv = targets_np[:split_idx]\n",
        "\n",
        "    X_val_cv = sequences_np[split_idx:]\n",
        "    X_static_val_cv = static_np[split_idx:]\n",
        "    y_val_cv = targets_np[split_idx:]\n",
        "\n",
        "    print(f\"\\nTrain-Validation Split:\")\n",
        "    print(f\"Training sequences: {len(X_train_cv)}\")\n",
        "    print(f\"Validation sequences: {len(X_val_cv)}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = WalmartTFTDataset(X_train_cv, y_train_cv, X_static_train_cv)\n",
        "    val_dataset = WalmartTFTDataset(X_val_cv, y_val_cv, X_static_val_cv)\n",
        "\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Simplified TFT model (closer to N-BEATS complexity)\n",
        "    class SimplifiedTFT(nn.Module):\n",
        "        \"\"\"Simplified TFT for reasonable training time\"\"\"\n",
        "\n",
        "        def __init__(self, num_time_features, num_static_features, hidden_dim=128,\n",
        "                     num_attention_heads=4, dropout_rate=0.1, forecast_horizon=1):\n",
        "            super().__init__()\n",
        "            self.hidden_dim = hidden_dim\n",
        "\n",
        "            # Simple projections instead of complex VSN\n",
        "            self.temporal_projection = nn.Linear(num_time_features, hidden_dim)\n",
        "\n",
        "            if num_static_features > 0:\n",
        "                self.static_projection = nn.Linear(num_static_features, hidden_dim)\n",
        "            else:\n",
        "                self.static_projection = None\n",
        "\n",
        "            # LSTM encoder (like N-BEATS but with attention)\n",
        "            self.encoder_lstm = nn.LSTM(\n",
        "                input_size=hidden_dim, hidden_size=hidden_dim,\n",
        "                batch_first=True, dropout=dropout_rate\n",
        "            )\n",
        "\n",
        "            # Simple attention\n",
        "            self.multihead_attn = nn.MultiheadAttention(\n",
        "                embed_dim=hidden_dim, num_heads=num_attention_heads,\n",
        "                dropout=dropout_rate, batch_first=True\n",
        "            )\n",
        "\n",
        "            # Output layers\n",
        "            self.output_projection = nn.Linear(hidden_dim, forecast_horizon)\n",
        "\n",
        "            # Layer normalization\n",
        "            self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        def forward(self, time_varying_inputs, static_inputs=None):\n",
        "            batch_size, seq_len, num_features = time_varying_inputs.shape\n",
        "\n",
        "            # Project temporal features\n",
        "            temporal_features = self.temporal_projection(time_varying_inputs)\n",
        "\n",
        "            # Add static features if available\n",
        "            if static_inputs is not None and self.static_projection is not None:\n",
        "                static_features = self.static_projection(static_inputs)\n",
        "                static_features = static_features.unsqueeze(1).expand(-1, seq_len, -1)\n",
        "                combined_features = temporal_features + static_features\n",
        "            else:\n",
        "                combined_features = temporal_features\n",
        "\n",
        "            # LSTM encoding\n",
        "            lstm_out, _ = self.encoder_lstm(combined_features)\n",
        "\n",
        "            # Attention\n",
        "            attn_out, _ = self.multihead_attn(lstm_out, lstm_out, lstm_out)\n",
        "\n",
        "            # Add & Norm\n",
        "            attn_out = self.layer_norm(attn_out + lstm_out)\n",
        "\n",
        "            # Use the last time step for prediction\n",
        "            last_output = attn_out[:, -1, :]\n",
        "\n",
        "            # Output projection\n",
        "            predictions = self.output_projection(last_output)\n",
        "\n",
        "            return predictions\n",
        "\n",
        "    # Initialize simplified TFT model\n",
        "    model_config = {\n",
        "        \"num_time_features\": max_time_features,\n",
        "        \"num_static_features\": max_static_features,\n",
        "        \"hidden_dim\": 128,\n",
        "        \"num_attention_heads\": 4,\n",
        "        \"dropout_rate\": 0.1,\n",
        "        \"forecast_horizon\": 1\n",
        "    }\n",
        "\n",
        "    model = SimplifiedTFT(**model_config).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"\\nSimplified TFT model initialized with config: {model_config}\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Training loop (same as N-BEATS)\n",
        "    num_epochs = 5  # Limited epochs for CV\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (sequences, static_feat, targets) in enumerate(train_loader):\n",
        "            sequences = sequences.to(device)\n",
        "            static_feat = static_feat.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences, static_feat)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences, static_feat, targets in val_loader:\n",
        "                sequences = sequences.to(device)\n",
        "                static_feat = static_feat.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                outputs = model(sequences, static_feat)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                all_predictions.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Calculate metrics\n",
        "        all_predictions = np.array(all_predictions).flatten()\n",
        "        all_targets = np.array(all_targets).flatten()\n",
        "\n",
        "        val_mae = mean_absolute_error(all_targets, all_predictions)\n",
        "        val_rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
        "        val_r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "        # Safe MAPE calculation\n",
        "        def safe_mape(y_true, y_pred):\n",
        "            mask = y_true != 0\n",
        "            if mask.sum() == 0:\n",
        "                return float('inf')\n",
        "            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "        val_mape = safe_mape(all_targets, all_predictions)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'  Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'  Val MAE: {val_mae:.2f}')\n",
        "        print(f'  Val RMSE: {val_rmse:.2f}')\n",
        "        print(f'  Val MAPE: {val_mape:.2f}%')\n",
        "        print(f'  Val R²: {val_r2:.4f}')\n",
        "\n",
        "        # Log to wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": avg_val_loss,\n",
        "            \"val_mae\": val_mae,\n",
        "            \"val_rmse\": val_rmse,\n",
        "            \"val_mape\": val_mape if not np.isinf(val_mape) else 0.0,\n",
        "            \"val_r2\": val_r2\n",
        "        })\n",
        "\n",
        "    # Final CV results\n",
        "    final_metrics = {\n",
        "        \"cv_final_train_loss\": train_losses[-1],\n",
        "        \"cv_final_val_loss\": val_losses[-1],\n",
        "        \"cv_final_val_mae\": val_mae,\n",
        "        \"cv_final_val_rmse\": val_rmse,\n",
        "        \"cv_final_val_mape\": val_mape if not np.isinf(val_mape) else 0.0,\n",
        "        \"cv_final_val_r2\": val_r2,\n",
        "        \"sequences_generated\": len(sequences_np),\n",
        "        \"train_sequences\": len(X_train_cv),\n",
        "        \"val_sequences\": len(X_val_cv)\n",
        "    }\n",
        "\n",
        "    wandb.log(final_metrics)\n",
        "\n",
        "    print(\"\\n✓ Cross validation completed and logged to wandb\")\n",
        "    print(f\"Final validation metrics: MAE={val_mae:.2f}, RMSE={val_rmse:.2f}, MAPE={val_mape:.2f}%, R²={val_r2:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>corr_CPI</td><td>▁</td></tr><tr><td>corr_Fuel_Price</td><td>▁</td></tr><tr><td>corr_MarkDown1</td><td>▁</td></tr><tr><td>corr_MarkDown2</td><td>▁</td></tr><tr><td>corr_MarkDown3</td><td>▁</td></tr><tr><td>corr_MarkDown4</td><td>▁</td></tr><tr><td>corr_MarkDown5</td><td>▁</td></tr><tr><td>corr_Temperature</td><td>▁</td></tr><tr><td>corr_Unemployment</td><td>▁</td></tr><tr><td>holiday_sales_boost</td><td>▁</td></tr><tr><td>selected_features_count</td><td>▁</td></tr><tr><td>static_features_count</td><td>▁</td></tr><tr><td>time_varying_features_count</td><td>▁</td></tr><tr><td>total_available_features</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>corr_CPI</td><td>-0.02092</td></tr><tr><td>corr_Fuel_Price</td><td>-0.00012</td></tr><tr><td>corr_MarkDown1</td><td>0.04717</td></tr><tr><td>corr_MarkDown2</td><td>0.02072</td></tr><tr><td>corr_MarkDown3</td><td>0.03856</td></tr><tr><td>corr_MarkDown4</td><td>0.03747</td></tr><tr><td>corr_MarkDown5</td><td>0.05047</td></tr><tr><td>corr_Temperature</td><td>-0.00231</td></tr><tr><td>corr_Unemployment</td><td>-0.02586</td></tr><tr><td>holiday_sales_boost</td><td>0.07134</td></tr><tr><td>selected_features_count</td><td>15</td></tr><tr><td>static_features_count</td><td>2</td></tr><tr><td>time_varying_features_count</td><td>8</td></tr><tr><td>total_available_features</td><td>17</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">TFT_Feature_Selection</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/hwoqeh2a' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/hwoqeh2a</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_181730-hwoqeh2a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_181752-p36d9or3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/p36d9or3' target=\"_blank\">TFT_Cross_Validation</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/p36d9or3' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/p36d9or3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CROSS VALIDATION ===\n",
            "Processing time-series data for TFT...\n",
            "Found 3331 store-dept combinations\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Generated 261083 valid sequences from 3331 store-dept combinations\n",
            "Generated 261083 sequences\n",
            "Sequence shape example: (52, 5)\n",
            "Target shape example: (1,)\n",
            "Static features shape example: (4,)\n",
            "Processed sequences shape: (261083, 52, 5)\n",
            "Processed static features shape: (261083, 4)\n",
            "Processed targets shape: (261083, 1)\n",
            "\n",
            "Train-Validation Split:\n",
            "Training sequences: 208866\n",
            "Validation sequences: 52217\n",
            "\n",
            "Simplified TFT model initialized with config: {'num_time_features': 5, 'num_static_features': 4, 'hidden_dim': 128, 'num_attention_heads': 4, 'dropout_rate': 0.1, 'forecast_horizon': 1}\n",
            "Total parameters: 199,937\n",
            "Epoch 1/5, Batch 0, Loss: 461803552.0000\n",
            "Epoch 1/5, Batch 50, Loss: 735711808.0000\n",
            "Epoch 1/5, Batch 100, Loss: 492844000.0000\n",
            "Epoch 1/5, Batch 150, Loss: 198830752.0000\n",
            "Epoch 1/5, Batch 200, Loss: 719259840.0000\n",
            "Epoch 1/5, Batch 250, Loss: 288476640.0000\n",
            "Epoch 1/5, Batch 300, Loss: 1498233088.0000\n",
            "Epoch 1/5, Batch 350, Loss: 557872896.0000\n",
            "Epoch 1/5, Batch 400, Loss: 1247184000.0000\n",
            "Epoch 1/5, Batch 450, Loss: 466037984.0000\n",
            "Epoch 1/5, Batch 500, Loss: 1139218944.0000\n",
            "Epoch 1/5, Batch 550, Loss: 578930816.0000\n",
            "Epoch 1/5, Batch 600, Loss: 1095343360.0000\n",
            "Epoch 1/5, Batch 650, Loss: 188206928.0000\n",
            "Epoch 1/5, Batch 700, Loss: 1303230976.0000\n",
            "Epoch 1/5, Batch 750, Loss: 245776448.0000\n",
            "Epoch 1/5, Batch 800, Loss: 273195776.0000\n",
            "Epoch 1/5, Batch 850, Loss: 1853192960.0000\n",
            "Epoch 1/5, Batch 900, Loss: 350159360.0000\n",
            "Epoch 1/5, Batch 950, Loss: 1241066368.0000\n",
            "Epoch 1/5, Batch 1000, Loss: 956236672.0000\n",
            "Epoch 1/5, Batch 1050, Loss: 804880640.0000\n",
            "Epoch 1/5, Batch 1100, Loss: 189168224.0000\n",
            "Epoch 1/5, Batch 1150, Loss: 366754432.0000\n",
            "Epoch 1/5, Batch 1200, Loss: 1364132992.0000\n",
            "Epoch 1/5, Batch 1250, Loss: 712320000.0000\n",
            "Epoch 1/5, Batch 1300, Loss: 269076096.0000\n",
            "Epoch 1/5, Batch 1350, Loss: 914364544.0000\n",
            "Epoch 1/5, Batch 1400, Loss: 866136128.0000\n",
            "Epoch 1/5, Batch 1450, Loss: 290433952.0000\n",
            "Epoch 1/5, Batch 1500, Loss: 506692064.0000\n",
            "Epoch 1/5, Batch 1550, Loss: 267768160.0000\n",
            "Epoch 1/5, Batch 1600, Loss: 926645184.0000\n",
            "Epoch 1/5, Batch 1650, Loss: 336109952.0000\n",
            "Epoch 1/5, Batch 1700, Loss: 730797696.0000\n",
            "Epoch 1/5, Batch 1750, Loss: 629973632.0000\n",
            "Epoch 1/5, Batch 1800, Loss: 1012142080.0000\n",
            "Epoch 1/5, Batch 1850, Loss: 522096736.0000\n",
            "Epoch 1/5, Batch 1900, Loss: 748685440.0000\n",
            "Epoch 1/5, Batch 1950, Loss: 718708864.0000\n",
            "Epoch 1/5, Batch 2000, Loss: 137256832.0000\n",
            "Epoch 1/5, Batch 2050, Loss: 375138624.0000\n",
            "Epoch 1/5, Batch 2100, Loss: 1518337792.0000\n",
            "Epoch 1/5, Batch 2150, Loss: 1137045760.0000\n",
            "Epoch 1/5, Batch 2200, Loss: 253017600.0000\n",
            "Epoch 1/5, Batch 2250, Loss: 574256000.0000\n",
            "Epoch 1/5, Batch 2300, Loss: 665897472.0000\n",
            "Epoch 1/5, Batch 2350, Loss: 248662784.0000\n",
            "Epoch 1/5, Batch 2400, Loss: 1601954048.0000\n",
            "Epoch 1/5, Batch 2450, Loss: 722960896.0000\n",
            "Epoch 1/5, Batch 2500, Loss: 278537568.0000\n",
            "Epoch 1/5, Batch 2550, Loss: 861404288.0000\n",
            "Epoch 1/5, Batch 2600, Loss: 233913408.0000\n",
            "Epoch 1/5, Batch 2650, Loss: 376004448.0000\n",
            "Epoch 1/5, Batch 2700, Loss: 439356544.0000\n",
            "Epoch 1/5, Batch 2750, Loss: 1902993024.0000\n",
            "Epoch 1/5, Batch 2800, Loss: 1256131072.0000\n",
            "Epoch 1/5, Batch 2850, Loss: 637665664.0000\n",
            "Epoch 1/5, Batch 2900, Loss: 558203968.0000\n",
            "Epoch 1/5, Batch 2950, Loss: 753226752.0000\n",
            "Epoch 1/5, Batch 3000, Loss: 452869248.0000\n",
            "Epoch 1/5, Batch 3050, Loss: 745544768.0000\n",
            "Epoch 1/5, Batch 3100, Loss: 163601344.0000\n",
            "Epoch 1/5, Batch 3150, Loss: 531816768.0000\n",
            "Epoch 1/5, Batch 3200, Loss: 669756416.0000\n",
            "Epoch 1/5, Batch 3250, Loss: 488525600.0000\n",
            "Epoch 1/5, Batch 3300, Loss: 429563456.0000\n",
            "Epoch 1/5, Batch 3350, Loss: 1308733568.0000\n",
            "Epoch 1/5, Batch 3400, Loss: 304965472.0000\n",
            "Epoch 1/5, Batch 3450, Loss: 1148308096.0000\n",
            "Epoch 1/5, Batch 3500, Loss: 1213755648.0000\n",
            "Epoch 1/5, Batch 3550, Loss: 1134358912.0000\n",
            "Epoch 1/5, Batch 3600, Loss: 437528800.0000\n",
            "Epoch 1/5, Batch 3650, Loss: 1475049472.0000\n",
            "Epoch 1/5, Batch 3700, Loss: 230311664.0000\n",
            "Epoch 1/5, Batch 3750, Loss: 386046720.0000\n",
            "Epoch 1/5, Batch 3800, Loss: 1009575296.0000\n",
            "Epoch 1/5, Batch 3850, Loss: 255722112.0000\n",
            "Epoch 1/5, Batch 3900, Loss: 578984448.0000\n",
            "Epoch 1/5, Batch 3950, Loss: 250886240.0000\n",
            "Epoch 1/5, Batch 4000, Loss: 152082544.0000\n",
            "Epoch 1/5, Batch 4050, Loss: 704767744.0000\n",
            "Epoch 1/5, Batch 4100, Loss: 546046400.0000\n",
            "Epoch 1/5, Batch 4150, Loss: 554112192.0000\n",
            "Epoch 1/5, Batch 4200, Loss: 221322896.0000\n",
            "Epoch 1/5, Batch 4250, Loss: 1307183232.0000\n",
            "Epoch 1/5, Batch 4300, Loss: 1279980800.0000\n",
            "Epoch 1/5, Batch 4350, Loss: 287607328.0000\n",
            "Epoch 1/5, Batch 4400, Loss: 1700976640.0000\n",
            "Epoch 1/5, Batch 4450, Loss: 431987488.0000\n",
            "Epoch 1/5, Batch 4500, Loss: 390044512.0000\n",
            "Epoch 1/5, Batch 4550, Loss: 438828384.0000\n",
            "Epoch 1/5, Batch 4600, Loss: 180974016.0000\n",
            "Epoch 1/5, Batch 4650, Loss: 359768320.0000\n",
            "Epoch 1/5, Batch 4700, Loss: 579926400.0000\n",
            "Epoch 1/5, Batch 4750, Loss: 530598304.0000\n",
            "Epoch 1/5, Batch 4800, Loss: 534905184.0000\n",
            "Epoch 1/5, Batch 4850, Loss: 311209440.0000\n",
            "Epoch 1/5, Batch 4900, Loss: 159995936.0000\n",
            "Epoch 1/5, Batch 4950, Loss: 158977664.0000\n",
            "Epoch 1/5, Batch 5000, Loss: 1149117952.0000\n",
            "Epoch 1/5, Batch 5050, Loss: 1574847744.0000\n",
            "Epoch 1/5, Batch 5100, Loss: 701491008.0000\n",
            "Epoch 1/5, Batch 5150, Loss: 275489856.0000\n",
            "Epoch 1/5, Batch 5200, Loss: 741458816.0000\n",
            "Epoch 1/5, Batch 5250, Loss: 510698432.0000\n",
            "Epoch 1/5, Batch 5300, Loss: 590948800.0000\n",
            "Epoch 1/5, Batch 5350, Loss: 209238112.0000\n",
            "Epoch 1/5, Batch 5400, Loss: 805170176.0000\n",
            "Epoch 1/5, Batch 5450, Loss: 279354016.0000\n",
            "Epoch 1/5, Batch 5500, Loss: 1088510080.0000\n",
            "Epoch 1/5, Batch 5550, Loss: 1099393408.0000\n",
            "Epoch 1/5, Batch 5600, Loss: 79609856.0000\n",
            "Epoch 1/5, Batch 5650, Loss: 338923744.0000\n",
            "Epoch 1/5, Batch 5700, Loss: 273380512.0000\n",
            "Epoch 1/5, Batch 5750, Loss: 102607624.0000\n",
            "Epoch 1/5, Batch 5800, Loss: 1325184256.0000\n",
            "Epoch 1/5, Batch 5850, Loss: 219943728.0000\n",
            "Epoch 1/5, Batch 5900, Loss: 447532224.0000\n",
            "Epoch 1/5, Batch 5950, Loss: 647544448.0000\n",
            "Epoch 1/5, Batch 6000, Loss: 131709560.0000\n",
            "Epoch 1/5, Batch 6050, Loss: 433338880.0000\n",
            "Epoch 1/5, Batch 6100, Loss: 640588928.0000\n",
            "Epoch 1/5, Batch 6150, Loss: 1038963456.0000\n",
            "Epoch 1/5, Batch 6200, Loss: 827827904.0000\n",
            "Epoch 1/5, Batch 6250, Loss: 82163808.0000\n",
            "Epoch 1/5, Batch 6300, Loss: 438819552.0000\n",
            "Epoch 1/5, Batch 6350, Loss: 187197504.0000\n",
            "Epoch 1/5, Batch 6400, Loss: 311537536.0000\n",
            "Epoch 1/5, Batch 6450, Loss: 30361868.0000\n",
            "Epoch 1/5, Batch 6500, Loss: 670095936.0000\n",
            "Epoch 1/5:\n",
            "  Train Loss: 686661020.3385\n",
            "  Val Loss: 278620720.5750\n",
            "  Val MAE: 7046.70\n",
            "  Val RMSE: 16693.05\n",
            "  Val MAPE: 134.10%\n",
            "  Val R²: 0.2552\n",
            "Epoch 2/5, Batch 0, Loss: 223572256.0000\n",
            "Epoch 2/5, Batch 50, Loss: 129131016.0000\n",
            "Epoch 2/5, Batch 100, Loss: 492978048.0000\n",
            "Epoch 2/5, Batch 150, Loss: 1071601024.0000\n",
            "Epoch 2/5, Batch 200, Loss: 196092624.0000\n",
            "Epoch 2/5, Batch 250, Loss: 140756320.0000\n",
            "Epoch 2/5, Batch 300, Loss: 404554048.0000\n",
            "Epoch 2/5, Batch 350, Loss: 1014013824.0000\n",
            "Epoch 2/5, Batch 400, Loss: 246603648.0000\n",
            "Epoch 2/5, Batch 450, Loss: 1142344832.0000\n",
            "Epoch 2/5, Batch 500, Loss: 233261520.0000\n",
            "Epoch 2/5, Batch 550, Loss: 870132416.0000\n",
            "Epoch 2/5, Batch 600, Loss: 1305133184.0000\n",
            "Epoch 2/5, Batch 650, Loss: 233261184.0000\n",
            "Epoch 2/5, Batch 700, Loss: 279051264.0000\n",
            "Epoch 2/5, Batch 750, Loss: 273424512.0000\n",
            "Epoch 2/5, Batch 800, Loss: 410933184.0000\n",
            "Epoch 2/5, Batch 850, Loss: 641041024.0000\n",
            "Epoch 2/5, Batch 900, Loss: 139581984.0000\n",
            "Epoch 2/5, Batch 950, Loss: 360116512.0000\n",
            "Epoch 2/5, Batch 1000, Loss: 170741552.0000\n",
            "Epoch 2/5, Batch 1050, Loss: 196336896.0000\n",
            "Epoch 2/5, Batch 1100, Loss: 607295488.0000\n",
            "Epoch 2/5, Batch 1150, Loss: 642212096.0000\n",
            "Epoch 2/5, Batch 1200, Loss: 553516608.0000\n",
            "Epoch 2/5, Batch 1250, Loss: 349881280.0000\n",
            "Epoch 2/5, Batch 1300, Loss: 132615696.0000\n",
            "Epoch 2/5, Batch 1350, Loss: 398648288.0000\n",
            "Epoch 2/5, Batch 1400, Loss: 51942012.0000\n",
            "Epoch 2/5, Batch 1450, Loss: 1153622912.0000\n",
            "Epoch 2/5, Batch 1500, Loss: 454723744.0000\n",
            "Epoch 2/5, Batch 1550, Loss: 102583192.0000\n",
            "Epoch 2/5, Batch 1600, Loss: 61135232.0000\n",
            "Epoch 2/5, Batch 1650, Loss: 677777728.0000\n",
            "Epoch 2/5, Batch 1700, Loss: 375148480.0000\n",
            "Epoch 2/5, Batch 1750, Loss: 633480960.0000\n",
            "Epoch 2/5, Batch 1800, Loss: 259447104.0000\n",
            "Epoch 2/5, Batch 1850, Loss: 844936000.0000\n",
            "Epoch 2/5, Batch 1900, Loss: 457130688.0000\n",
            "Epoch 2/5, Batch 1950, Loss: 148020752.0000\n",
            "Epoch 2/5, Batch 2000, Loss: 541310784.0000\n",
            "Epoch 2/5, Batch 2050, Loss: 90790112.0000\n",
            "Epoch 2/5, Batch 2100, Loss: 73178000.0000\n",
            "Epoch 2/5, Batch 2150, Loss: 27540646.0000\n",
            "Epoch 2/5, Batch 2200, Loss: 260895184.0000\n",
            "Epoch 2/5, Batch 2250, Loss: 469041952.0000\n",
            "Epoch 2/5, Batch 2300, Loss: 225747504.0000\n",
            "Epoch 2/5, Batch 2350, Loss: 156266000.0000\n",
            "Epoch 2/5, Batch 2400, Loss: 302124288.0000\n",
            "Epoch 2/5, Batch 2450, Loss: 1009005248.0000\n",
            "Epoch 2/5, Batch 2500, Loss: 110397920.0000\n",
            "Epoch 2/5, Batch 2550, Loss: 203975712.0000\n",
            "Epoch 2/5, Batch 2600, Loss: 313474976.0000\n",
            "Epoch 2/5, Batch 2650, Loss: 256799440.0000\n",
            "Epoch 2/5, Batch 2700, Loss: 237031664.0000\n",
            "Epoch 2/5, Batch 2750, Loss: 18400916.0000\n",
            "Epoch 2/5, Batch 2800, Loss: 404069504.0000\n",
            "Epoch 2/5, Batch 2850, Loss: 248805648.0000\n",
            "Epoch 2/5, Batch 2900, Loss: 214712256.0000\n",
            "Epoch 2/5, Batch 2950, Loss: 139610080.0000\n",
            "Epoch 2/5, Batch 3000, Loss: 488549728.0000\n",
            "Epoch 2/5, Batch 3050, Loss: 345111936.0000\n",
            "Epoch 2/5, Batch 3100, Loss: 659985600.0000\n",
            "Epoch 2/5, Batch 3150, Loss: 20512040.0000\n",
            "Epoch 2/5, Batch 3200, Loss: 488532576.0000\n",
            "Epoch 2/5, Batch 3250, Loss: 174770352.0000\n",
            "Epoch 2/5, Batch 3300, Loss: 280321440.0000\n",
            "Epoch 2/5, Batch 3350, Loss: 291720704.0000\n",
            "Epoch 2/5, Batch 3400, Loss: 85119664.0000\n",
            "Epoch 2/5, Batch 3450, Loss: 161990896.0000\n",
            "Epoch 2/5, Batch 3500, Loss: 242889200.0000\n",
            "Epoch 2/5, Batch 3550, Loss: 92931040.0000\n",
            "Epoch 2/5, Batch 3600, Loss: 234836512.0000\n",
            "Epoch 2/5, Batch 3650, Loss: 187063776.0000\n",
            "Epoch 2/5, Batch 3700, Loss: 139369920.0000\n",
            "Epoch 2/5, Batch 3750, Loss: 399706304.0000\n",
            "Epoch 2/5, Batch 3800, Loss: 150726352.0000\n",
            "Epoch 2/5, Batch 3850, Loss: 332469632.0000\n",
            "Epoch 2/5, Batch 3900, Loss: 708095104.0000\n",
            "Epoch 2/5, Batch 3950, Loss: 138439200.0000\n",
            "Epoch 2/5, Batch 4000, Loss: 408103424.0000\n",
            "Epoch 2/5, Batch 4050, Loss: 365606272.0000\n",
            "Epoch 2/5, Batch 4100, Loss: 986150784.0000\n",
            "Epoch 2/5, Batch 4150, Loss: 171086640.0000\n",
            "Epoch 2/5, Batch 4200, Loss: 529888928.0000\n",
            "Epoch 2/5, Batch 4250, Loss: 632735488.0000\n",
            "Epoch 2/5, Batch 4300, Loss: 413780544.0000\n",
            "Epoch 2/5, Batch 4350, Loss: 154032128.0000\n",
            "Epoch 2/5, Batch 4400, Loss: 454688352.0000\n",
            "Epoch 2/5, Batch 4450, Loss: 187418496.0000\n",
            "Epoch 2/5, Batch 4500, Loss: 345002208.0000\n",
            "Epoch 2/5, Batch 4550, Loss: 502834976.0000\n",
            "Epoch 2/5, Batch 4600, Loss: 289397472.0000\n",
            "Epoch 2/5, Batch 4650, Loss: 487133696.0000\n",
            "Epoch 2/5, Batch 4700, Loss: 632443200.0000\n",
            "Epoch 2/5, Batch 4750, Loss: 143686496.0000\n",
            "Epoch 2/5, Batch 4800, Loss: 145904912.0000\n",
            "Epoch 2/5, Batch 4850, Loss: 308937792.0000\n",
            "Epoch 2/5, Batch 4900, Loss: 93340688.0000\n",
            "Epoch 2/5, Batch 4950, Loss: 138160944.0000\n",
            "Epoch 2/5, Batch 5000, Loss: 338005280.0000\n",
            "Epoch 2/5, Batch 5050, Loss: 176046096.0000\n",
            "Epoch 2/5, Batch 5100, Loss: 527723296.0000\n",
            "Epoch 2/5, Batch 5150, Loss: 688872448.0000\n",
            "Epoch 2/5, Batch 5200, Loss: 893880000.0000\n",
            "Epoch 2/5, Batch 5250, Loss: 179869872.0000\n",
            "Epoch 2/5, Batch 5300, Loss: 124953488.0000\n",
            "Epoch 2/5, Batch 5350, Loss: 200028608.0000\n",
            "Epoch 2/5, Batch 5400, Loss: 334444992.0000\n",
            "Epoch 2/5, Batch 5450, Loss: 513596000.0000\n",
            "Epoch 2/5, Batch 5500, Loss: 174827216.0000\n",
            "Epoch 2/5, Batch 5550, Loss: 110991240.0000\n",
            "Epoch 2/5, Batch 5600, Loss: 983339648.0000\n",
            "Epoch 2/5, Batch 5650, Loss: 363311776.0000\n",
            "Epoch 2/5, Batch 5700, Loss: 168985968.0000\n",
            "Epoch 2/5, Batch 5750, Loss: 520379104.0000\n",
            "Epoch 2/5, Batch 5800, Loss: 193595456.0000\n",
            "Epoch 2/5, Batch 5850, Loss: 683363840.0000\n",
            "Epoch 2/5, Batch 5900, Loss: 888070400.0000\n",
            "Epoch 2/5, Batch 5950, Loss: 995758336.0000\n",
            "Epoch 2/5, Batch 6000, Loss: 21356548.0000\n",
            "Epoch 2/5, Batch 6050, Loss: 1074894464.0000\n",
            "Epoch 2/5, Batch 6100, Loss: 333234624.0000\n",
            "Epoch 2/5, Batch 6150, Loss: 342639296.0000\n",
            "Epoch 2/5, Batch 6200, Loss: 448051456.0000\n",
            "Epoch 2/5, Batch 6250, Loss: 91431248.0000\n",
            "Epoch 2/5, Batch 6300, Loss: 363255424.0000\n",
            "Epoch 2/5, Batch 6350, Loss: 153595888.0000\n",
            "Epoch 2/5, Batch 6400, Loss: 160362496.0000\n",
            "Epoch 2/5, Batch 6450, Loss: 177310768.0000\n",
            "Epoch 2/5, Batch 6500, Loss: 29539058.0000\n",
            "Epoch 2/5:\n",
            "  Train Loss: 391568386.5232\n",
            "  Val Loss: 143772866.9102\n",
            "  Val MAE: 5291.62\n",
            "  Val RMSE: 11991.34\n",
            "  Val MAPE: 1239.50%\n",
            "  Val R²: 0.6157\n",
            "Epoch 3/5, Batch 0, Loss: 268219760.0000\n",
            "Epoch 3/5, Batch 50, Loss: 70510464.0000\n",
            "Epoch 3/5, Batch 100, Loss: 464038656.0000\n",
            "Epoch 3/5, Batch 150, Loss: 1185376640.0000\n",
            "Epoch 3/5, Batch 200, Loss: 185776864.0000\n",
            "Epoch 3/5, Batch 250, Loss: 356912832.0000\n",
            "Epoch 3/5, Batch 300, Loss: 416295840.0000\n",
            "Epoch 3/5, Batch 350, Loss: 580321088.0000\n",
            "Epoch 3/5, Batch 400, Loss: 302739744.0000\n",
            "Epoch 3/5, Batch 450, Loss: 129245568.0000\n",
            "Epoch 3/5, Batch 500, Loss: 1239047168.0000\n",
            "Epoch 3/5, Batch 550, Loss: 1900973184.0000\n",
            "Epoch 3/5, Batch 600, Loss: 231493536.0000\n",
            "Epoch 3/5, Batch 650, Loss: 195745600.0000\n",
            "Epoch 3/5, Batch 700, Loss: 243677296.0000\n",
            "Epoch 3/5, Batch 750, Loss: 142587936.0000\n",
            "Epoch 3/5, Batch 800, Loss: 277614560.0000\n",
            "Epoch 3/5, Batch 850, Loss: 164381504.0000\n",
            "Epoch 3/5, Batch 900, Loss: 143606608.0000\n",
            "Epoch 3/5, Batch 950, Loss: 88929424.0000\n",
            "Epoch 3/5, Batch 1000, Loss: 198608384.0000\n",
            "Epoch 3/5, Batch 1050, Loss: 352356032.0000\n",
            "Epoch 3/5, Batch 1100, Loss: 856998272.0000\n",
            "Epoch 3/5, Batch 1150, Loss: 410477024.0000\n",
            "Epoch 3/5, Batch 1200, Loss: 143175648.0000\n",
            "Epoch 3/5, Batch 1250, Loss: 168216144.0000\n",
            "Epoch 3/5, Batch 1300, Loss: 735673408.0000\n",
            "Epoch 3/5, Batch 1350, Loss: 141174144.0000\n",
            "Epoch 3/5, Batch 1400, Loss: 2021862912.0000\n",
            "Epoch 3/5, Batch 1450, Loss: 529153600.0000\n",
            "Epoch 3/5, Batch 1500, Loss: 464628608.0000\n",
            "Epoch 3/5, Batch 1550, Loss: 785755648.0000\n",
            "Epoch 3/5, Batch 1600, Loss: 968945920.0000\n",
            "Epoch 3/5, Batch 1650, Loss: 296395040.0000\n",
            "Epoch 3/5, Batch 1700, Loss: 224566272.0000\n",
            "Epoch 3/5, Batch 1750, Loss: 1055203328.0000\n",
            "Epoch 3/5, Batch 1800, Loss: 269148544.0000\n",
            "Epoch 3/5, Batch 1850, Loss: 760379136.0000\n",
            "Epoch 3/5, Batch 1900, Loss: 1145670272.0000\n",
            "Epoch 3/5, Batch 1950, Loss: 299069088.0000\n",
            "Epoch 3/5, Batch 2000, Loss: 93205280.0000\n",
            "Epoch 3/5, Batch 2050, Loss: 342659776.0000\n",
            "Epoch 3/5, Batch 2100, Loss: 421156256.0000\n",
            "Epoch 3/5, Batch 2150, Loss: 413087744.0000\n",
            "Epoch 3/5, Batch 2200, Loss: 205163008.0000\n",
            "Epoch 3/5, Batch 2250, Loss: 410964064.0000\n",
            "Epoch 3/5, Batch 2300, Loss: 186618544.0000\n",
            "Epoch 3/5, Batch 2350, Loss: 384264096.0000\n",
            "Epoch 3/5, Batch 2400, Loss: 449263296.0000\n",
            "Epoch 3/5, Batch 2450, Loss: 356049920.0000\n",
            "Epoch 3/5, Batch 2500, Loss: 707749888.0000\n",
            "Epoch 3/5, Batch 2550, Loss: 445436544.0000\n",
            "Epoch 3/5, Batch 2600, Loss: 274509184.0000\n",
            "Epoch 3/5, Batch 2650, Loss: 413032416.0000\n",
            "Epoch 3/5, Batch 2700, Loss: 252668448.0000\n",
            "Epoch 3/5, Batch 2750, Loss: 105125008.0000\n",
            "Epoch 3/5, Batch 2800, Loss: 229666608.0000\n",
            "Epoch 3/5, Batch 2850, Loss: 349934912.0000\n",
            "Epoch 3/5, Batch 2900, Loss: 166536736.0000\n",
            "Epoch 3/5, Batch 2950, Loss: 242502544.0000\n",
            "Epoch 3/5, Batch 3000, Loss: 206998560.0000\n",
            "Epoch 3/5, Batch 3050, Loss: 124479968.0000\n",
            "Epoch 3/5, Batch 3100, Loss: 57397820.0000\n",
            "Epoch 3/5, Batch 3150, Loss: 463392864.0000\n",
            "Epoch 3/5, Batch 3200, Loss: 323062976.0000\n",
            "Epoch 3/5, Batch 3250, Loss: 114676128.0000\n",
            "Epoch 3/5, Batch 3300, Loss: 309088256.0000\n",
            "Epoch 3/5, Batch 3350, Loss: 527470848.0000\n",
            "Epoch 3/5, Batch 3400, Loss: 514311488.0000\n",
            "Epoch 3/5, Batch 3450, Loss: 1012118400.0000\n",
            "Epoch 3/5, Batch 3500, Loss: 855228224.0000\n",
            "Epoch 3/5, Batch 3550, Loss: 171236272.0000\n",
            "Epoch 3/5, Batch 3600, Loss: 737168704.0000\n",
            "Epoch 3/5, Batch 3650, Loss: 789639104.0000\n",
            "Epoch 3/5, Batch 3700, Loss: 265839552.0000\n",
            "Epoch 3/5, Batch 3750, Loss: 1092917888.0000\n",
            "Epoch 3/5, Batch 3800, Loss: 440206656.0000\n",
            "Epoch 3/5, Batch 3850, Loss: 325390848.0000\n",
            "Epoch 3/5, Batch 3900, Loss: 425954208.0000\n",
            "Epoch 3/5, Batch 3950, Loss: 248609648.0000\n",
            "Epoch 3/5, Batch 4000, Loss: 296047040.0000\n",
            "Epoch 3/5, Batch 4050, Loss: 251602096.0000\n",
            "Epoch 3/5, Batch 4100, Loss: 420634432.0000\n",
            "Epoch 3/5, Batch 4150, Loss: 597031040.0000\n",
            "Epoch 3/5, Batch 4200, Loss: 594351104.0000\n",
            "Epoch 3/5, Batch 4250, Loss: 582130880.0000\n",
            "Epoch 3/5, Batch 4300, Loss: 777728256.0000\n",
            "Epoch 3/5, Batch 4350, Loss: 258933920.0000\n",
            "Epoch 3/5, Batch 4400, Loss: 279606848.0000\n",
            "Epoch 3/5, Batch 4450, Loss: 448931840.0000\n",
            "Epoch 3/5, Batch 4500, Loss: 771986432.0000\n",
            "Epoch 3/5, Batch 4550, Loss: 713767296.0000\n",
            "Epoch 3/5, Batch 4600, Loss: 396530880.0000\n",
            "Epoch 3/5, Batch 4650, Loss: 906159808.0000\n",
            "Epoch 3/5, Batch 4700, Loss: 284615904.0000\n",
            "Epoch 3/5, Batch 4750, Loss: 379646208.0000\n",
            "Epoch 3/5, Batch 4800, Loss: 1061018816.0000\n",
            "Epoch 3/5, Batch 4850, Loss: 630677632.0000\n",
            "Epoch 3/5, Batch 4900, Loss: 363140384.0000\n",
            "Epoch 3/5, Batch 4950, Loss: 444313376.0000\n",
            "Epoch 3/5, Batch 5000, Loss: 661252416.0000\n",
            "Epoch 3/5, Batch 5050, Loss: 600468864.0000\n",
            "Epoch 3/5, Batch 5100, Loss: 567670656.0000\n",
            "Epoch 3/5, Batch 5150, Loss: 525319744.0000\n",
            "Epoch 3/5, Batch 5200, Loss: 993368320.0000\n",
            "Epoch 3/5, Batch 5250, Loss: 455667168.0000\n",
            "Epoch 3/5, Batch 5300, Loss: 501099840.0000\n",
            "Epoch 3/5, Batch 5350, Loss: 356351040.0000\n",
            "Epoch 3/5, Batch 5400, Loss: 217329600.0000\n",
            "Epoch 3/5, Batch 5450, Loss: 153785968.0000\n",
            "Epoch 3/5, Batch 5500, Loss: 332395328.0000\n",
            "Epoch 3/5, Batch 5550, Loss: 1401230976.0000\n",
            "Epoch 3/5, Batch 5600, Loss: 654724352.0000\n",
            "Epoch 3/5, Batch 5650, Loss: 481599616.0000\n",
            "Epoch 3/5, Batch 5700, Loss: 207529840.0000\n",
            "Epoch 3/5, Batch 5750, Loss: 250659360.0000\n",
            "Epoch 3/5, Batch 5800, Loss: 254181632.0000\n",
            "Epoch 3/5, Batch 5850, Loss: 793579392.0000\n",
            "Epoch 3/5, Batch 5900, Loss: 625367808.0000\n",
            "Epoch 3/5, Batch 5950, Loss: 650415296.0000\n",
            "Epoch 3/5, Batch 6000, Loss: 410952288.0000\n",
            "Epoch 3/5, Batch 6050, Loss: 924256896.0000\n",
            "Epoch 3/5, Batch 6100, Loss: 437205184.0000\n",
            "Epoch 3/5, Batch 6150, Loss: 506924416.0000\n",
            "Epoch 3/5, Batch 6200, Loss: 185868096.0000\n",
            "Epoch 3/5, Batch 6250, Loss: 549869184.0000\n",
            "Epoch 3/5, Batch 6300, Loss: 303201536.0000\n",
            "Epoch 3/5, Batch 6350, Loss: 1879747072.0000\n",
            "Epoch 3/5, Batch 6400, Loss: 317644224.0000\n",
            "Epoch 3/5, Batch 6450, Loss: 223443616.0000\n",
            "Epoch 3/5, Batch 6500, Loss: 821065984.0000\n",
            "Epoch 3/5:\n",
            "  Train Loss: 475775951.6523\n",
            "  Val Loss: 393528024.8850\n",
            "  Val MAE: 15230.92\n",
            "  Val RMSE: 19837.89\n",
            "  Val MAPE: 21925.22%\n",
            "  Val R²: -0.0518\n",
            "Epoch 4/5, Batch 0, Loss: 510699648.0000\n",
            "Epoch 4/5, Batch 50, Loss: 749778112.0000\n",
            "Epoch 4/5, Batch 100, Loss: 507521152.0000\n",
            "Epoch 4/5, Batch 150, Loss: 725894144.0000\n",
            "Epoch 4/5, Batch 200, Loss: 1021303040.0000\n",
            "Epoch 4/5, Batch 250, Loss: 1670415104.0000\n",
            "Epoch 4/5, Batch 300, Loss: 311224960.0000\n",
            "Epoch 4/5, Batch 350, Loss: 615939712.0000\n",
            "Epoch 4/5, Batch 400, Loss: 163113760.0000\n",
            "Epoch 4/5, Batch 450, Loss: 391611264.0000\n",
            "Epoch 4/5, Batch 500, Loss: 731522560.0000\n",
            "Epoch 4/5, Batch 550, Loss: 1205825408.0000\n",
            "Epoch 4/5, Batch 600, Loss: 752403712.0000\n",
            "Epoch 4/5, Batch 650, Loss: 674885952.0000\n",
            "Epoch 4/5, Batch 700, Loss: 765015168.0000\n",
            "Epoch 4/5, Batch 750, Loss: 116942432.0000\n",
            "Epoch 4/5, Batch 800, Loss: 260988624.0000\n",
            "Epoch 4/5, Batch 850, Loss: 207754720.0000\n",
            "Epoch 4/5, Batch 900, Loss: 239937872.0000\n",
            "Epoch 4/5, Batch 950, Loss: 301759040.0000\n",
            "Epoch 4/5, Batch 1000, Loss: 545569408.0000\n",
            "Epoch 4/5, Batch 1050, Loss: 1157899648.0000\n",
            "Epoch 4/5, Batch 1100, Loss: 395605600.0000\n",
            "Epoch 4/5, Batch 1150, Loss: 1038536064.0000\n",
            "Epoch 4/5, Batch 1200, Loss: 325070752.0000\n",
            "Epoch 4/5, Batch 1250, Loss: 337300096.0000\n",
            "Epoch 4/5, Batch 1300, Loss: 373380864.0000\n",
            "Epoch 4/5, Batch 1350, Loss: 331846368.0000\n",
            "Epoch 4/5, Batch 1400, Loss: 372661376.0000\n",
            "Epoch 4/5, Batch 1450, Loss: 291602496.0000\n",
            "Epoch 4/5, Batch 1500, Loss: 428833152.0000\n",
            "Epoch 4/5, Batch 1550, Loss: 888840704.0000\n",
            "Epoch 4/5, Batch 1600, Loss: 317988928.0000\n",
            "Epoch 4/5, Batch 1650, Loss: 596408064.0000\n",
            "Epoch 4/5, Batch 1700, Loss: 721301696.0000\n",
            "Epoch 4/5, Batch 1750, Loss: 155377168.0000\n",
            "Epoch 4/5, Batch 1800, Loss: 375086400.0000\n",
            "Epoch 4/5, Batch 1850, Loss: 591923456.0000\n",
            "Epoch 4/5, Batch 1900, Loss: 329026368.0000\n",
            "Epoch 4/5, Batch 1950, Loss: 879476416.0000\n",
            "Epoch 4/5, Batch 2000, Loss: 142642272.0000\n",
            "Epoch 4/5, Batch 2050, Loss: 186496384.0000\n",
            "Epoch 4/5, Batch 2100, Loss: 235225536.0000\n",
            "Epoch 4/5, Batch 2150, Loss: 1294604032.0000\n",
            "Epoch 4/5, Batch 2200, Loss: 1138739072.0000\n",
            "Epoch 4/5, Batch 2250, Loss: 552544896.0000\n",
            "Epoch 4/5, Batch 2300, Loss: 148903360.0000\n",
            "Epoch 4/5, Batch 2350, Loss: 187003136.0000\n",
            "Epoch 4/5, Batch 2400, Loss: 998263168.0000\n",
            "Epoch 4/5, Batch 2450, Loss: 169041760.0000\n",
            "Epoch 4/5, Batch 2500, Loss: 660917248.0000\n",
            "Epoch 4/5, Batch 2550, Loss: 231201920.0000\n",
            "Epoch 4/5, Batch 2600, Loss: 838486144.0000\n",
            "Epoch 4/5, Batch 2650, Loss: 684646848.0000\n",
            "Epoch 4/5, Batch 2700, Loss: 608117376.0000\n",
            "Epoch 4/5, Batch 2750, Loss: 251599200.0000\n",
            "Epoch 4/5, Batch 2800, Loss: 276595008.0000\n",
            "Epoch 4/5, Batch 2850, Loss: 458627328.0000\n",
            "Epoch 4/5, Batch 2900, Loss: 298505216.0000\n",
            "Epoch 4/5, Batch 2950, Loss: 289121152.0000\n",
            "Epoch 4/5, Batch 3000, Loss: 793413824.0000\n",
            "Epoch 4/5, Batch 3050, Loss: 286736960.0000\n",
            "Epoch 4/5, Batch 3100, Loss: 791347008.0000\n",
            "Epoch 4/5, Batch 3150, Loss: 193148016.0000\n",
            "Epoch 4/5, Batch 3200, Loss: 350070016.0000\n",
            "Epoch 4/5, Batch 3250, Loss: 2264685568.0000\n",
            "Epoch 4/5, Batch 3300, Loss: 504747712.0000\n",
            "Epoch 4/5, Batch 3350, Loss: 1815531392.0000\n",
            "Epoch 4/5, Batch 3400, Loss: 509423616.0000\n",
            "Epoch 4/5, Batch 3450, Loss: 197525952.0000\n",
            "Epoch 4/5, Batch 3500, Loss: 337650688.0000\n",
            "Epoch 4/5, Batch 3550, Loss: 302453952.0000\n",
            "Epoch 4/5, Batch 3600, Loss: 393249280.0000\n",
            "Epoch 4/5, Batch 3650, Loss: 234082304.0000\n",
            "Epoch 4/5, Batch 3700, Loss: 563417088.0000\n",
            "Epoch 4/5, Batch 3750, Loss: 360167488.0000\n",
            "Epoch 4/5, Batch 3800, Loss: 1299722752.0000\n",
            "Epoch 4/5, Batch 3850, Loss: 206880208.0000\n",
            "Epoch 4/5, Batch 3900, Loss: 289466240.0000\n",
            "Epoch 4/5, Batch 3950, Loss: 829852288.0000\n",
            "Epoch 4/5, Batch 4000, Loss: 533725632.0000\n",
            "Epoch 4/5, Batch 4050, Loss: 662280576.0000\n",
            "Epoch 4/5, Batch 4100, Loss: 186110512.0000\n",
            "Epoch 4/5, Batch 4150, Loss: 722991616.0000\n",
            "Epoch 4/5, Batch 4200, Loss: 332396928.0000\n",
            "Epoch 4/5, Batch 4250, Loss: 249374304.0000\n",
            "Epoch 4/5, Batch 4300, Loss: 163247248.0000\n",
            "Epoch 4/5, Batch 4350, Loss: 370113952.0000\n",
            "Epoch 4/5, Batch 4400, Loss: 394109952.0000\n",
            "Epoch 4/5, Batch 4450, Loss: 268018960.0000\n",
            "Epoch 4/5, Batch 4500, Loss: 239679840.0000\n",
            "Epoch 4/5, Batch 4550, Loss: 1472464512.0000\n",
            "Epoch 4/5, Batch 4600, Loss: 290346848.0000\n",
            "Epoch 4/5, Batch 4650, Loss: 392514848.0000\n",
            "Epoch 4/5, Batch 4700, Loss: 914518400.0000\n",
            "Epoch 4/5, Batch 4750, Loss: 392605952.0000\n",
            "Epoch 4/5, Batch 4800, Loss: 366851744.0000\n",
            "Epoch 4/5, Batch 4850, Loss: 293201792.0000\n",
            "Epoch 4/5, Batch 4900, Loss: 844767744.0000\n",
            "Epoch 4/5, Batch 4950, Loss: 769562752.0000\n",
            "Epoch 4/5, Batch 5000, Loss: 254468432.0000\n",
            "Epoch 4/5, Batch 5050, Loss: 394288544.0000\n",
            "Epoch 4/5, Batch 5100, Loss: 278984384.0000\n",
            "Epoch 4/5, Batch 5150, Loss: 423681728.0000\n",
            "Epoch 4/5, Batch 5200, Loss: 615800512.0000\n",
            "Epoch 4/5, Batch 5250, Loss: 1066075776.0000\n",
            "Epoch 4/5, Batch 5300, Loss: 387350720.0000\n",
            "Epoch 4/5, Batch 5350, Loss: 298645248.0000\n",
            "Epoch 4/5, Batch 5400, Loss: 590461440.0000\n",
            "Epoch 4/5, Batch 5450, Loss: 299144832.0000\n",
            "Epoch 4/5, Batch 5500, Loss: 144130000.0000\n",
            "Epoch 4/5, Batch 5550, Loss: 752329344.0000\n",
            "Epoch 4/5, Batch 5600, Loss: 787878528.0000\n",
            "Epoch 4/5, Batch 5650, Loss: 446333440.0000\n",
            "Epoch 4/5, Batch 5700, Loss: 603576512.0000\n",
            "Epoch 4/5, Batch 5750, Loss: 405779744.0000\n",
            "Epoch 4/5, Batch 5800, Loss: 395652608.0000\n",
            "Epoch 4/5, Batch 5850, Loss: 1212435200.0000\n",
            "Epoch 4/5, Batch 5900, Loss: 161960592.0000\n",
            "Epoch 4/5, Batch 5950, Loss: 434362016.0000\n",
            "Epoch 4/5, Batch 6000, Loss: 1240042240.0000\n",
            "Epoch 4/5, Batch 6050, Loss: 503829312.0000\n",
            "Epoch 4/5, Batch 6100, Loss: 767703296.0000\n",
            "Epoch 4/5, Batch 6150, Loss: 380553024.0000\n",
            "Epoch 4/5, Batch 6200, Loss: 458890048.0000\n",
            "Epoch 4/5, Batch 6250, Loss: 563059136.0000\n",
            "Epoch 4/5, Batch 6300, Loss: 186297088.0000\n",
            "Epoch 4/5, Batch 6350, Loss: 1049871616.0000\n",
            "Epoch 4/5, Batch 6400, Loss: 338892480.0000\n",
            "Epoch 4/5, Batch 6450, Loss: 163959904.0000\n",
            "Epoch 4/5, Batch 6500, Loss: 310122144.0000\n",
            "Epoch 4/5:\n",
            "  Train Loss: 552804084.5123\n",
            "  Val Loss: 389233551.8285\n",
            "  Val MAE: 14971.04\n",
            "  Val RMSE: 19729.40\n",
            "  Val MAPE: 21286.96%\n",
            "  Val R²: -0.0404\n",
            "Epoch 5/5, Batch 0, Loss: 605558080.0000\n",
            "Epoch 5/5, Batch 50, Loss: 235817536.0000\n",
            "Epoch 5/5, Batch 100, Loss: 311894944.0000\n",
            "Epoch 5/5, Batch 150, Loss: 353216448.0000\n",
            "Epoch 5/5, Batch 200, Loss: 568800640.0000\n",
            "Epoch 5/5, Batch 250, Loss: 269330400.0000\n",
            "Epoch 5/5, Batch 300, Loss: 719421760.0000\n",
            "Epoch 5/5, Batch 350, Loss: 882591040.0000\n",
            "Epoch 5/5, Batch 400, Loss: 506155072.0000\n",
            "Epoch 5/5, Batch 450, Loss: 402778496.0000\n",
            "Epoch 5/5, Batch 500, Loss: 694825088.0000\n",
            "Epoch 5/5, Batch 550, Loss: 285227840.0000\n",
            "Epoch 5/5, Batch 600, Loss: 204839008.0000\n",
            "Epoch 5/5, Batch 650, Loss: 169783744.0000\n",
            "Epoch 5/5, Batch 700, Loss: 900406720.0000\n",
            "Epoch 5/5, Batch 750, Loss: 454048896.0000\n",
            "Epoch 5/5, Batch 800, Loss: 286244640.0000\n",
            "Epoch 5/5, Batch 850, Loss: 238846320.0000\n",
            "Epoch 5/5, Batch 900, Loss: 542810496.0000\n",
            "Epoch 5/5, Batch 950, Loss: 664044032.0000\n",
            "Epoch 5/5, Batch 1000, Loss: 295295392.0000\n",
            "Epoch 5/5, Batch 1050, Loss: 445951616.0000\n",
            "Epoch 5/5, Batch 1100, Loss: 628105472.0000\n",
            "Epoch 5/5, Batch 1150, Loss: 612148480.0000\n",
            "Epoch 5/5, Batch 1200, Loss: 192265952.0000\n",
            "Epoch 5/5, Batch 1250, Loss: 636258560.0000\n",
            "Epoch 5/5, Batch 1300, Loss: 766863232.0000\n",
            "Epoch 5/5, Batch 1350, Loss: 251961936.0000\n",
            "Epoch 5/5, Batch 1400, Loss: 482271872.0000\n",
            "Epoch 5/5, Batch 1450, Loss: 255992592.0000\n",
            "Epoch 5/5, Batch 1500, Loss: 546784960.0000\n",
            "Epoch 5/5, Batch 1550, Loss: 426176896.0000\n",
            "Epoch 5/5, Batch 1600, Loss: 492523232.0000\n",
            "Epoch 5/5, Batch 1650, Loss: 251256000.0000\n",
            "Epoch 5/5, Batch 1700, Loss: 226913376.0000\n",
            "Epoch 5/5, Batch 1750, Loss: 139227136.0000\n",
            "Epoch 5/5, Batch 1800, Loss: 263533200.0000\n",
            "Epoch 5/5, Batch 1850, Loss: 1169632512.0000\n",
            "Epoch 5/5, Batch 1900, Loss: 687482240.0000\n",
            "Epoch 5/5, Batch 1950, Loss: 175229856.0000\n",
            "Epoch 5/5, Batch 2000, Loss: 393960448.0000\n",
            "Epoch 5/5, Batch 2050, Loss: 353856224.0000\n",
            "Epoch 5/5, Batch 2100, Loss: 314224800.0000\n",
            "Epoch 5/5, Batch 2150, Loss: 484850048.0000\n",
            "Epoch 5/5, Batch 2200, Loss: 1294181888.0000\n",
            "Epoch 5/5, Batch 2250, Loss: 972240128.0000\n",
            "Epoch 5/5, Batch 2300, Loss: 453169344.0000\n",
            "Epoch 5/5, Batch 2350, Loss: 405373344.0000\n",
            "Epoch 5/5, Batch 2400, Loss: 896404608.0000\n",
            "Epoch 5/5, Batch 2450, Loss: 471787040.0000\n",
            "Epoch 5/5, Batch 2500, Loss: 176025056.0000\n",
            "Epoch 5/5, Batch 2550, Loss: 300650560.0000\n",
            "Epoch 5/5, Batch 2600, Loss: 186413248.0000\n",
            "Epoch 5/5, Batch 2650, Loss: 376211648.0000\n",
            "Epoch 5/5, Batch 2700, Loss: 347511136.0000\n",
            "Epoch 5/5, Batch 2750, Loss: 604305280.0000\n",
            "Epoch 5/5, Batch 2800, Loss: 332226496.0000\n",
            "Epoch 5/5, Batch 2850, Loss: 388333824.0000\n",
            "Epoch 5/5, Batch 2900, Loss: 548256000.0000\n",
            "Epoch 5/5, Batch 2950, Loss: 289695360.0000\n",
            "Epoch 5/5, Batch 3000, Loss: 745252096.0000\n",
            "Epoch 5/5, Batch 3050, Loss: 302696512.0000\n",
            "Epoch 5/5, Batch 3100, Loss: 613155904.0000\n",
            "Epoch 5/5, Batch 3150, Loss: 217307264.0000\n",
            "Epoch 5/5, Batch 3200, Loss: 485316032.0000\n",
            "Epoch 5/5, Batch 3250, Loss: 373338176.0000\n",
            "Epoch 5/5, Batch 3300, Loss: 300064576.0000\n",
            "Epoch 5/5, Batch 3350, Loss: 296423712.0000\n",
            "Epoch 5/5, Batch 3400, Loss: 558436416.0000\n",
            "Epoch 5/5, Batch 3450, Loss: 1355192960.0000\n",
            "Epoch 5/5, Batch 3500, Loss: 599115008.0000\n",
            "Epoch 5/5, Batch 3550, Loss: 387952000.0000\n",
            "Epoch 5/5, Batch 3600, Loss: 408214144.0000\n",
            "Epoch 5/5, Batch 3650, Loss: 487825760.0000\n",
            "Epoch 5/5, Batch 3700, Loss: 521359328.0000\n",
            "Epoch 5/5, Batch 3750, Loss: 341366784.0000\n",
            "Epoch 5/5, Batch 3800, Loss: 1160240512.0000\n",
            "Epoch 5/5, Batch 3850, Loss: 383244608.0000\n",
            "Epoch 5/5, Batch 3900, Loss: 360036896.0000\n",
            "Epoch 5/5, Batch 3950, Loss: 797455552.0000\n",
            "Epoch 5/5, Batch 4000, Loss: 887267136.0000\n",
            "Epoch 5/5, Batch 4050, Loss: 627567232.0000\n",
            "Epoch 5/5, Batch 4100, Loss: 379060160.0000\n",
            "Epoch 5/5, Batch 4150, Loss: 553135680.0000\n",
            "Epoch 5/5, Batch 4200, Loss: 1446603520.0000\n",
            "Epoch 5/5, Batch 4250, Loss: 383717440.0000\n",
            "Epoch 5/5, Batch 4300, Loss: 257211008.0000\n",
            "Epoch 5/5, Batch 4350, Loss: 512596096.0000\n",
            "Epoch 5/5, Batch 4400, Loss: 159895440.0000\n",
            "Epoch 5/5, Batch 4450, Loss: 293428864.0000\n",
            "Epoch 5/5, Batch 4500, Loss: 289583712.0000\n",
            "Epoch 5/5, Batch 4550, Loss: 603099136.0000\n",
            "Epoch 5/5, Batch 4600, Loss: 269328896.0000\n",
            "Epoch 5/5, Batch 4650, Loss: 281639808.0000\n",
            "Epoch 5/5, Batch 4700, Loss: 1147657728.0000\n",
            "Epoch 5/5, Batch 4750, Loss: 621671808.0000\n",
            "Epoch 5/5, Batch 4800, Loss: 510396224.0000\n",
            "Epoch 5/5, Batch 4850, Loss: 359287328.0000\n",
            "Epoch 5/5, Batch 4900, Loss: 426356352.0000\n",
            "Epoch 5/5, Batch 4950, Loss: 462405440.0000\n",
            "Epoch 5/5, Batch 5000, Loss: 228951424.0000\n",
            "Epoch 5/5, Batch 5050, Loss: 519707264.0000\n",
            "Epoch 5/5, Batch 5100, Loss: 374094656.0000\n",
            "Epoch 5/5, Batch 5150, Loss: 194295248.0000\n",
            "Epoch 5/5, Batch 5200, Loss: 573784448.0000\n",
            "Epoch 5/5, Batch 5250, Loss: 464038560.0000\n",
            "Epoch 5/5, Batch 5300, Loss: 438204928.0000\n",
            "Epoch 5/5, Batch 5350, Loss: 980302912.0000\n",
            "Epoch 5/5, Batch 5400, Loss: 412011072.0000\n",
            "Epoch 5/5, Batch 5450, Loss: 800248640.0000\n",
            "Epoch 5/5, Batch 5500, Loss: 429448192.0000\n",
            "Epoch 5/5, Batch 5550, Loss: 542132224.0000\n",
            "Epoch 5/5, Batch 5600, Loss: 972259648.0000\n",
            "Epoch 5/5, Batch 5650, Loss: 474700608.0000\n",
            "Epoch 5/5, Batch 5700, Loss: 676226688.0000\n",
            "Epoch 5/5, Batch 5750, Loss: 628970496.0000\n",
            "Epoch 5/5, Batch 5800, Loss: 115753584.0000\n",
            "Epoch 5/5, Batch 5850, Loss: 453354368.0000\n",
            "Epoch 5/5, Batch 5900, Loss: 386829888.0000\n",
            "Epoch 5/5, Batch 5950, Loss: 234495680.0000\n",
            "Epoch 5/5, Batch 6000, Loss: 736456704.0000\n",
            "Epoch 5/5, Batch 6050, Loss: 388439168.0000\n",
            "Epoch 5/5, Batch 6100, Loss: 1079053440.0000\n",
            "Epoch 5/5, Batch 6150, Loss: 1001743808.0000\n",
            "Epoch 5/5, Batch 6200, Loss: 400502464.0000\n",
            "Epoch 5/5, Batch 6250, Loss: 396603040.0000\n",
            "Epoch 5/5, Batch 6300, Loss: 288560960.0000\n",
            "Epoch 5/5, Batch 6350, Loss: 528845888.0000\n",
            "Epoch 5/5, Batch 6400, Loss: 1557251456.0000\n",
            "Epoch 5/5, Batch 6450, Loss: 276169792.0000\n",
            "Epoch 5/5, Batch 6500, Loss: 383838944.0000\n",
            "Epoch 5/5:\n",
            "  Train Loss: 552208989.0392\n",
            "  Val Loss: 386510671.1536\n",
            "  Val MAE: 14792.11\n",
            "  Val RMSE: 19660.30\n",
            "  Val MAPE: 20832.29%\n",
            "  Val R²: -0.0331\n",
            "\n",
            "✓ Cross validation completed and logged to wandb\n",
            "Final validation metrics: MAE=14792.11, RMSE=19660.30, MAPE=20832.29%, R²=-0.0331\n"
          ]
        }
      ],
      "execution_count": 10,
      "id": "cross_validation_code"
    },
    {
      "metadata": {
        "id": "final_training_section"
      },
      "cell_type": "markdown",
      "source": [
        "# Final Training & Model Registry"
      ],
      "id": "final_training_section"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "final_training_code",
        "outputId": "5c99f786-3c44-4582-ef36-970824aea3bc"
      },
      "cell_type": "code",
      "source": [
        "# Start new wandb run for final training\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=\"walmart-sales-forecasting\",\n",
        "    name=\"TFT_Final_Training\",\n",
        "    config={\n",
        "        \"stage\": \"final_training\",\n",
        "        \"model_config\": model_config,\n",
        "        \"num_epochs\": 20,\n",
        "        \"batch_size\": 32\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\n=== FINAL TRAINING ===\")\n",
        "\n",
        "# OPTIMIZATION: Use the efficient processor from CV instead of re-creating\n",
        "print(\"Re-processing data for final training (using efficient processor)...\")\n",
        "\n",
        "# Use the efficient processor we defined in CV instead of the slow original\n",
        "tft_processor_final = EfficientTFTDataProcessor(lookback_window=52, forecast_horizon=1)\n",
        "tft_processor_final.fit(train_selected)\n",
        "processed_data_final = tft_processor_final.transform(train_selected)\n",
        "\n",
        "sequences_final = processed_data_final['sequences']\n",
        "targets_final = processed_data_final['targets']\n",
        "static_features_final = processed_data_final['static_features']\n",
        "metadata_final = processed_data_final['metadata']\n",
        "\n",
        "print(f\"Re-generated {len(sequences_final)} sequences for final training\")\n",
        "\n",
        "if len(sequences_final) == 0:\n",
        "    print(\"❌ No sequences generated for final training. Check data processing.\")\n",
        "    wandb.log({\"final_training_failed\": True, \"reason\": \"no_sequences\"})\n",
        "else:\n",
        "    # Convert to consistent numpy arrays (same as CV)\n",
        "    max_time_features = max([seq.shape[1] if len(seq.shape) > 1 else 1 for seq in sequences_final])\n",
        "    max_static_features = max([sf.shape[0] if len(sf.shape) > 0 else 1 for sf in static_features_final])\n",
        "    lookback_length = sequences_final[0].shape[0]\n",
        "\n",
        "    padded_sequences_final = []\n",
        "    padded_static_final = []\n",
        "    valid_targets_final = []\n",
        "\n",
        "    for i, (seq, static, tgt) in enumerate(zip(sequences_final, static_features_final, targets_final)):\n",
        "        # Process time-varying sequence\n",
        "        if len(seq.shape) == 1:\n",
        "            seq = seq.reshape(-1, 1)\n",
        "\n",
        "        if seq.shape[1] < max_time_features:\n",
        "            padding = np.zeros((seq.shape[0], max_time_features - seq.shape[1]), dtype=np.float32)\n",
        "            seq = np.column_stack([seq, padding]).astype(np.float32)\n",
        "        else:\n",
        "            seq = seq.astype(np.float32)\n",
        "\n",
        "        # Process static features\n",
        "        if len(static.shape) == 0:\n",
        "            static = static.reshape(1)\n",
        "\n",
        "        if static.shape[0] < max_static_features:\n",
        "            padding = np.zeros(max_static_features - static.shape[0], dtype=np.float32)\n",
        "            static = np.concatenate([static, padding]).astype(np.float32)\n",
        "        else:\n",
        "            static = static.astype(np.float32)\n",
        "\n",
        "        padded_sequences_final.append(seq)\n",
        "        padded_static_final.append(static)\n",
        "        valid_targets_final.append(tgt.astype(np.float32))\n",
        "\n",
        "    sequences_final_np = np.array(padded_sequences_final, dtype=np.float32)\n",
        "    static_final_np = np.array(padded_static_final, dtype=np.float32)\n",
        "    targets_final_np = np.array(valid_targets_final, dtype=np.float32)\n",
        "\n",
        "    print(f\"Final training data shape: {sequences_final_np.shape}\")\n",
        "    print(f\"Final static features shape: {static_final_np.shape}\")\n",
        "    print(f\"Final training targets shape: {targets_final_np.shape}\")\n",
        "\n",
        "    # Create dataset with CPU numpy arrays\n",
        "    final_dataset = WalmartTFTDataset(sequences_final_np, targets_final_np, static_final_np)\n",
        "    final_loader = DataLoader(final_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # OPTIMIZATION: Use the simplified model from CV instead of complex TFT\n",
        "    # This will train much faster while still providing good results\n",
        "    final_model = SimplifiedTFT(**model_config).to(device)\n",
        "    final_optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
        "    final_criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"Training on {len(sequences_final_np)} sequences...\")\n",
        "    print(f\"Using simplified TFT model with {sum(p.numel() for p in final_model.parameters()):,} parameters\")\n",
        "\n",
        "    # OPTIMIZATION: Slightly reduced epochs for first run\n",
        "    num_epochs_final = 15  # Reduced from 20 to 15 for faster completion\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs_final):\n",
        "        final_model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (sequences_batch, static_batch, targets_batch) in enumerate(final_loader):\n",
        "            sequences_batch = sequences_batch.to(device)\n",
        "            static_batch = static_batch.to(device)\n",
        "            targets_batch = targets_batch.to(device)\n",
        "\n",
        "            final_optimizer.zero_grad()\n",
        "            outputs = final_model(sequences_batch, static_batch)\n",
        "            loss = final_criterion(outputs, targets_batch)\n",
        "            loss.backward()\n",
        "            final_optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(final_loader)\n",
        "\n",
        "        if avg_epoch_loss < best_loss:\n",
        "            best_loss = avg_epoch_loss\n",
        "\n",
        "        if (epoch + 1) % 3 == 0:  # More frequent logging\n",
        "            print(f'Final Training Epoch {epoch+1}/{num_epochs_final}, Loss: {avg_epoch_loss:.4f}')\n",
        "\n",
        "            # Log training progress\n",
        "            wandb.log({\n",
        "                \"final_epoch\": epoch + 1,\n",
        "                \"final_train_loss\": avg_epoch_loss,\n",
        "                \"best_loss\": best_loss\n",
        "            })\n",
        "\n",
        "    # Final evaluation on training data\n",
        "    print(\"\\nEvaluating final model...\")\n",
        "    final_model.eval()\n",
        "    all_final_predictions = []\n",
        "    all_final_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences_batch, static_batch, targets_batch in final_loader:\n",
        "            sequences_batch = sequences_batch.to(device)\n",
        "            static_batch = static_batch.to(device)\n",
        "            outputs = final_model(sequences_batch, static_batch)\n",
        "            all_final_predictions.extend(outputs.cpu().numpy())\n",
        "            all_final_targets.extend(targets_batch.numpy())\n",
        "\n",
        "    all_final_predictions = np.array(all_final_predictions).flatten()\n",
        "    all_final_targets = np.array(all_final_targets).flatten()\n",
        "\n",
        "    # Calculate final metrics\n",
        "    final_mae = mean_absolute_error(all_final_targets, all_final_predictions)\n",
        "    final_rmse = np.sqrt(mean_squared_error(all_final_targets, all_final_predictions))\n",
        "    final_r2 = r2_score(all_final_targets, all_final_predictions)\n",
        "\n",
        "    # Safe MAPE calculation\n",
        "    def safe_mape(y_true, y_pred):\n",
        "        mask = y_true != 0\n",
        "        if mask.sum() == 0:\n",
        "            return float('inf')\n",
        "        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "    final_mape = safe_mape(all_final_targets, all_final_predictions)\n",
        "\n",
        "    print(f\"\\nFinal Training Metrics:\")\n",
        "    print(f\"MAE: {final_mae:.2f}\")\n",
        "    print(f\"RMSE: {final_rmse:.2f}\")\n",
        "    print(f\"MAPE: {final_mape:.2f}%\")\n",
        "    print(f\"R²: {final_r2:.4f}\")\n",
        "\n",
        "    # Create complete pipeline\n",
        "    class TFTPipeline:\n",
        "        \"\"\"Complete pipeline for TFT inference\"\"\"\n",
        "\n",
        "        def __init__(self, feature_merger, missing_handler, tft_processor, model):\n",
        "            self.feature_merger = feature_merger\n",
        "            self.missing_handler = missing_handler\n",
        "            self.tft_processor = tft_processor\n",
        "            self.model = model\n",
        "            self.model.eval()\n",
        "\n",
        "        def predict(self, X_raw, stores_df=None, features_df=None):\n",
        "            \"\"\"Make predictions on raw test data\"\"\"\n",
        "            # If auxiliary data provided, update the merger\n",
        "            if stores_df is not None or features_df is not None:\n",
        "                self.feature_merger.fit(X_raw, stores_df=stores_df, features_df=features_df)\n",
        "\n",
        "            # Process through pipeline\n",
        "            merged_data = self.feature_merger.transform(X_raw)\n",
        "            cleaned_data = self.missing_handler.transform(merged_data)\n",
        "\n",
        "            # Process for TFT\n",
        "            processed = self.tft_processor.transform(cleaned_data)\n",
        "\n",
        "            if len(processed['sequences']) == 0:\n",
        "                return np.array([])\n",
        "\n",
        "            # Convert to tensors and predict\n",
        "            sequences_tensor = torch.FloatTensor(processed['sequences']).to(device)\n",
        "            static_tensor = torch.FloatTensor(processed['static_features']).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predictions = self.model(sequences_tensor, static_tensor)\n",
        "\n",
        "            return predictions.cpu().numpy().flatten()\n",
        "\n",
        "    # Create final pipeline\n",
        "    final_pipeline = TFTPipeline(\n",
        "        feature_merger=feature_merger,\n",
        "        missing_handler=missing_handler,\n",
        "        tft_processor=tft_processor_final,\n",
        "        model=final_model\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== SAVING FINAL MODEL ===\")\n",
        "\n",
        "    # Save pipeline with cloudpickle\n",
        "    try:\n",
        "        import cloudpickle\n",
        "    except ImportError:\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', 'cloudpickle'])\n",
        "        import cloudpickle\n",
        "\n",
        "    # Create filename\n",
        "    pipeline_filename = f\"tft_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
        "\n",
        "    # Save with cloudpickle\n",
        "    with open(pipeline_filename, 'wb') as f:\n",
        "        cloudpickle.dump(final_pipeline, f)\n",
        "\n",
        "    print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "\n",
        "    # Try to upload to wandb with error handling\n",
        "    try:\n",
        "        # Create model artifact\n",
        "        model_artifact = wandb.Artifact(\n",
        "            name=\"TFT_pipeline\",\n",
        "            type=\"model\",\n",
        "            description=\"Final TFT pipeline for Walmart sales forecasting\",\n",
        "            metadata={\n",
        "                \"train_mae\": float(final_mae),\n",
        "                \"train_rmse\": float(final_rmse),\n",
        "                \"train_mape\": float(final_mape) if not np.isinf(final_mape) else 0.0,\n",
        "                \"train_r2\": float(final_r2),\n",
        "                \"sequences_count\": len(sequences_final_np),\n",
        "                \"training_samples\": len(all_final_targets),\n",
        "                \"model_type\": \"TFT_Simplified\",\n",
        "                \"lookback_window\": 52,\n",
        "                \"forecast_horizon\": 1,\n",
        "                \"hidden_dim\": model_config[\"hidden_dim\"],\n",
        "                \"num_attention_heads\": model_config[\"num_attention_heads\"],\n",
        "                \"optimization\": \"simplified_for_speed\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Add model file to artifact\n",
        "        model_artifact.add_file(pipeline_filename)\n",
        "\n",
        "        # Log artifact\n",
        "        wandb.log_artifact(model_artifact)\n",
        "        print(\"✓ Model artifact logged to wandb successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error uploading to wandb: {e}\")\n",
        "        print(\"Model saved locally - you can manually upload later\")\n",
        "\n",
        "        # Log just the metrics without artifact\n",
        "        wandb.log({\n",
        "            'final_train_mae': final_mae,\n",
        "            'final_train_rmse': final_rmse,\n",
        "            'final_train_mape': final_mape if not np.isinf(final_mape) else 0.0,\n",
        "            'final_train_r2': final_r2,\n",
        "            'model_saved_locally': pipeline_filename\n",
        "        })\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL MODEL SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Model Type: Temporal Fusion Transformer (Simplified)\")\n",
        "    print(f\"Training Sequences: {len(all_final_targets):,}\")\n",
        "    print(f\"Lookback Window: 52 weeks\")\n",
        "    print(f\"Forecast Horizon: 1 week\")\n",
        "    print(f\"Time-varying Features: {max_time_features}\")\n",
        "    print(f\"Static Features: {max_static_features}\")\n",
        "    print(f\"Training MAE: {final_mae:.2f}\")\n",
        "    print(f\"Training RMSE: {final_rmse:.2f}\")\n",
        "    print(f\"Training MAPE: {final_mape:.2f}%\")\n",
        "    print(f\"Training R²: {final_r2:.4f}\")\n",
        "    print(f\"Pipeline saved as: {pipeline_filename}\")\n",
        "    print(f\"Optimizations: Efficient processor, simplified model, 15 epochs\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "wandb.finish()\n",
        "print(\"\\n✓ Final training completed and model saved!\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">TFT_Final_Training</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/97wxobt5' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/97wxobt5</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_182843-97wxobt5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250706_183137-62nlkc8d</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/62nlkc8d' target=\"_blank\">TFT_Final_Training</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/62nlkc8d' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/62nlkc8d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL TRAINING ===\n",
            "Re-processing data for final training (using efficient processor)...\n",
            "Found 3331 store-dept combinations\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Generated 261083 valid sequences from 3331 store-dept combinations\n",
            "Re-generated 261083 sequences for final training\n",
            "Final training data shape: (261083, 52, 5)\n",
            "Final static features shape: (261083, 4)\n",
            "Final training targets shape: (261083, 1)\n",
            "Training on 261083 sequences...\n",
            "Using simplified TFT model with 199,937 parameters\n",
            "Final Training Epoch 3/15, Loss: 518992803.8534\n",
            "Final Training Epoch 6/15, Loss: 519063895.9029\n",
            "Final Training Epoch 9/15, Loss: 518917823.9157\n",
            "Final Training Epoch 12/15, Loss: 508648936.8830\n",
            "Final Training Epoch 15/15, Loss: 518921973.2497\n",
            "\n",
            "Evaluating final model...\n",
            "\n",
            "Final Training Metrics:\n",
            "MAE: 15366.54\n",
            "RMSE: 22776.36\n",
            "MAPE: 15608.71%\n",
            "R²: 0.0000\n",
            "\n",
            "=== SAVING FINAL MODEL ===\n",
            "Pipeline saved as: tft_pipeline_20250706_185107.pkl\n",
            "✓ Model artifact logged to wandb successfully!\n",
            "\n",
            "============================================================\n",
            "FINAL MODEL SUMMARY\n",
            "============================================================\n",
            "Model Type: Temporal Fusion Transformer (Simplified)\n",
            "Training Sequences: 261,083\n",
            "Lookback Window: 52 weeks\n",
            "Forecast Horizon: 1 week\n",
            "Time-varying Features: 5\n",
            "Static Features: 4\n",
            "Training MAE: 15366.54\n",
            "Training RMSE: 22776.36\n",
            "Training MAPE: 15608.71%\n",
            "Training R²: 0.0000\n",
            "Pipeline saved as: tft_pipeline_20250706_185107.pkl\n",
            "Optimizations: Efficient processor, simplified model, 15 epochs\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>▁▁▁▁▁</td></tr><tr><td>final_epoch</td><td>▁▃▅▆█</td></tr><tr><td>final_train_loss</td><td>███▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_loss</td><td>319042889.71384</td></tr><tr><td>final_epoch</td><td>15</td></tr><tr><td>final_train_loss</td><td>518921973.24966</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">TFT_Final_Training</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/62nlkc8d' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/62nlkc8d</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250706_183137-62nlkc8d/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Final training completed and model saved!\n"
          ]
        }
      ],
      "execution_count": 12,
      "id": "final_training_code"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ugg0r4Oc_2I6"
      },
      "id": "ugg0r4Oc_2I6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}