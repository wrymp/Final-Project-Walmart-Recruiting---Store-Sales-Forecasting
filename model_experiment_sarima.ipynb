{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle wandb onnx pmdarima -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrPcx2vcShpm",
        "outputId": "0f7f95f8-fd4e-460f-aff4-e2f29c3bd2c8"
      },
      "id": "lrPcx2vcShpm",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Kaggle_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKBWkcfiSjzy",
        "outputId": "1c025d28-ba43-4705-e451-dea49df82db4"
      },
      "id": "YKBWkcfiSjzy",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb -qU\n",
        "# !pip uninstall -y pmdarima numpy scipy statsmodels\n",
        "# !pip install numpy==1.24.4 scipy==1.10.1 statsmodels==0.13.5 pmdarima==2.0.3"
      ],
      "metadata": {
        "id": "c9q1VEtoTZrZ"
      },
      "id": "c9q1VEtoTZrZ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "# ! unzip /content/walmart-recruiting-store-sales-forecasting.zip\n",
        "# ! unzip /content/train.csv.zip\n",
        "# ! unzip /content/test.csv.zip\n",
        "# ! unzip /content/features.csv.zip\n",
        "# ! unzip /content/sampleSubmission.csv.zip"
      ],
      "metadata": {
        "id": "ZXV9O_3ZSlKT"
      },
      "id": "ZXV9O_3ZSlKT",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import dill\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import pmdarima as pm\n",
        "from pmdarima import auto_arima\n",
        "from scipy import stats\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings and logging\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "# WandB setup\n",
        "wandb.init(project=\"walmart-sales-forecasting\", name=\"ARIMA_TimeSeries_Optimized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nu4V0DdtSlkz",
        "outputId": "158ba071-0669-4ef2-bcbd-12e0768e2596"
      },
      "id": "nu4V0DdtSlkz",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cluster_models</td><td>▁</td></tr><tr><td>date_range_days</td><td>▁</td></tr><tr><td>feature_engineering_time</td><td>▁</td></tr><tr><td>individual_models</td><td>▁</td></tr><tr><td>models_trained</td><td>▁</td></tr><tr><td>n_departments</td><td>▁</td></tr><tr><td>n_stores</td><td>▁</td></tr><tr><td>negative_predictions_cleaned</td><td>▁</td></tr><tr><td>predictions_per_second</td><td>▁</td></tr><tr><td>submission_creation_time</td><td>▁</td></tr><tr><td>submission_file_size_mb</td><td>▁</td></tr><tr><td>test_prediction_time</td><td>▁</td></tr><tr><td>test_predictions_count</td><td>▁</td></tr><tr><td>test_predictions_max</td><td>▁</td></tr><tr><td>test_predictions_mean</td><td>▁</td></tr><tr><td>test_predictions_median</td><td>▁</td></tr><tr><td>test_predictions_min</td><td>▁</td></tr><tr><td>test_predictions_std</td><td>▁</td></tr><tr><td>test_samples</td><td>▁</td></tr><tr><td>total_block5_time</td><td>▁</td></tr><tr><td>total_time</td><td>▁</td></tr><tr><td>train_samples</td><td>▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_samples</td><td>▁</td></tr><tr><td>validation_mae</td><td>▁</td></tr><tr><td>validation_rmse</td><td>▁</td></tr><tr><td>validation_time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cluster_models</td><td>50</td></tr><tr><td>date_range_days</td><td>994</td></tr><tr><td>feature_engineering_time</td><td>2.67433</td></tr><tr><td>individual_models</td><td>0</td></tr><tr><td>models_trained</td><td>50</td></tr><tr><td>n_departments</td><td>81</td></tr><tr><td>n_stores</td><td>45</td></tr><tr><td>negative_predictions_cleaned</td><td>2</td></tr><tr><td>predictions_per_second</td><td>2381.47355</td></tr><tr><td>submission_creation_time</td><td>0.00488</td></tr><tr><td>submission_file_size_mb</td><td>3.61954</td></tr><tr><td>test_prediction_time</td><td>48.3163</td></tr><tr><td>test_predictions_count</td><td>115064</td></tr><tr><td>test_predictions_max</td><td>50000</td></tr><tr><td>test_predictions_mean</td><td>13555.09478</td></tr><tr><td>test_predictions_median</td><td>4182.33987</td></tr><tr><td>test_predictions_min</td><td>100</td></tr><tr><td>test_predictions_std</td><td>16211.31917</td></tr><tr><td>test_samples</td><td>115064</td></tr><tr><td>total_block5_time</td><td>48.32119</td></tr><tr><td>total_time</td><td>81.46288</td></tr><tr><td>train_samples</td><td>421570</td></tr><tr><td>training_time</td><td>59.5918</td></tr><tr><td>val_samples</td><td>23729</td></tr><tr><td>validation_mae</td><td>6634.61898</td></tr><tr><td>validation_mape</td><td>inf</td></tr><tr><td>validation_rmse</td><td>11439.1834</td></tr><tr><td>validation_time</td><td>19.19675</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ARIMA_TimeSeries_Optimized</strong> at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/bxyavy3c' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/bxyavy3c</a><br> View project at: <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250731_212128-bxyavy3c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250731_213422-n0fxmhz2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/n0fxmhz2' target=\"_blank\">ARIMA_TimeSeries_Optimized</a></strong> to <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/n0fxmhz2' target=\"_blank\">https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/n0fxmhz2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dshan21-free-university-of-tbilisi-/walmart-sales-forecasting/runs/n0fxmhz2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78aff45d8bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 1: Data Loading and Initial Setup\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "features_df = pd.read_csv(\"/content/features.csv\")\n",
        "stores_df = pd.read_csv(\"/content/stores.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")\n",
        "\n",
        "# Convert dates\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "print(f\"Data loaded: Train {train_df.shape}, Test {test_df.shape}\")\n",
        "print(f\"Train columns: {list(train_df.columns)}\")\n",
        "print(f\"Features columns: {list(features_df.columns)}\")\n",
        "print(f\"Date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "\n",
        "# Log basic info\n",
        "wandb.log({\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"n_stores\": train_df['Store'].nunique(),\n",
        "    \"n_departments\": train_df['Dept'].nunique(),\n",
        "    \"date_range_days\": (train_df['Date'].max() - train_df['Date'].min()).days\n",
        "})\n",
        "\n",
        "# Comprehensive logging suppression\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkvK4IYaSnxq",
        "outputId": "f0561312-7abb-44b6-fc0f-7b5922f849af"
      },
      "id": "LkvK4IYaSnxq",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Data loaded: Train (421570, 5), Test (115064, 4)\n",
            "Train columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']\n",
            "Features columns: ['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday']\n",
            "Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 2: Time Series Feature Engineering for ARIMA\n",
        "# =============================================================================\n",
        "\n",
        "class ARIMATimeSeriesFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Enhanced time series feature engineering optimized for ARIMA models\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "        print(f\"Input shape: {df.shape}\")\n",
        "        print(f\"Input columns: {list(df.columns)}\")\n",
        "\n",
        "        # Merge external features\n",
        "        print(\"Merging with features...\")\n",
        "        df = df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
        "        print(f\"After features merge: {df.shape}\")\n",
        "\n",
        "        print(\"Merging with stores...\")\n",
        "        df = df.merge(stores_df, on='Store', how='left')\n",
        "        print(f\"After stores merge: {df.shape}\")\n",
        "\n",
        "        # Handle IsHoliday column conflicts\n",
        "        if 'IsHoliday_feat' in df.columns:\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(df['IsHoliday_feat'])\n",
        "            df = df.drop('IsHoliday_feat', axis=1)\n",
        "\n",
        "        # Ensure IsHoliday exists and is properly formatted\n",
        "        if 'IsHoliday' in df.columns:\n",
        "            df['IsHoliday'] = df['IsHoliday'].fillna(False).astype(int)\n",
        "        else:\n",
        "            print(\"Warning: IsHoliday column not found, creating default\")\n",
        "            df['IsHoliday'] = 0\n",
        "\n",
        "        # Fill missing values efficiently\n",
        "        numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "                print(f\"Filled {col}: {df[col].isnull().sum()} missing values\")\n",
        "            else:\n",
        "                print(f\"Warning: {col} not found in data\")\n",
        "\n",
        "        # Markdown columns (promotional effects)\n",
        "        markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        for col in markdown_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna(0)\n",
        "            else:\n",
        "                print(f\"Creating {col} with default values\")\n",
        "                df[col] = 0\n",
        "\n",
        "        # Store type and size handling\n",
        "        if 'Type' in df.columns:\n",
        "            df['Type'] = df['Type'].fillna('A')\n",
        "        else:\n",
        "            df['Type'] = 'A'\n",
        "\n",
        "        if 'Size' in df.columns:\n",
        "            df['Size'] = df['Size'].fillna(df['Size'].median())\n",
        "        else:\n",
        "            df['Size'] = 151315\n",
        "\n",
        "        # Enhanced time-based features for ARIMA\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['DayOfMonth'] = df['Date'].dt.day\n",
        "        df['WeekOfMonth'] = (df['Date'].dt.day - 1) // 7 + 1\n",
        "\n",
        "        # Cyclical encoding for better ARIMA performance\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "\n",
        "        # Enhanced seasonal indicators (crucial for retail ARIMA)\n",
        "        df['IsQ4'] = (df['Quarter'] == 4).astype(int)\n",
        "        df['IsHolidaySeason'] = df['Month'].isin([11, 12]).astype(int)\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8, 9]).astype(int)\n",
        "        df['IsSpring'] = df['Month'].isin([3, 4, 5]).astype(int)\n",
        "        df['IsSummer'] = df['Month'].isin([6, 7, 8]).astype(int)\n",
        "        df['IsFall'] = df['Month'].isin([9, 10, 11]).astype(int)\n",
        "        df['IsWinter'] = df['Month'].isin([12, 1, 2]).astype(int)\n",
        "\n",
        "        # Weekend effect (important for retail)\n",
        "        df['IsWeekend'] = (df['DayOfWeek'].isin([5, 6])).astype(int)\n",
        "        df['IsMonday'] = (df['DayOfWeek'] == 0).astype(int)\n",
        "        df['IsFriday'] = (df['DayOfWeek'] == 4).astype(int)\n",
        "\n",
        "        # Holiday-specific features for ARIMA\n",
        "        df['IsNewYear'] = ((df['Month'] == 1) & (df['DayOfMonth'] == 1)).astype(int)\n",
        "        df['IsChristmas'] = ((df['Month'] == 12) & (df['DayOfMonth'] == 25)).astype(int)\n",
        "        df['IsThanksgiving'] = ((df['Month'] == 11) & (df['WeekOfMonth'] == 4) & (df['DayOfWeek'] == 3)).astype(int)\n",
        "\n",
        "        # Enhanced promotional features\n",
        "        df['TotalMarkDown'] = sum(df[col] for col in markdown_cols if col in df.columns)\n",
        "        df['HasPromotion'] = (df['TotalMarkDown'] > 0).astype(int)\n",
        "        df['PromotionIntensity'] = np.log1p(df['TotalMarkDown'])  # Log transformation for better ARIMA handling\n",
        "\n",
        "        # Individual markdown effects\n",
        "        for i, col in enumerate(markdown_cols, 1):\n",
        "            if col in df.columns:\n",
        "                df[f'HasMarkDown{i}'] = (df[col] > 0).astype(int)\n",
        "                df[f'MarkDown{i}_log'] = np.log1p(df[col])\n",
        "\n",
        "        # Economic indicators (lagged effects important for ARIMA)\n",
        "        if 'CPI' in df.columns and 'Unemployment' in df.columns:\n",
        "            df['EconomicIndex'] = df['CPI'] / df['Unemployment']  # Modified for better scaling\n",
        "            df['CPI_normalized'] = (df['CPI'] - df['CPI'].mean()) / df['CPI'].std()\n",
        "            df['Unemployment_normalized'] = (df['Unemployment'] - df['Unemployment'].mean()) / df['Unemployment'].std()\n",
        "        else:\n",
        "            df['EconomicIndex'] = 1\n",
        "            df['CPI_normalized'] = 0\n",
        "            df['Unemployment_normalized'] = 0\n",
        "\n",
        "        # Weather effects (important for retail sales)\n",
        "        if 'Temperature' in df.columns:\n",
        "            df['Temperature_normalized'] = (df['Temperature'] - df['Temperature'].mean()) / df['Temperature'].std()\n",
        "            df['IsExtremeCold'] = (df['Temperature'] < 32).astype(int)  # Below freezing\n",
        "            df['IsExtremeHot'] = (df['Temperature'] > 85).astype(int)   # Very hot\n",
        "        else:\n",
        "            df['Temperature_normalized'] = 0\n",
        "            df['IsExtremeCold'] = 0\n",
        "            df['IsExtremeHot'] = 0\n",
        "\n",
        "        # Fuel price effects\n",
        "        if 'Fuel_Price' in df.columns:\n",
        "            df['Fuel_Price_normalized'] = (df['Fuel_Price'] - df['Fuel_Price'].mean()) / df['Fuel_Price'].std()\n",
        "            df['IsHighFuelPrice'] = (df['Fuel_Price'] > df['Fuel_Price'].quantile(0.75)).astype(int)\n",
        "        else:\n",
        "            df['Fuel_Price_normalized'] = 0\n",
        "            df['IsHighFuelPrice'] = 0\n",
        "\n",
        "        # Store characteristics for ARIMA regressors\n",
        "        if 'Size' in df.columns:\n",
        "            df['StoreSizeCategory'] = pd.cut(df['Size'], bins=3, labels=[0, 1, 2]).astype(int)\n",
        "            df['Size_normalized'] = (df['Size'] - df['Size'].mean()) / df['Size'].std()\n",
        "        else:\n",
        "            df['StoreSizeCategory'] = 1\n",
        "            df['Size_normalized'] = 0\n",
        "\n",
        "        # Store type dummies for ARIMA\n",
        "        if 'Type' in df.columns:\n",
        "            df['Type_A'] = (df['Type'] == 'A').astype(int)\n",
        "            df['Type_B'] = (df['Type'] == 'B').astype(int)\n",
        "            df['Type_C'] = (df['Type'] == 'C').astype(int)\n",
        "        else:\n",
        "            df['Type_A'] = 1\n",
        "            df['Type_B'] = 0\n",
        "            df['Type_C'] = 0\n",
        "\n",
        "        # Interaction effects (important for capturing complex patterns)\n",
        "        df['Holiday_Promotion'] = df['IsHoliday'] * df['HasPromotion']\n",
        "        df['Weekend_Holiday'] = df['IsWeekend'] * df['IsHoliday']\n",
        "        df['Q4_Promotion'] = df['IsQ4'] * df['HasPromotion']\n",
        "        df['Temperature_Holiday'] = df['Temperature_normalized'] * df['IsHoliday']\n",
        "\n",
        "        print(f\"Final processed shape: {df.shape}\")\n",
        "        print(\"Sample of engineered features:\")\n",
        "        feature_sample = ['IsHoliday', 'TotalMarkDown', 'Month_sin', 'IsQ4', 'Holiday_Promotion']\n",
        "        for col in feature_sample:\n",
        "            if col in df.columns:\n",
        "                print(f\"  {col}: mean={df[col].mean():.3f}, std={df[col].std():.3f}\")\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "CNBUPHM0SqGx"
      },
      "id": "CNBUPHM0SqGx",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 3: SARIMAX Model for Time Series\n",
        "# =============================================================================\n",
        "\n",
        "class SuppressOutput:\n",
        "    \"\"\"Enhanced context manager to suppress all output\"\"\"\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        self._original_stderr = sys.stderr\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "        sys.stderr = open(os.devnull, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stderr.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "        sys.stderr = self._original_stderr\n",
        "\n",
        "class WalmartARIMAModel(BaseEstimator):\n",
        "    \"\"\"Optimized SARIMAX model for Walmart sales forecasting with comprehensive time series handling\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 seasonal_order=(1, 1, 1, 52),  # Weekly seasonality\n",
        "                 max_p=3, max_q=3, max_P=2, max_Q=2,\n",
        "                 stepwise=True,\n",
        "                 suppress_warnings=True,\n",
        "                 min_samples=30,\n",
        "                 use_auto_arima=True):\n",
        "        self.seasonal_order = seasonal_order\n",
        "        self.max_p = max_p\n",
        "        self.max_q = max_q\n",
        "        self.max_P = max_P\n",
        "        self.max_Q = max_Q\n",
        "        self.stepwise = stepwise\n",
        "        self.suppress_warnings = suppress_warnings\n",
        "        self.min_samples = min_samples\n",
        "        self.use_auto_arima = use_auto_arima\n",
        "        self.models = {}\n",
        "        self.model_orders = {}\n",
        "        self.global_median = None\n",
        "\n",
        "    def _get_exogenous_features(self, df):\n",
        "        \"\"\"Get the list of exogenous variables for ARIMA\"\"\"\n",
        "        base_features = [\n",
        "            'Temperature_normalized', 'Fuel_Price_normalized', 'IsHoliday',\n",
        "            'Month_sin', 'Month_cos', 'IsQ4', 'IsHolidaySeason',\n",
        "            'IsBackToSchool', 'TotalMarkDown', 'HasPromotion',\n",
        "            'CPI_normalized', 'Unemployment_normalized', 'IsWeekend',\n",
        "            'Holiday_Promotion', 'Q4_Promotion', 'Type_A', 'Type_B',\n",
        "            'StoreSizeCategory', 'IsExtremeCold', 'IsExtremeHot'\n",
        "        ]\n",
        "\n",
        "        # Filter to only existing columns\n",
        "        available_features = [col for col in base_features if col in df.columns]\n",
        "        print(f\"Using {len(available_features)} exogenous features: {available_features[:5]}...\")\n",
        "        return available_features\n",
        "\n",
        "    def _prepare_time_series_data(self, group, exog_features):\n",
        "        \"\"\"Prepare data for ARIMA modeling\"\"\"\n",
        "        # Sort by date to ensure proper time series order\n",
        "        group_sorted = group.sort_values('Date').copy()\n",
        "\n",
        "        # Create time series\n",
        "        y = group_sorted['Weekly_Sales'] if 'Weekly_Sales' in group_sorted.columns else group_sorted.iloc[:, -1]\n",
        "\n",
        "        # Handle negative sales (log transformation issues)\n",
        "        y = np.maximum(y, 1)  # Ensure positive values\n",
        "\n",
        "        # Prepare exogenous variables\n",
        "        if exog_features:\n",
        "            X = group_sorted[exog_features].fillna(0)\n",
        "            # Ensure no infinite values\n",
        "            X = X.replace([np.inf, -np.inf], 0)\n",
        "        else:\n",
        "            X = None\n",
        "\n",
        "        return y, X, group_sorted['Date']\n",
        "\n",
        "    def _detect_and_handle_outliers(self, y):\n",
        "        \"\"\"Detect and handle outliers in the time series\"\"\"\n",
        "        Q1 = y.quantile(0.25)\n",
        "        Q3 = y.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 3 * IQR  # More conservative than 1.5\n",
        "        upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "        # Cap outliers instead of removing (to preserve time series structure)\n",
        "        y_cleaned = y.copy()\n",
        "        y_cleaned = np.where(y_cleaned < lower_bound, lower_bound, y_cleaned)\n",
        "        y_cleaned = np.where(y_cleaned > upper_bound, upper_bound, y_cleaned)\n",
        "\n",
        "        outlier_count = np.sum((y < lower_bound) | (y > upper_bound))\n",
        "        return pd.Series(y_cleaned, index=y.index), outlier_count\n",
        "\n",
        "    def _fit_individual_arima(self, y, X, store, dept):\n",
        "        \"\"\"Fit ARIMA model for individual store-department combination\"\"\"\n",
        "        try:\n",
        "            # Clean outliers\n",
        "            y_clean, outlier_count = self._detect_and_handle_outliers(y)\n",
        "\n",
        "            if self.use_auto_arima:\n",
        "                # Use auto_arima for automatic model selection\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "                    model = pm.auto_arima(\n",
        "                        y_clean,\n",
        "                        exogenous=X,\n",
        "                        seasonal=True,\n",
        "                        m=52,  # Weekly seasonality (52 weeks in a year)\n",
        "                        max_p=self.max_p,\n",
        "                        max_q=self.max_q,\n",
        "                        max_P=self.max_P,\n",
        "                        max_Q=self.max_Q,\n",
        "                        stepwise=self.stepwise,\n",
        "                        suppress_warnings=self.suppress_warnings,\n",
        "                        error_action='ignore',\n",
        "                        trace=False,\n",
        "                        approximation=False,\n",
        "                        maxiter=50\n",
        "                    )\n",
        "\n",
        "                # Store the order for future reference\n",
        "                self.model_orders[(store, dept)] = {\n",
        "                    'order': model.order,\n",
        "                    'seasonal_order': model.seasonal_order,\n",
        "                    'outliers_handled': outlier_count\n",
        "                }\n",
        "\n",
        "            else:\n",
        "                # Use fixed SARIMAX model\n",
        "                model = SARIMAX(\n",
        "                    y_clean,\n",
        "                    exog=X,\n",
        "                    order=(1, 1, 1),  # Default ARIMA order\n",
        "                    seasonal_order=self.seasonal_order,\n",
        "                    enforce_stationarity=False,\n",
        "                    enforce_invertibility=False\n",
        "                )\n",
        "                model = model.fit(disp=False, maxiter=100)\n",
        "\n",
        "                self.model_orders[(store, dept)] = {\n",
        "                    'order': (1, 1, 1),\n",
        "                    'seasonal_order': self.seasonal_order,\n",
        "                    'outliers_handled': outlier_count\n",
        "                }\n",
        "\n",
        "            return model, y_clean.median()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting ARIMA for Store {store}, Dept {dept}: {str(e)}\")\n",
        "            return None, y.median()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit ARIMA models for each store-department combination\"\"\"\n",
        "        print(f\"Training ARIMA models...\")\n",
        "\n",
        "        # Calculate global median for fallback\n",
        "        if 'Weekly_Sales' in X.columns:\n",
        "            self.global_median = X['Weekly_Sales'].median()\n",
        "        else:\n",
        "            self.global_median = 15000  # Reasonable default\n",
        "\n",
        "        # Get exogenous features\n",
        "        exog_features = self._get_exogenous_features(X)\n",
        "\n",
        "        trained_count = 0\n",
        "        total_groups = len(X.groupby(['Store', 'Dept']))\n",
        "        failed_count = 0\n",
        "\n",
        "        print(f\"Training on {total_groups} store-department combinations...\")\n",
        "\n",
        "        for i, ((store, dept), group) in enumerate(X.groupby(['Store', 'Dept'])):\n",
        "            if i % 25 == 0:\n",
        "                print(f\"Progress: {i}/{total_groups} ({100*i/total_groups:.1f}%) - Trained: {trained_count}, Failed: {failed_count}\")\n",
        "\n",
        "            # Skip if insufficient data\n",
        "            if len(group) < self.min_samples:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Prepare time series data\n",
        "                y_series, X_exog, dates = self._prepare_time_series_data(group, exog_features)\n",
        "\n",
        "                if len(y_series) < self.min_samples:\n",
        "                    continue\n",
        "\n",
        "                # Fit individual ARIMA model\n",
        "                model, median_sales = self._fit_individual_arima(y_series, X_exog, store, dept)\n",
        "\n",
        "                if model is not None:\n",
        "                    self.models[(store, dept)] = {\n",
        "                        'model': model,\n",
        "                        'exog_features': exog_features,\n",
        "                        'median_sales': median_sales,\n",
        "                        'last_date': dates.max(),\n",
        "                        'n_obs': len(y_series)\n",
        "                    }\n",
        "                    trained_count += 1\n",
        "                else:\n",
        "                    failed_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_count += 1\n",
        "                if i < 3:  # Show first few errors for debugging\n",
        "                    print(f\"Error training model for Store {store}, Dept {dept}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Training completed:\")\n",
        "        print(f\"  Successfully trained: {trained_count} models\")\n",
        "        print(f\"  Failed: {failed_count} models\")\n",
        "        print(f\"  Total groups: {total_groups}\")\n",
        "\n",
        "        # Log model order statistics\n",
        "        if self.model_orders:\n",
        "            orders = [info['order'] for info in self.model_orders.values()]\n",
        "            print(f\"  Most common ARIMA order: {max(set(orders), key=orders.count)}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Generate predictions using trained ARIMA models\"\"\"\n",
        "        predictions = []\n",
        "        exog_features = self._get_exogenous_features(X)\n",
        "\n",
        "        for (store, dept), group in X.groupby(['Store', 'Dept']):\n",
        "            if (store, dept) in self.models:\n",
        "                model_info = self.models[(store, dept)]\n",
        "                model = model_info['model']\n",
        "\n",
        "                # Prepare exogenous data for prediction\n",
        "                group_sorted = group.sort_values('Date')\n",
        "                if exog_features:\n",
        "                    X_pred = group_sorted[exog_features].fillna(0)\n",
        "                    X_pred = X_pred.replace([np.inf, -np.inf], 0)\n",
        "                else:\n",
        "                    X_pred = None\n",
        "\n",
        "                try:\n",
        "                    # Generate predictions\n",
        "                    n_periods = len(group)\n",
        "                    if self.use_auto_arima:\n",
        "                        forecast = model.predict(n_periods=n_periods, exogenous=X_pred)\n",
        "                    else:\n",
        "                        forecast = model.forecast(steps=n_periods, exog=X_pred)\n",
        "\n",
        "                    # Ensure positive predictions\n",
        "                    forecast = np.maximum(forecast, model_info['median_sales'] * 0.1)\n",
        "                    predictions.extend(forecast)\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Fallback to median with seasonal adjustment\n",
        "                    median_pred = model_info['median_sales']\n",
        "\n",
        "                    # Apply seasonal adjustment based on month\n",
        "                    seasonal_preds = []\n",
        "                    for _, row in group_sorted.iterrows():\n",
        "                        seasonal_multiplier = 1.0\n",
        "                        if 'Month' in row:\n",
        "                            month = row['Month']\n",
        "                            if month in [11, 12]:  # Holiday season\n",
        "                                seasonal_multiplier = 1.4\n",
        "                            elif month in [1, 2]:  # Post holiday\n",
        "                                seasonal_multiplier = 0.8\n",
        "                            elif month in [6, 7, 8]:  # Summer\n",
        "                                seasonal_multiplier = 1.1\n",
        "\n",
        "                        seasonal_preds.append(median_pred * seasonal_multiplier)\n",
        "\n",
        "                    predictions.extend(seasonal_preds)\n",
        "            else:\n",
        "                # Use global median for unseen store-dept combinations with seasonal adjustment\n",
        "                seasonal_predictions = []\n",
        "                for _, row in group.iterrows():\n",
        "                    seasonal_multiplier = 1.0\n",
        "                    if 'Month' in row:\n",
        "                        month = row['Month']\n",
        "                        if month in [11, 12]:  # Holiday season\n",
        "                            seasonal_multiplier = 1.3\n",
        "                        elif month in [1, 2]:  # Post holiday\n",
        "                            seasonal_multiplier = 0.9\n",
        "\n",
        "                    # Also consider store type if available\n",
        "                    if 'Type' in row:\n",
        "                        if row['Type'] == 'A':  # Supercenters typically have higher sales\n",
        "                            seasonal_multiplier *= 1.2\n",
        "                        elif row['Type'] == 'C':  # Neighborhood markets typically lower\n",
        "                            seasonal_multiplier *= 0.8\n",
        "\n",
        "                    pred_value = self.global_median * seasonal_multiplier\n",
        "                    seasonal_predictions.append(pred_value)\n",
        "\n",
        "                predictions.extend(seasonal_predictions)\n",
        "\n",
        "        return np.array(predictions)"
      ],
      "metadata": {
        "id": "EuXkO7xzSs37"
      },
      "id": "EuXkO7xzSs37",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 4: ADVANCED Pipeline Training and Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=== ADVANCED WALMART FORECASTING - TARGETING SUB-4K ===\")\n",
        "\n",
        "# Enhanced feature engineering\n",
        "print(\"Applying advanced feature engineering...\")\n",
        "start_time = time.time()\n",
        "\n",
        "advanced_engineer = ARIMATimeSeriesFeatureEngineer()\n",
        "advanced_engineer.fit(train_df)\n",
        "\n",
        "processed_train = advanced_engineer.transform(train_df)\n",
        "processed_test = advanced_engineer.transform(test_df)\n",
        "\n",
        "feature_time = time.time() - start_time\n",
        "print(f\"Feature engineering completed in {feature_time:.2f} seconds\")\n",
        "print(f\"Final training shape: {processed_train.shape}\")\n",
        "\n",
        "# Advanced validation strategy\n",
        "print(\"Creating advanced validation split...\")\n",
        "\n",
        "# Use last 10 weeks for validation (more representative)\n",
        "max_date = processed_train['Date'].max()\n",
        "val_split_date = max_date - timedelta(weeks=10)\n",
        "\n",
        "train_data = processed_train[processed_train['Date'] <= val_split_date].copy()\n",
        "val_data = processed_train[processed_train['Date'] > val_split_date].copy()\n",
        "\n",
        "print(f\"Advanced split - Train: {len(train_data)}, Val: {len(val_data)}\")\n",
        "\n",
        "# Remove samples with insufficient history for lag features\n",
        "min_date = train_data['Date'].min() + timedelta(weeks=12)  # Need 12 weeks for lag features\n",
        "train_data = train_data[train_data['Date'] >= min_date]\n",
        "\n",
        "print(f\"After lag feature filtering - Train: {len(train_data)}\")\n",
        "\n",
        "# Train ensemble model\n",
        "print(\"Training high-accuracy ensemble model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "ensemble_model = HighAccuracyWalmartEnsemble()\n",
        "ensemble_model.fit(train_data)\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Ensemble training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# Advanced validation\n",
        "print(\"Performing advanced validation...\")\n",
        "val_predictions = ensemble_model.predict(val_data)\n",
        "\n",
        "# Multiple metrics\n",
        "val_mae = mean_absolute_error(val_data['Weekly_Sales'], val_predictions)\n",
        "val_rmse = np.sqrt(np.mean((val_data['Weekly_Sales'] - val_predictions) ** 2))\n",
        "val_mape = np.mean(np.abs((val_data['Weekly_Sales'] - val_predictions) / val_data['Weekly_Sales'])) * 100\n",
        "\n",
        "print(f\"=== VALIDATION RESULTS ===\")\n",
        "print(f\"MAE: {val_mae:.2f}\")\n",
        "print(f\"RMSE: {val_rmse:.2f}\")\n",
        "print(f\"MAPE: {val_mape:.2f}%\")\n",
        "\n",
        "# Analyze by segments for debugging\n",
        "print(\"\\n=== VALIDATION BY SEGMENTS ===\")\n",
        "val_with_pred = val_data.copy()\n",
        "val_with_pred['Predictions'] = val_predictions\n",
        "val_with_pred['Error'] = np.abs(val_with_pred['Weekly_Sales'] - val_predictions)\n",
        "\n",
        "# By store type\n",
        "if 'Type' in val_with_pred.columns:\n",
        "    type_results = val_with_pred.groupby('Type')['Error'].mean()\n",
        "    print(\"MAE by Store Type:\")\n",
        "    for store_type, mae in type_results.items():\n",
        "        print(f\"  Type {store_type}: {mae:.2f}\")\n",
        "\n",
        "# By holiday vs non-holiday\n",
        "holiday_mae = val_with_pred[val_with_pred['IsHoliday'] == 1]['Error'].mean()\n",
        "non_holiday_mae = val_with_pred[val_with_pred['IsHoliday'] == 0]['Error'].mean()\n",
        "print(f\"Holiday MAE: {holiday_mae:.2f}\")\n",
        "print(f\"Non-Holiday MAE: {non_holiday_mae:.2f}\")\n",
        "\n",
        "# By Q4 vs other quarters\n",
        "q4_mae = val_with_pred[val_with_pred['IsQ4'] == 1]['Error'].mean()\n",
        "non_q4_mae = val_with_pred[val_with_pred['IsQ4'] == 0]['Error'].mean()\n",
        "print(f\"Q4 MAE: {q4_mae:.2f}\")\n",
        "print(f\"Non-Q4 MAE: {non_q4_mae:.2f}\")\n",
        "\n",
        "# Show feature importance\n",
        "if ensemble_model.feature_importance is not None:\n",
        "    print(\"\\n=== TOP 15 FEATURE IMPORTANCES ===\")\n",
        "    top_features = ensemble_model.feature_importance.head(15)\n",
        "    for _, row in top_features.iterrows():\n",
        "        print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "# Error analysis for improvement insights\n",
        "print(\"\\n=== ERROR ANALYSIS ===\")\n",
        "error_percentiles = np.percentile(val_with_pred['Error'], [50, 75, 90, 95, 99])\n",
        "print(f\"Error Percentiles - 50th: {error_percentiles[0]:.0f}, 75th: {error_percentiles[1]:.0f}, 90th: {error_percentiles[2]:.0f}, 95th: {error_percentiles[3]:.0f}, 99th: {error_percentiles[4]:.0f}\")\n",
        "\n",
        "# Worst predictions analysis\n",
        "worst_predictions = val_with_pred.nlargest(10, 'Error')[['Store', 'Dept', 'Date', 'Weekly_Sales', 'Predictions', 'Error', 'IsHoliday', 'IsQ4']]\n",
        "print(\"\\nWorst 10 Predictions:\")\n",
        "print(worst_predictions.to_string(index=False))\n",
        "\n",
        "# Log detailed results\n",
        "wandb.log({\n",
        "    'advanced_validation_mae': val_mae,\n",
        "    'advanced_validation_rmse': val_rmse,\n",
        "    'advanced_validation_mape': val_mape,\n",
        "    'holiday_mae': holiday_mae,\n",
        "    'non_holiday_mae': non_holiday_mae,\n",
        "    'q4_mae': q4_mae,\n",
        "    'non_q4_mae': non_q4_mae,\n",
        "    'feature_engineering_time': feature_time,\n",
        "    'ensemble_training_time': training_time,\n",
        "    'total_features': processed_train.shape[1],\n",
        "    'ensemble_models': len(ensemble_model.models),\n",
        "    'error_50th_percentile': error_percentiles[0],\n",
        "    'error_95th_percentile': error_percentiles[3]\n",
        "})\n",
        "\n",
        "print(f\"\\n🎯 TARGET: Sub-4K MAE\")\n",
        "print(f\"🔥 CURRENT: {val_mae:.2f} MAE\")\n",
        "if val_mae < 4000:\n",
        "    print(\"✅ TARGET ACHIEVED!\")\n",
        "else:\n",
        "    print(f\"📈 Need {val_mae - 4000:.0f} point improvement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LhmXRKpeSzbq",
        "outputId": "3a7fea59-577f-4f70-c3b9-a713c9ffd96a"
      },
      "id": "LhmXRKpeSzbq",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ADVANCED WALMART FORECASTING - TARGETING SUB-4K ===\n",
            "Applying advanced feature engineering...\n",
            "Input shape: (421570, 5)\n",
            "Input columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']\n",
            "Merging with features...\n",
            "After features merge: (421570, 15)\n",
            "Merging with stores...\n",
            "After stores merge: (421570, 17)\n",
            "Filled Temperature: 0 missing values\n",
            "Filled Fuel_Price: 0 missing values\n",
            "Filled CPI: 0 missing values\n",
            "Filled Unemployment: 0 missing values\n",
            "Final processed shape: (421570, 69)\n",
            "Sample of engineered features:\n",
            "  IsHoliday: mean=0.070, std=0.256\n",
            "  TotalMarkDown: mean=6684.041, std=14750.942\n",
            "  Month_sin: mean=-0.006, std=0.727\n",
            "  IsQ4: mean=0.218, std=0.413\n",
            "  Holiday_Promotion: mean=0.028, std=0.166\n",
            "Input shape: (115064, 4)\n",
            "Input columns: ['Store', 'Dept', 'Date', 'IsHoliday']\n",
            "Merging with features...\n",
            "After features merge: (115064, 14)\n",
            "Merging with stores...\n",
            "After stores merge: (115064, 16)\n",
            "Filled Temperature: 0 missing values\n",
            "Filled Fuel_Price: 0 missing values\n",
            "Filled CPI: 0 missing values\n",
            "Filled Unemployment: 0 missing values\n",
            "Final processed shape: (115064, 68)\n",
            "Sample of engineered features:\n",
            "  IsHoliday: mean=0.078, std=0.268\n",
            "  TotalMarkDown: mean=19585.100, std=29688.350\n",
            "  Month_sin: mean=0.305, std=0.551\n",
            "  IsQ4: mean=0.233, std=0.423\n",
            "  Holiday_Promotion: mean=0.078, std=0.268\n",
            "Feature engineering completed in 2.51 seconds\n",
            "Final training shape: (421570, 69)\n",
            "Creating advanced validation split...\n",
            "Advanced split - Train: 391919, Val: 29651\n",
            "After lag feature filtering - Train: 356515\n",
            "Training high-accuracy ensemble model...\n",
            "Training high-accuracy ensemble...\n",
            "Training on 64 features, 356515 samples\n",
            "Training lightgbm...\n",
            "lightgbm CV MAE: 15313.33\n",
            "Training xgboost...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "XGBoostError",
          "evalue": "[21:45:55] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x2a6ecc) [0x78afaf4a6ecc]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5d7680) [0x78afaf7d7680]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5eb966) [0x78afaf7eb966]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5e971b) [0x78afaf7e971b]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5ea949) [0x78afaf7ea949]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x57b541) [0x78afaf77b541]\n  [bt] (6) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x78afaf3b93c8]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x78b05c2fde2e]\n  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x78b05c2fa493]\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3496947587.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mensemble_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHighAccuracyWalmartEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mensemble_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2406394749.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingCallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalsLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1223\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[1;32m    627\u001b[0m     way.\"\"\"\n\u001b[0;32m--> 628\u001b[0;31m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_use_qdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"gblinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m                 return QuantileDMatrix(\n\u001b[0m\u001b[1;32m   1138\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[0m\n\u001b[1;32m   1612\u001b[0m                 )\n\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         self._init(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m             \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m         \u001b[0;31m# delay check_call to throw intermediate exception first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mXGBoostError\u001b[0m: [21:45:55] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x2a6ecc) [0x78afaf4a6ecc]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5d7680) [0x78afaf7d7680]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5eb966) [0x78afaf7eb966]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5e971b) [0x78afaf7e971b]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5ea949) [0x78afaf7ea949]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x57b541) [0x78afaf77b541]\n  [bt] (6) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x78afaf3b93c8]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x78b05c2fde2e]\n  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x78b05c2fa493]\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# Block 5: ADVANCED Final Training and Prediction\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BLOCK 5: ADVANCED FINAL TRAINING AND PREDICTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Decision: Retrain on full data vs use validation model\n",
        "RETRAIN_ON_FULL_DATA = True  # Set to True for maximum accuracy\n",
        "\n",
        "if RETRAIN_ON_FULL_DATA:\n",
        "    print(\"🔄 Retraining ensemble on FULL dataset for maximum accuracy...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Prepare full training data with lag features\n",
        "    full_train_data = processed_train.copy()\n",
        "    min_date_full = full_train_data['Date'].min() + timedelta(weeks=12)\n",
        "    full_train_data = full_train_data[full_train_data['Date'] >= min_date_full]\n",
        "\n",
        "    print(f\"Full training data shape: {full_train_data.shape}\")\n",
        "\n",
        "    # Train final ensemble\n",
        "    final_ensemble = HighAccuracyWalmartEnsemble()\n",
        "    final_ensemble.fit(full_train_data)\n",
        "\n",
        "    final_training_time = time.time() - start_time\n",
        "    print(f\"Final ensemble training completed in {final_training_time:.2f} seconds\")\n",
        "\n",
        "else:\n",
        "    print(\"⚡ Using pre-trained validation model for speed...\")\n",
        "    final_ensemble = ensemble_model\n",
        "    final_training_time = 0\n",
        "\n",
        "# Advanced test prediction with post-processing\n",
        "print(\"🔮 Generating advanced test predictions...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate base predictions\n",
        "test_predictions = final_ensemble.predict(processed_test)\n",
        "\n",
        "# Post-processing for better accuracy\n",
        "print(\"📊 Applying post-processing improvements...\")\n",
        "\n",
        "# 1. Handle extreme predictions\n",
        "original_predictions = test_predictions.copy()\n",
        "\n",
        "# Cap based on training data distribution\n",
        "train_stats = processed_train['Weekly_Sales'].describe()\n",
        "lower_bound = max(0, train_stats['25%'] * 0.1)  # Allow some flexibility below Q1\n",
        "upper_bound = train_stats['99%'] * 1.2  # Cap at 120% of 99th percentile\n",
        "\n",
        "extreme_low = np.sum(test_predictions < lower_bound)\n",
        "extreme_high = np.sum(test_predictions > upper_bound)\n",
        "\n",
        "test_predictions = np.clip(test_predictions, lower_bound, upper_bound)\n",
        "\n",
        "print(f\"Capped {extreme_low} extremely low and {extreme_high} extremely high predictions\")\n",
        "\n",
        "# 2. Seasonal adjustment for unseen store-dept combinations\n",
        "print(\"🎯 Applying intelligent seasonal adjustments...\")\n",
        "\n",
        "# Create test dataframe with predictions for analysis\n",
        "test_with_pred = processed_test.copy()\n",
        "test_with_pred['Predictions'] = test_predictions\n",
        "\n",
        "# Adjust predictions based on business logic\n",
        "adjustments_made = 0\n",
        "\n",
        "for idx, row in test_with_pred.iterrows():\n",
        "    original_pred = test_predictions[idx]\n",
        "    adjusted_pred = original_pred\n",
        "\n",
        "    # Holiday boost (especially important for Walmart)\n",
        "    if row['IsHoliday'] == 1:\n",
        "        if row['IsBlackFriday'] == 1:\n",
        "            adjusted_pred *= 1.25  # Black Friday boost\n",
        "        elif row['IsChristmasWeek'] == 1:\n",
        "            adjusted_pred *= 1.15  # Christmas week boost\n",
        "        elif row['IsThanksgiving'] == 1:\n",
        "            adjusted_pred *= 1.20  # Thanksgiving boost\n",
        "        else:\n",
        "            adjusted_pred *= 1.10  # General holiday boost\n",
        "\n",
        "    # Q4 seasonal patterns\n",
        "    if row['IsQ4'] == 1 and row['IsHoliday'] == 0:\n",
        "        adjusted_pred *= 1.05  # General Q4 boost for non-holidays\n",
        "\n",
        "    # January post-holiday dip\n",
        "    if row['IsJanuary'] == 1 and row['IsHoliday'] == 0:\n",
        "        adjusted_pred *= 0.92  # Post-holiday reduction\n",
        "\n",
        "    # Back to school boost\n",
        "    if row['IsBackToSchool'] == 1:\n",
        "        adjusted_pred *= 1.08\n",
        "\n",
        "    # Summer patterns\n",
        "    if row['IsSummer'] == 1 and row['IsHoliday'] == 0:\n",
        "        adjusted_pred *= 1.03\n",
        "\n",
        "    # Promotion interaction\n",
        "    if row['HasAnyPromotion'] == 1:\n",
        "        if row['IsHoliday'] == 1:\n",
        "            adjusted_pred *= 1.05  # Extra boost for holiday promotions\n",
        "        else:\n",
        "            adjusted_pred *= 1.02  # Modest promotion boost\n",
        "\n",
        "    # Store type adjustments\n",
        "    if 'Type_A' in row and row['Type_A'] == 1:\n",
        "        adjusted_pred *= 1.02  # Supercenters typically higher\n",
        "    elif 'Type_C' in row and row['Type_C'] == 1:\n",
        "        adjusted_pred *= 0.98  # Neighborhood markets typically lower\n",
        "\n",
        "    # Apply adjustment if significant\n",
        "    if abs(adjusted_pred - original_pred) / original_pred > 0.01:  # Only if >1% change\n",
        "        test_predictions[idx] = adjusted_pred\n",
        "        adjustments_made += 1\n",
        "\n",
        "print(f\"Applied intelligent adjustments to {adjustments_made} predictions\")\n",
        "\n",
        "# 3. Final quality checks and fixes\n",
        "print(\"🔍 Final quality assurance...\")\n",
        "\n",
        "# Ensure no negative predictions\n",
        "negative_count = np.sum(test_predictions < 0)\n",
        "if negative_count > 0:\n",
        "    print(f\"Fixed {negative_count} negative predictions\")\n",
        "    test_predictions = np.maximum(test_predictions, 100)  # Minimum $100 sales\n",
        "\n",
        "# Check for NaN or infinite values\n",
        "nan_count = np.sum(np.isnan(test_predictions))\n",
        "inf_count = np.sum(np.isinf(test_predictions))\n",
        "\n",
        "if nan_count > 0:\n",
        "    print(f\"Fixed {nan_count} NaN predictions\")\n",
        "    median_pred = np.nanmedian(test_predictions)\n",
        "    test_predictions = np.where(np.isnan(test_predictions), median_pred, test_predictions)\n",
        "\n",
        "if inf_count > 0:\n",
        "    print(f\"Fixed {inf_count} infinite predictions\")\n",
        "    median_pred = np.median(test_predictions[np.isfinite(test_predictions)])\n",
        "    test_predictions = np.where(np.isinf(test_predictions), median_pred, test_predictions)\n",
        "\n",
        "prediction_time = time.time() - start_time\n",
        "print(f\"Advanced predictions completed in {prediction_time:.2f} seconds\")\n",
        "\n",
        "# Comprehensive prediction analysis\n",
        "print(f\"\\n=== FINAL PREDICTION ANALYSIS ===\")\n",
        "print(f\"Total predictions: {len(test_predictions):,}\")\n",
        "print(f\"Mean: ${np.mean(test_predictions):,.2f}\")\n",
        "print(f\"Median: ${np.median(test_predictions):,.2f}\")\n",
        "print(f\"Std: ${np.std(test_predictions):,.2f}\")\n",
        "print(f\"Min: ${np.min(test_predictions):,.2f}\")\n",
        "print(f\"Max: ${np.max(test_predictions):,.2f}\")\n",
        "\n",
        "# Compare with training distribution\n",
        "train_mean = processed_train['Weekly_Sales'].mean()\n",
        "train_median = processed_train['Weekly_Sales'].median()\n",
        "train_std = processed_train['Weekly_Sales'].std()\n",
        "\n",
        "print(f\"\\n📊 Comparison with Training Data:\")\n",
        "print(f\"Mean - Train: ${train_mean:,.2f}, Test: ${np.mean(test_predictions):,.2f}, Ratio: {np.mean(test_predictions)/train_mean:.3f}\")\n",
        "print(f\"Median - Train: ${train_median:,.2f}, Test: ${np.median(test_predictions):,.2f}, Ratio: {np.median(test_predictions)/train_median:.3f}\")\n",
        "print(f\"Std - Train: ${train_std:,.2f}, Test: ${np.std(test_predictions):,.2f}, Ratio: {np.std(test_predictions)/train_std:.3f}\")\n",
        "\n",
        "# Distribution analysis\n",
        "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
        "pred_percentiles = np.percentile(test_predictions, percentiles)\n",
        "train_percentiles = np.percentile(processed_train['Weekly_Sales'], percentiles)\n",
        "\n",
        "print(f\"\\n📈 Percentile Comparison:\")\n",
        "for p, pred_p, train_p in zip(percentiles, pred_percentiles, train_percentiles):\n",
        "    print(f\"{p}th - Train: ${train_p:,.0f}, Test: ${pred_p:,.0f}, Ratio: {pred_p/train_p:.3f}\")\n",
        "\n",
        "# Create submission with validation\n",
        "print(\"📄 Creating final submission...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Ensure prediction count matches submission template\n",
        "if len(test_predictions) != len(sample_submission):\n",
        "    print(f\"⚠️ Warning: Prediction count ({len(test_predictions)}) != submission count ({len(sample_submission)})\")\n",
        "    if len(test_predictions) > len(sample_submission):\n",
        "        test_predictions = test_predictions[:len(sample_submission)]\n",
        "        print(\"Truncated excess predictions\")\n",
        "    else:\n",
        "        # This shouldn't happen with proper processing, but handle gracefully\n",
        "        median_pred = np.median(test_predictions)\n",
        "        padding = [median_pred] * (len(sample_submission) - len(test_predictions))\n",
        "        test_predictions = np.concatenate([test_predictions, padding])\n",
        "        print(\"Padded missing predictions with median\")\n",
        "\n",
        "# Create submission\n",
        "submission = sample_submission.copy()\n",
        "submission['Weekly_Sales'] = test_predictions\n",
        "\n",
        "# Final validation of submission format\n",
        "assert len(submission) == len(sample_submission), \"Submission length mismatch\"\n",
        "assert not submission['Weekly_Sales'].isna().any(), \"NaN values in submission\"\n",
        "assert (submission['Weekly_Sales'] >= 0).all(), \"Negative values in submission\"\n",
        "\n",
        "# Save with advanced metadata\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "submission_filename = f'advanced_ensemble_submission_{timestamp}.csv'\n",
        "\n",
        "try:\n",
        "    submission.to_csv(submission_filename, index=False)\n",
        "    print(f\"✅ Submission saved: {submission_filename}\")\n",
        "\n",
        "    # Verify file integrity\n",
        "    verification = pd.read_csv(submission_filename)\n",
        "    assert len(verification) == len(submission), \"File verification failed\"\n",
        "    print(f\"✅ File integrity verified\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error saving submission: {e}\")\n",
        "    alt_filename = f'backup_advanced_submission_{timestamp}.csv'\n",
        "    submission.to_csv(alt_filename, index=False)\n",
        "    submission_filename = alt_filename\n",
        "    print(f\"✅ Saved to backup: {alt_filename}\")\n",
        "\n",
        "submission_time = time.time() - start_time\n",
        "print(f\"Submission created in {submission_time:.2f} seconds\")\n",
        "\n",
        "# Comprehensive logging\n",
        "total_time = prediction_time + submission_time\n",
        "if RETRAIN_ON_FULL_DATA:\n",
        "    total_time += final_training_time\n",
        "\n",
        "wandb.log({\n",
        "    'final_training_time': final_training_time,\n",
        "    'prediction_time': prediction_time,\n",
        "    'submission_time': submission_time,\n",
        "    'total_block5_time': total_time,\n",
        "    'final_predictions_count': len(test_predictions),\n",
        "    'final_predictions_mean': float(np.mean(test_predictions)),\n",
        "    'final_predictions_median': float(np.median(test_predictions)),\n",
        "    'final_predictions_std': float(np.std(test_predictions)),\n",
        "    'seasonal_adjustments_made': adjustments_made,\n",
        "    'extreme_predictions_capped': extreme_low + extreme_high,\n",
        "    'prediction_train_mean_ratio': np.mean(test_predictions) / train_mean,\n",
        "    'prediction_train_median_ratio': np.median(test_predictions) / train_median,\n",
        "    'file_size_mb': os.path.getsize(submission_filename) / (1024*1024)\n",
        "})\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"🚀 ADVANCED ENSEMBLE FORECASTING COMPLETE!\")\n",
        "print(f\"=\"*60)\n",
        "print(f\"📈 Validation MAE: {val_mae:.2f}\")\n",
        "print(f\"🎯 Target: <4,000 MAE\")\n",
        "print(f\"⏱️ Total Time: {total_time:.1f}s\")\n",
        "print(f\"📁 Submission: {submission_filename}\")\n",
        "print(f\"🔥 Ready for Kaggle submission!\")\n",
        "\n",
        "if val_mae < 4000:\n",
        "    print(\"🏆 CONGRATULATIONS - TARGET ACHIEVED!\")\n",
        "else:\n",
        "    print(f\"📊 Current gap to target: {val_mae - 4000:.0f} points\")\n",
        "    print(\"💡 Consider: More lag features, hyperparameter tuning, or additional external data\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "GeSG9nBjS1x4"
      },
      "id": "GeSG9nBjS1x4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Block 6: Pipeline Saving and Artifact Creation\n",
        "# =============================================================================\n",
        "\n",
        "class WalmartARIMAPipeline(BaseEstimator):\n",
        "    \"\"\"Complete ARIMA pipeline for Walmart sales forecasting\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_engineer = ARIMATimeSeriesFeatureEngineer()\n",
        "        self.model = WalmartARIMAModel()\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"Fitting complete ARIMA pipeline...\")\n",
        "        processed_data = self.feature_engineer.fit_transform(X)\n",
        "        self.model.fit(processed_data)\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Pipeline must be fitted before prediction\")\n",
        "        processed_data = self.feature_engineer.transform(X)\n",
        "        return self.model.predict(processed_data)\n",
        "\n",
        "# Create and save pipeline\n",
        "print(\"Creating complete ARIMA pipeline...\")\n",
        "pipeline = WalmartARIMAPipeline()\n",
        "pipeline.fit(train_df)\n",
        "\n",
        "# Save pipeline\n",
        "pipeline_filename = f'walmart_arima_pipeline_{timestamp}.pkl'\n",
        "with open(pipeline_filename, 'wb') as f:\n",
        "    dill.dump(pipeline, f)\n",
        "\n",
        "print(f\"Pipeline saved: {pipeline_filename}\")\n",
        "\n",
        "# Create WandB artifacts\n",
        "pipeline_artifact = wandb.Artifact(\n",
        "    name=\"walmart_arima_pipeline\",\n",
        "    type=\"model\",\n",
        "    description=\"Complete ARIMA pipeline for Walmart sales forecasting\",\n",
        "    metadata={\n",
        "        \"model_type\": \"SARIMAX\",\n",
        "        \"validation_mae\": val_mae,\n",
        "        \"validation_rmse\": val_rmse,\n",
        "        \"validation_mape\": val_mape,\n",
        "        \"models_trained\": len(final_model.models),\n",
        "        \"timestamp\": timestamp,\n",
        "        \"seasonal_order\": str(final_model.seasonal_order),\n",
        "        \"use_auto_arima\": final_model.use_auto_arima\n",
        "    }\n",
        ")\n",
        "\n",
        "pipeline_artifact.add_file(pipeline_filename)\n",
        "wandb.log_artifact(pipeline_artifact)\n",
        "\n",
        "submission_artifact = wandb.Artifact(\n",
        "    name=\"walmart_arima_submission\",\n",
        "    type=\"dataset\",\n",
        "    description=f\"ARIMA submission for Walmart sales - {timestamp}\"\n",
        ")\n",
        "submission_artifact.add_file(submission_filename)\n",
        "wandb.log_artifact(submission_artifact)\n",
        "\n",
        "# Final comprehensive logging\n",
        "wandb.log({\n",
        "    'pipeline_saved': True,\n",
        "    'submission_created': True,\n",
        "    'test_predictions_mean': np.mean(test_predictions),\n",
        "    'test_predictions_median': np.median(test_predictions),\n",
        "    'test_predictions_std': np.std(test_predictions),\n",
        "    'final_models_count': len(final_model.models),\n",
        "    'model_orders_variety': len(set(str(info['order']) for info in final_model.model_orders.values())),\n",
        "    'negative_predictions_handled': np.sum(test_predictions < 0) == 0\n",
        "})\n",
        "\n",
        "print(\"Walmart ARIMA forecasting completed successfully!\")\n",
        "print(f\"Validation MAE: {val_mae:.2f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Validation MAPE: {val_mape:.2f}%\")\n",
        "print(f\"Models trained: {len(final_model.models)}\")\n",
        "print(\"Pipeline and submission saved to WandB!\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "31H5aKc7Sezi"
      },
      "id": "31H5aKc7Sezi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}